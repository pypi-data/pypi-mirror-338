{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the CHILDES Tokenizer\n",
    "\n",
    "Using the phonemes in our CHILDES dataset, we train a tokenizer that just splits according to whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zebulongoriely/Documents/UniDocs/PHD/research/projects/CorpusPhonemizers/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from datasets import load_dataset, get_dataset_config_names\n",
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, trainers, processors, decoders\n",
    "from transformers import GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nz/6tzh0bsj2txd1cz18gpcms_c0000gn/T/ipykernel_84216/769076702.py:1: DtypeWarning: Columns (4,7,8,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  phoible = pd.read_csv('../../../data/phoible.csv')\n"
     ]
    }
   ],
   "source": [
    "phoible = pd.read_csv('../../../data/phoible.csv')\n",
    "phoible_phonemes = phoible.Phoneme.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_COUNT = 10\n",
    "STRESS_RE = re.compile(r\"[ˈˌ'-]+\")\n",
    "\n",
    "def build_vocabulary(datasets, column='phonemized_utterance', allow_non_phoible=False, allow_stressed_tokens=False):\n",
    "\n",
    "    vocab = {'UNK' : 0, 'PAD' : 1, 'WORD_BOUNDARY' : 2, 'UTT_BOUNDARY' : 3}\n",
    "    unk_tokens = []\n",
    "    token_counts = {}\n",
    "    for dataset in datasets:\n",
    "        for line in dataset[column]:\n",
    "            tokens = line.strip().split()\n",
    "            for token in tokens:\n",
    "                if token not in token_counts:\n",
    "                    token_counts[token] = 0\n",
    "                token_counts[token] += 1\n",
    "        \n",
    "    # Add tokens to vocab if they are not in phoible and have a count greater than MIN_COUNT\n",
    "    for token, count in token_counts.items():\n",
    "        if count > MIN_COUNT and token not in vocab:\n",
    "            if token not in phoible_phonemes and not allow_non_phoible:\n",
    "                if allow_stressed_tokens and STRESS_RE.findall(token):\n",
    "                    vocab[token] = len(vocab)\n",
    "                else:\n",
    "                    unk_tokens.append(token)\n",
    "            else:\n",
    "                vocab[token] = len(vocab)\n",
    "\n",
    "    print('Tokens not found in phoible: ', {token: token_counts[token] for token in unk_tokens})\n",
    "    print('Vocab: ', vocab)\n",
    "    print('Vocab size: ', len(vocab))\n",
    "    return vocab\n",
    "\n",
    "def build_phoneme_tokenizer(vocab, add_stress_replacer=False):\n",
    "\n",
    "    # We replace any kind of stress marker with a single primary stress marker\n",
    "    norms = []\n",
    "    if add_stress_replacer:\n",
    "        new_vocab = {}\n",
    "        for token in vocab:\n",
    "            if STRESS_RE.findall(token):\n",
    "                new_token = \"ˈ\" + STRESS_RE.sub('', token)\n",
    "                if token != new_token:\n",
    "                    norms.append(normalizers.Replace(token, new_token))\n",
    "                token = new_token\n",
    "            if token not in new_vocab:\n",
    "                new_vocab[token] = len(new_vocab)\n",
    "        vocab = new_vocab\n",
    "        print('Using only primary stress markers...')\n",
    "        print('New vocab: ', vocab)\n",
    "        print('New vocab size: ', len(vocab))\n",
    "    norms.append(normalizers.Strip())\n",
    "\n",
    "    tokenizer = Tokenizer(models.WordLevel(vocab=vocab, unk_token='UNK'))\n",
    "    # tokenizer.normalizer = normalizers.Sequence([normalizers.Replace(' WORD_BOUNDARY', ''), normalizers.Strip()]) \n",
    "    tokenizer.normalizer = normalizers.Sequence(norms) \n",
    "    tokenizer.add_special_tokens([\"UNK\", \"PAD\", \"UTT_BOUNDARY\", \"WORD_BOUNDARY\"])\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "    tokenizer.post_processor = processors.TemplateProcessing(\n",
    "        single=\"UTT_BOUNDARY $A\",\n",
    "        pair=\"UTT_BOUNDARY $A UTT_BOUNDARY $B:1\",\n",
    "        special_tokens=[(\"UTT_BOUNDARY\", tokenizer.token_to_id(\"UTT_BOUNDARY\"))],\n",
    "    )\n",
    "\n",
    "    wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer, bos_token='UTT_BOUNDARY', eos_token='UTT_BOUNDARY', pad_token='PAD', unk_token='UNK')\n",
    "    return wrapped_tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Tokenizer for each language in CHILDES\n",
    "\n",
    "We create a unique tokenizer for each language, to keep the vocabulary size appropriate for each language. For most languages we remove any tokens not found in Phoible. We do not do this for Mandarin or Cantonese as for these languages we merge the tone marker and preceding vowel into one phoneme, whereas Phoible treats tone markers as independent symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 435/435 [00:00<00:00, 1.35MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages: ['English', 'French', 'Dutch']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 885M/885M [00:26<00:00, 33.9MB/s] \n",
      "Generating train split: 2564614 examples [00:12, 210110.14 examples/s]\n",
      "Downloading data: 100%|██████████| 198M/198M [00:05<00:00, 38.0MB/s] \n",
      "Generating train split: 721121 examples [00:03, 232560.48 examples/s]\n",
      "Downloading data: 100%|██████████| 133M/133M [00:03<00:00, 35.8MB/s] \n",
      "Generating train split: 403472 examples [00:01, 217400.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "languages = get_dataset_config_names('phonemetransformers/CHILDES-stress')\n",
    "#languages = ['Mandarin', 'Cantonese']\n",
    "print('Languages:', languages)\n",
    "datasets = {language : load_dataset('phonemetransformers/CHILDES-stress', language, split='train') for language in languages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training tokenizer for English...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'd̠ʒ': 4, 'ˈʌ': 5, 's': 6, 't': 7, 'l': 8, 'ˈaɪ': 9, 'k': 10, 'j': 11, 'ʊ': 12, 'ɹ': 13, 'b': 14, 'ˈʊ': 15, 'æ': 16, 'h': 17, 'ˈoʊ': 18, 'm': 19, 'ˈiː': 20, 'ð': 21, 'ɛ': 22, 'z': 23, 'ʌ': 24, 'f': 25, 'ˈeɪ': 26, 'w': 27, 'ɪ': 28, 'ɡ': 29, 'ˈæ': 30, 'ˈɑ': 31, 'ə': 32, 'p': 33, 'ˈuː': 34, 'ˈɛ': 35, 'i': 36, 'ˌuː': 37, 'ɑ': 38, 'θ': 39, 'ˈɪ': 40, 'ŋ': 41, 'iː': 42, 'uː': 43, 'ɔ': 44, 'aɪ': 45, 'ˈɔɪ': 46, 'n': 47, 'd': 48, 'ˈɔ': 49, 'ˈaʊ': 50, 'v': 51, 'ˈɜː': 52, 'ˌʌ': 53, 't̠ʃ': 54, 'ˌɔ': 55, 'oʊ': 56, 'ˌoʊ': 57, 'ˌʊ': 58, 'ˌeɪ': 59, 'ʃ': 60, 'ˌɛ': 61, 'ɜː': 62, 'ˌɑ': 63, 'ˌaʊ': 64, 'ˌaɪ': 65, 'ˌə': 66, 'ˌiː': 67, 'ˌɪ': 68, 'eɪ': 69, 'iə': 70, 'ˈiə': 71, 'ˌæ': 72, 'ˌɜː': 73, 'aʊ': 74, 'ˌɔɪ': 75, 'ɔɪ': 76, 'ˌiə': 77, 'ʒ': 78, 'ˈə': 79, 'x': 80}\n",
      "Vocab size:  81\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'd̠ʒ': 4, 'ˈʌ': 5, 's': 6, 't': 7, 'l': 8, 'ˈaɪ': 9, 'k': 10, 'j': 11, 'ʊ': 12, 'ɹ': 13, 'b': 14, 'ˈʊ': 15, 'æ': 16, 'h': 17, 'ˈoʊ': 18, 'm': 19, 'ˈiː': 20, 'ð': 21, 'ɛ': 22, 'z': 23, 'ʌ': 24, 'f': 25, 'ˈeɪ': 26, 'w': 27, 'ɪ': 28, 'ɡ': 29, 'ˈæ': 30, 'ˈɑ': 31, 'ə': 32, 'p': 33, 'ˈuː': 34, 'ˈɛ': 35, 'i': 36, 'ɑ': 37, 'θ': 38, 'ˈɪ': 39, 'ŋ': 40, 'iː': 41, 'uː': 42, 'ɔ': 43, 'aɪ': 44, 'ˈɔɪ': 45, 'n': 46, 'd': 47, 'ˈɔ': 48, 'ˈaʊ': 49, 'v': 50, 'ˈɜː': 51, 't̠ʃ': 52, 'oʊ': 53, 'ʃ': 54, 'ɜː': 55, 'ˈə': 56, 'eɪ': 57, 'iə': 58, 'ˈiə': 59, 'aʊ': 60, 'ɔɪ': 61, 'ʒ': 62, 'x': 63}\n",
      "New vocab size:  64\n",
      "Tokenizer for English pushed to the hub.\n",
      "\n",
      "Training tokenizer for French...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'm': 4, 'a': 5, 'ˈɑ̃': 6, 'd': 7, 'ˈɔ': 8, 'n': 9, 'b': 10, 'ˈa': 11, 'ʁ': 12, 'ə': 13, 'ɡ': 14, 'ɔ': 15, 'ʒ': 16, 'ˈi': 17, 'v': 18, 't': 19, 'k': 20, 'ˈo': 21, 'ˈɛ̃': 22, 'w': 23, 'y': 24, 'j': 25, 'ˈy': 26, 'e': 27, 'ɔ̃': 28, 'ˈe': 29, 'e-': 30, 'p': 31, 'ɛ': 32, 'f': 33, 's': 34, 'z': 35, 'l': 36, 'ə-': 37, 'ˈɛ': 38, 'u': 39, 'o': 40, 'ʃ': 41, 'a-': 42, 'i': 43, 'ˈu': 44, 'ɛ̃': 45, 'ˌa': 46, 'œ': 47, 'ˈø': 48, 'ˌɛ': 49, 'ˌi': 50, 'ɑ̃': 51, 'ˈœ': 52, 'ˈɔ̃': 53, 'ˌɔ': 54, 'ø': 55, 'ˌɑ̃': 56, 'ˈə-': 57, 'y-': 58, 'ˈe-': 59, 'ˈa-': 60, 'ˌɛ̃': 61, 'ˈə': 62, 'ˌu': 63, 'ˌœ': 64, 'ˌø': 65, 'ɲ': 66, 'ˌe': 67, 'ˈw': 68, 'ˌə': 69, 't̠ʃ': 70, 'ˌy': 71, 'd̠ʒ': 72, 'ˌo': 73}\n",
      "Vocab size:  74\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'm': 4, 'a': 5, 'ˈɑ̃': 6, 'd': 7, 'ˈɔ': 8, 'n': 9, 'b': 10, 'ˈa': 11, 'ʁ': 12, 'ə': 13, 'ɡ': 14, 'ɔ': 15, 'ʒ': 16, 'ˈi': 17, 'v': 18, 't': 19, 'k': 20, 'ˈo': 21, 'ˈɛ̃': 22, 'w': 23, 'y': 24, 'j': 25, 'ˈy': 26, 'e': 27, 'ɔ̃': 28, 'ˈe': 29, 'p': 30, 'ɛ': 31, 'f': 32, 's': 33, 'z': 34, 'l': 35, 'ˈə': 36, 'ˈɛ': 37, 'u': 38, 'o': 39, 'ʃ': 40, 'i': 41, 'ˈu': 42, 'ɛ̃': 43, 'œ': 44, 'ˈø': 45, 'ɑ̃': 46, 'ˈœ': 47, 'ˈɔ̃': 48, 'ø': 49, 'ɲ': 50, 'ˈw': 51, 't̠ʃ': 52, 'd̠ʒ': 53}\n",
      "New vocab size:  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer for French pushed to the hub.\n",
      "\n",
      "Training tokenizer for Dutch...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'z': 4, 'ˈoː': 5, 'j': 6, 'ˈãː': 7, 'ɦ': 8, 'ɾ': 9, 'd': 10, 'i': 11, 'ɛ': 12, 'p': 13, 'ɪ': 14, 'k': 15, 'ˈɑ': 16, 'l': 17, 'ˈɛː': 18, 'n': 19, 's': 20, 'v': 21, 'ə': 22, 'ˈɛi': 23, 'ʋ': 24, 'ˈɛ': 25, 't': 26, 'm': 27, 'ɣ': 28, 'ˈʏ': 29, 'ãː': 30, 'oː': 31, 'ˈɔ': 32, 'x': 33, 'ɑ': 34, 'ˈu': 35, 'f': 36, 'ŋ': 37, 'ˈøː': 38, 'ɔ': 39, 'ˈi': 40, 'b': 41, 'ˌãː': 42, 'ɔː': 43, 'ˈɪ': 44, 'ˌi': 45, 'ɛː': 46, 'ˈʌu': 47, 'ɛi': 48, 'ˌɛi': 49, 'ˈy': 50, 'ˌʌu': 51, 'ʏ': 52, 'ˈœy': 53, 'tʲ': 54, 'ˌɛ': 55, 'ˌɑ': 56, 'ʌu': 57, 'u': 58, 'ˌʏ': 59, 'ˈɔː': 60, 'œy': 61, 'ˌɛː': 62, 'w': 63, 'ˌu': 64, 'y': 65, 'ˌɪ': 66, 'ˌoː': 67, 'ˌə': 68, 'ˌøː': 69, 'ˌɔ': 70, 'ʃ': 71, 'ˈə': 72, 't̠ʃ': 73, 'ɲ': 74, 'ˌy': 75, 'ʒ': 76, 'ˌœy': 77, 'ˌɔː': 78, 'ˈiː': 79, 'ɡ': 80, 'øː': 81, 'd̠ʒ': 82, 'ã': 83}\n",
      "Vocab size:  84\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'z': 4, 'ˈoː': 5, 'j': 6, 'ˈãː': 7, 'ɦ': 8, 'ɾ': 9, 'd': 10, 'i': 11, 'ɛ': 12, 'p': 13, 'ɪ': 14, 'k': 15, 'ˈɑ': 16, 'l': 17, 'ˈɛː': 18, 'n': 19, 's': 20, 'v': 21, 'ə': 22, 'ˈɛi': 23, 'ʋ': 24, 'ˈɛ': 25, 't': 26, 'm': 27, 'ɣ': 28, 'ˈʏ': 29, 'ãː': 30, 'oː': 31, 'ˈɔ': 32, 'x': 33, 'ɑ': 34, 'ˈu': 35, 'f': 36, 'ŋ': 37, 'ˈøː': 38, 'ɔ': 39, 'ˈi': 40, 'b': 41, 'ɔː': 42, 'ˈɪ': 43, 'ɛː': 44, 'ˈʌu': 45, 'ɛi': 46, 'ˈy': 47, 'ʏ': 48, 'ˈœy': 49, 'tʲ': 50, 'ʌu': 51, 'u': 52, 'ˈɔː': 53, 'œy': 54, 'w': 55, 'y': 56, 'ˈə': 57, 'ʃ': 58, 't̠ʃ': 59, 'ɲ': 60, 'ʒ': 61, 'ˈiː': 62, 'ɡ': 63, 'øː': 64, 'd̠ʒ': 65, 'ã': 66}\n",
      "New vocab size:  67\n",
      "Tokenizer for Dutch pushed to the hub.\n"
     ]
    }
   ],
   "source": [
    "for language, dataset in datasets.items():\n",
    "    print(f'\\nTraining tokenizer for {language}...')\n",
    "    allow_non_phoible = language in ['Mandarin', 'Cantonese'] # For Mandarin and Cantonese, allow non-phoible tokens since we merge tone with vowels\n",
    "    vocab = build_vocabulary([dataset], allow_non_phoible=allow_non_phoible, allow_stressed_tokens=True)\n",
    "    tokenizer = build_phoneme_tokenizer(vocab, add_stress_replacer=True)\n",
    "    tokenizer.push_to_hub(f\"phonemetransformers/CHILDES-{language}-phoneme-tokenizer-stress\")\n",
    "    print(f'Tokenizer for {language} pushed to the hub.')\n",
    "\n",
    "# print(f'\\nTrainking tokenizer for all languages...')\n",
    "# vocab = build_vocabulary(datasets.values())\n",
    "# tokenizer = build_phoneme_tokenizer(vocab)\n",
    "# tokenizer.push_to_hub(\"phonemetransformers/CHILDES-phoneme-tokenizer\")\n",
    "# print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English tokenizer is ok: True\n",
      "French tokenizer is ok: True\n",
      "Dutch tokenizer is ok: True\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def check_tokenizer(tokenizer):\n",
    "    # It turns out that the Whitespace normalizer does not include tone symbols, so for the Cantonese \n",
    "    # and Mandarin tokenizers, it was splitting phonemes like 'a˥' in two, and so converting them to two UNK\n",
    "    # tokens. This is fixed by using WhitespaceSplit normalizer, which works like split().\n",
    "    is_ok = True\n",
    "    for v, x in tokenizer.vocab.items():\n",
    "        if not (tokenizer.encode(v)[1:] == [x]):\n",
    "            #print(f'Tokenizer failed to encode \"{v}\", gave {tokenizer.encode(v)[1:]}')\n",
    "            is_ok = False\n",
    "    return is_ok\n",
    "\n",
    "for language in datasets.keys():\n",
    "    t = AutoTokenizer.from_pretrained(f'phonemetransformers/CHILDES-{language}-phoneme-tokenizer')\n",
    "    is_ok = check_tokenizer(t)\n",
    "    print(f'{language} tokenizer is ok: {is_ok}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE Tokenizers for CHILDES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('phonemetransformers/CHILDES', 'English', split='train')\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "        [normalizers.NFD(),\n",
    "         normalizers.Lowercase(),\n",
    "         normalizers.Strip(),\n",
    "         normalizers.StripAccents(),\n",
    "        ]\n",
    "    )\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "\n",
    "trainer = trainers.BpeTrainer(vocab_size=8192, special_tokens=[\"UTT_BOUNDARY\", \"PAD\", \"UNK\"])\n",
    "tokenizer.train_from_iterator(dataset['processed_gloss'], trainer=trainer)\n",
    "\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"UTT_BOUNDARY $A\",\n",
    "    pair=\"UTT_BOUNDARY $A UTT_BOUNDARY $B:1\",\n",
    "    special_tokens=[(\"UTT_BOUNDARY\", tokenizer.token_to_id(\"UTT_BOUNDARY\"))],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: is that what you saw?\n",
      "['UTT_BOUNDARY', 'Ġis', 'Ġthat', 'Ġwhat', 'Ġyou', 'Ġsaw', '?']\n"
     ]
    }
   ],
   "source": [
    "example = dataset['processed_gloss'][300]\n",
    "encoding = tokenizer.encode(example)\n",
    "print(f'Example: {example}')\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/phonemetransformers/CHILDES-English-BPE-gloss-tokenizer/commit/dc70201e9f3dc609aea522ae4df6cc435f07a55e', commit_message='Upload tokenizer', commit_description='', oid='dc70201e9f3dc609aea522ae4df6cc435f07a55e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/phonemetransformers/CHILDES-English-BPE-gloss-tokenizer', endpoint='https://huggingface.co', repo_type='model', repo_id='phonemetransformers/CHILDES-English-BPE-gloss-tokenizer'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer, pad_token='PAD', unk_token='UNK', bos_token='UTT_BOUNDARY', eos_token='UTT_BOUNDARY', add_prefix_space=True)\n",
    "wrapped_tokenizer.push_to_hub(\"phonemetransformers/CHILDES-English-BPE-gloss-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 115, 92, 95, 67, 781, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = wrapped_tokenizer(example, padding='max_length', max_length=20, truncation=True, add_special_tokens=True)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UTT_BOUNDARY',\n",
       " 'Ġis',\n",
       " 'Ġthat',\n",
       " 'Ġwhat',\n",
       " 'Ġyou',\n",
       " 'Ġsaw',\n",
       " '?',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.convert_ids_to_tokens(tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 124, 115, 61, 3630, 45, 6], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer('this is a test .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
