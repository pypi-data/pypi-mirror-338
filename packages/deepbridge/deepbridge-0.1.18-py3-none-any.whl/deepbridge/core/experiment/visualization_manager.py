import typing as t
from deepbridge.core.experiment.test_runner import TestRunner
from deepbridge.core.experiment.managers import (
    RobustnessManager, UncertaintyManager, ResilienceManager, HyperparameterManager
)

class VisualizationManager:
    """
    Manages visualization and retrieval of test results.
    Extracted from Experiment class to centralize visualization responsibilities.
    """
    
    def __init__(self, test_runner: TestRunner):
        """
        Initialize the visualization manager with a test runner.
        
        Args:
            test_runner: The TestRunner instance containing test results
        """
        self.test_runner = test_runner
    
    # Robustness visualization methods
    def get_robustness_results(self):
        """
        Get the robustness test results.
        
        Returns:
            dict: Dictionary containing robustness test results for main model and alternatives.
                  Returns None if robustness tests haven't been run.
        """
        results = self.test_runner.get_test_results('robustness')
        if results is None and "robustness" in self.test_runner.tests:
            # Run robustness tests if they were requested but not run yet
            robustness_manager = RobustnessManager(
                self.test_runner.dataset, 
                self.test_runner.alternative_models, 
                self.test_runner.verbose
            )
            results = robustness_manager.run_tests()
            self.test_runner.test_results['robustness'] = results
        
        return results
        
    def get_robustness_visualizations(self):
        """
        Get the robustness visualizations generated by the tests.
        
        Returns:
            dict: Dictionary of plotly figures for robustness visualizations.
                  Returns empty dict if no visualizations are available.
        """
        results = self.get_robustness_results()
        if not results:
            return {}
                
        return results.get('visualizations', {})
        
    def plot_robustness_comparison(self):
        """
        Get the plotly figure showing the comparison of robustness across models.
        
        Returns:
            plotly.graph_objects.Figure: Comparison plot of models robustness.
                                         Returns None if visualization not available.
        """
        visualizations = self.get_robustness_visualizations()
        return visualizations.get('models_comparison')
    
    def plot_robustness_distribution(self):
        """
        Get the boxplot showing distribution of robustness scores.
        
        Returns:
            plotly.graph_objects.Figure: Boxplot of robustness score distributions.
                                         Returns None if visualization not available.
        """
        visualizations = self.get_robustness_visualizations()
        return visualizations.get('score_distribution')
    
    def plot_feature_importance_robustness(self):
        """
        Get the plotly figure showing feature importance for robustness.
        
        Returns:
            plotly.graph_objects.Figure: Feature importance bar chart.
                                         Returns None if visualization not available.
        """
        visualizations = self.get_robustness_visualizations()
        return visualizations.get('feature_importance')
    
    def plot_perturbation_methods_comparison(self):
        """
        Get the plotly figure comparing different perturbation methods.
        
        Returns:
            plotly.graph_objects.Figure: Comparison plot of perturbation methods.
                                         Returns None if visualization not available.
        """
        visualizations = self.get_robustness_visualizations()
        return visualizations.get('perturbation_methods')
        
    # Uncertainty visualization methods
    def get_uncertainty_results(self):
        """
        Get the uncertainty test results.
        
        Returns:
            dict: Dictionary containing uncertainty test results for main model and alternatives.
                  Returns None if uncertainty tests haven't been run.
        """
        results = self.test_runner.get_test_results('uncertainty')
        if results is None and "uncertainty" in self.test_runner.tests:
            # Run uncertainty tests if they were requested but not run yet
            uncertainty_manager = UncertaintyManager(
                self.test_runner.dataset, 
                self.test_runner.alternative_models, 
                self.test_runner.verbose
            )
            results = uncertainty_manager.run_tests()
            self.test_runner.test_results['uncertainty'] = results
        
        return results
        
    def get_uncertainty_visualizations(self):
        """
        Get the uncertainty visualizations generated by the tests.
        
        Returns:
            dict: Dictionary of plotly figures for uncertainty visualizations.
                  Returns empty dict if no visualizations are available.
        """
        results = self.get_uncertainty_results()
        if not results:
            return {}
                
        return results.get('visualizations', {})
        
    def plot_uncertainty_alpha_comparison(self):
        """
        Get the plotly figure showing the comparison of different alpha levels.
        
        Returns:
            plotly.graph_objects.Figure: Comparison plot of alpha levels.
                                         Returns None if visualization not available.
        """
        visualizations = self.get_uncertainty_visualizations()
        return visualizations.get('alpha_comparison')
    
    def plot_uncertainty_width_distribution(self):
        """
        Get the boxplot showing distribution of interval widths.
        
        Returns:
            plotly.graph_objects.Figure: Boxplot of interval width distributions.
                                         Returns None if visualization not available.
        """
        visualizations = self.get_uncertainty_visualizations()
        return visualizations.get('width_distribution')
    
    def plot_feature_importance_uncertainty(self):
        """
        Get the plotly figure showing feature importance for uncertainty.
        
        Returns:
            plotly.graph_objects.Figure: Feature importance bar chart.
                                         Returns None if visualization not available.
        """
        visualizations = self.get_uncertainty_visualizations()
        return visualizations.get('feature_importance')
    
    def plot_coverage_vs_width(self):
        """
        Get the plotly figure showing trade-off between coverage and width.
        
        Returns:
            plotly.graph_objects.Figure: Coverage vs width plot.
                                         Returns None if visualization not available.
        """
        visualizations = self.get_uncertainty_visualizations()
        return visualizations.get('coverage_vs_width')
        
    # Resilience results methods
    def get_resilience_results(self):
        """
        Get the resilience test results.
        
        Returns:
            dict: Dictionary containing resilience test results for main model and alternatives.
                  Returns None if resilience tests haven't been run.
        """
        results = self.test_runner.get_test_results('resilience')
        if results is None and "resilience" in self.test_runner.tests:
            # Run resilience tests if they were requested but not run yet
            resilience_manager = ResilienceManager(
                self.test_runner.dataset, 
                self.test_runner.alternative_models, 
                self.test_runner.verbose
            )
            results = resilience_manager.run_tests()
            self.test_runner.test_results['resilience'] = results
        
        return results
    
    # Hyperparameter methods
    def get_hyperparameter_results(self):
        """
        Get the hyperparameter importance test results.
        
        Returns:
            dict: Dictionary containing hyperparameter importance results for main model and alternatives.
                  Returns None if hyperparameter tests haven't been run.
        """
        results = self.test_runner.get_test_results('hyperparameters')
        if results is None and "hyperparameters" in self.test_runner.tests:
            # Run hyperparameter tests if they were requested but not run yet
            hyperparameter_manager = HyperparameterManager(
                self.test_runner.dataset, 
                self.test_runner.alternative_models, 
                self.test_runner.verbose
            )
            results = hyperparameter_manager.run_tests()
            self.test_runner.test_results['hyperparameters'] = results
        
        return results
    
    def get_hyperparameter_importance(self):
        """
        Get the hyperparameter importance scores for the primary model.
        
        Returns:
            dict: Dictionary of parameter names to importance scores.
                  Returns None if hyperparameter tests haven't been run.
        """
        results = self.get_hyperparameter_results()
        if results and 'primary_model' in results:
            return results['primary_model'].get('sorted_importance', {})
        return None
    
    def get_hyperparameter_tuning_order(self):
        """
        Get the suggested hyperparameter tuning order for the primary model.
        
        Returns:
            list: List of parameter names in recommended tuning order.
                  Returns None if hyperparameter tests haven't been run.
        """
        results = self.get_hyperparameter_results()
        if results and 'primary_model' in results:
            return results['primary_model'].get('tuning_order', [])
        return None