"""eventually to go in dedicated modules"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_core.ipynb.

# %% auto 0
__all__ = ['convert_palette_to_hex', 'create_group_color_mapping', 'norm_loading', 'quantileNormalize', 'norm_loading_TMT',
           'ires_norm', 'clean_id', 'mod_hist_legend', 'clean_axes', 'parse_fasta_file', 'add_desc', 'get_scaled_df']

# %% ../nbs/00_core.ipynb 4
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.colors as mcolors
from matplotlib.patches import Patch
import numpy as np
import re
import matplotlib

# %% ../nbs/00_core.ipynb 5
def convert_palette_to_hex(palette_name, n_colors):
    """
    Convert a named color palette to hex color codes.
    
    Parameters:
    -----------
    palette_name : str
        Name of the palette (e.g., 'tab10', 'Set1', 'husl', 'viridis')
    n_colors : int
        Number of colors to generate
        
    Returns:
    --------
    list
        List of hex color codes
    """

    
    try:
        # Try to get palette from seaborn
        palette = sns.color_palette(palette_name, n_colors)
        hex_colors = [mcolors.rgb2hex(color) for color in palette]
        return hex_colors
    except:
        try:
            # Try as a matplotlib colormap
            cmap = plt.cm.get_cmap(palette_name, n_colors)
            hex_colors = [mcolors.rgb2hex(cmap(i)) for i in range(n_colors)]
            return hex_colors
        except:
            # Fallback to default
            print(f"Palette '{palette_name}' not found, using default.")
            return sns.color_palette("husl", n_colors).as_hex()


# %% ../nbs/00_core.ipynb 6
def create_group_color_mapping(items, group_size=3, palette=None, palette_name=None, return_color_to_group=False):
    """
    Create a color mapping dictionary that assigns the same color to items in groups.
    
    Parameters:
    -----------
    items : list
        List of items to be mapped to colors
    group_size : int
        Number of items to assign to each color (default: 3)
    palette : list or None
        List of colors to use. If None, uses default colors in hex format.
    palette_name : str or None
        Name of a seaborn/matplotlib palette (e.g., 'tab10', 'Set1') to use.
        This is used if palette is None.
    return_color_to_group : bool
        If True, also returns a dictionary mapping colors to group names
        
    Returns:
    --------
    dict or tuple
        Dictionary mapping each item to its assigned color (hex format)
        If return_color_to_group is True, also returns a dict mapping colors to group names
    """
    # Calculate how many colors we need
    num_groups = (len(items) + group_size - 1) // group_size  # Ceiling division
    
    # Generate or use provided color palette in hex format
    if palette is None:
        if palette_name is not None:
            # Use the specified palette name
            colors = convert_palette_to_hex(palette_name, num_groups)
        else:
            # Default hex color palette
            default_colors = [
                '#FF5733', '#33FF57', '#3357FF', '#FF33A8', '#33FFF5', 
                '#FFD133', '#A833FF', '#FF8D33', '#33ACFF', '#FF3352'
            ]
            
            # If we need more colors, generate them
            if num_groups > len(default_colors):
                
                
                # Use seaborn to generate additional colors in hex
                additional_colors = sns.color_palette("husl", num_groups - len(default_colors)).as_hex()
                colors = default_colors + additional_colors
            else:
                colors = default_colors[:num_groups]
    else:
        # Convert any non-hex colors in provided palette to hex format
        
        colors = []
        for color in palette:
            if isinstance(color, str) and color.startswith('#'):
                colors.append(color)
            else:
                try:
                    colors.append(mcolors.to_hex(color))
                except:
                    colors.append('#888888')  # Default gray if conversion fails
        
        # If palette is too small, cycle it
        if len(colors) < num_groups:
            colors = colors * (num_groups // len(colors) + 1)
        
        colors = colors[:num_groups]
    
    # Create the mapping dictionary
    color_mapping = {}
    # If requested, also create a mapping from color to group name
    color_to_group = {}
    
    for i, item in enumerate(items):
        group_idx = i // group_size
        group_name = f"Group {group_idx + 1}"
        color_idx = min(group_idx, len(colors) - 1)  # Ensure we don't go out of bounds
        color = colors[color_idx]
        
        color_mapping[item] = color
        
        # Add to color_to_group dictionary if it doesn't exist yet
        if return_color_to_group and color not in color_to_group:
            color_to_group[color] = group_name
    
    if return_color_to_group:
        return color_mapping, color_to_group
    else:
        return color_mapping



# %% ../nbs/00_core.ipynb 9
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def norm_loading(df):
    '''
    Normalize datasets by equalizing the medians of all columns to a common target value.
    
    This function implements a median normalization strategy that:
    1. Calculates the median value for each column in the input dataframe
    2. Computes a target value (the mean of all column medians)
    3. Derives normalization factors to adjust each column to the target median
    4. Applies these normalization factors to create a normalized dataset
    
    Parameters:
    -----------
    df : pandas.DataFrame
        Input dataframe where:
        - Each column represents a sample/replicate
        - Each row represents a feature/variable (e.g., protein, gene, metabolite)
        
    Returns:
    --------
    pandas.DataFrame
        Normalized dataframe with the same dimensions as the input,
        where all columns have approximately the same median value
    '''
    # Calculate the median of each column
    medians = df.median(axis=0)
    print('medians', np.array(medians))
    
    # Compute target value (the mean of all column medians)
    # This becomes our reference point to normalize all columns
    target = np.mean(medians)
    print('target', target)
    
    # Calculate normalization factors for each column
    # Columns with medians below the target will be scaled up (norm_fac > 1)
    # Columns with medians above the target will be scaled down (norm_fac < 1)
    norm_facs = target / medians
    print('norm_facs', np.array(norm_facs))
    
    # Apply normalization factors to each column
    # This preserves relative differences within each column while
    # equalizing the median values across all columns
    data_norm = df.multiply(norm_facs, axis=1)
    
    return data_norm



# %% ../nbs/00_core.ipynb 11
def quantileNormalize(df_input, keep_na=True):
    """
    Perform quantile normalization on a pandas DataFrame.
    
    Quantile normalization is a technique that makes the distribution of values
    for each column identical by transforming the values to match the distribution
    of the mean of quantiles across all columns.
    
    Algorithm:
    1. Sort values in each column independently
    2. Calculate the mean across rows of the sorted data (creating a reference distribution)
    3. For each original value, assign the corresponding value from the reference distribution
       based on its rank in its original column
    
    Parameters:
    -----------
    df_input : pandas.DataFrame
        Input dataframe where:
        - Each column represents a sample/variable
        - Each row represents an observation/feature
    
    keep_na : bool, default=True
        If True, preserves NaN values in the original data
        If False, NaN values will be assigned a normalized value
    
    Returns:
    --------
    pandas.DataFrame
        Normalized dataframe with the same shape as the input, where
        all columns have identical distributions
    
    """
    # Create a copy of the input dataframe to avoid modifying the original
    df = df_input.copy()
    
    # Step 1: Sort values in each column and store in a dictionary
    dic = {}
    for col in df:
        dic.update({col: sorted(df[col])})
    
    # Create a dataframe from the sorted values
    sorted_df = pd.DataFrame(dic)
    
    # Step 2: Calculate the mean across each row to create the reference distribution
    # This represents the average quantile values across all columns
    rank = sorted_df.mean(axis=1).tolist()
    
    # Step 3: Map each value in the original dataframe to its corresponding value
    # in the reference distribution based on its rank
    for col in df:
        # Find the rank of each value in the original column
        # np.searchsorted returns indices where original values would be inserted
        # into the sorted array to maintain order
        t = np.searchsorted(np.sort(df[col]), df[col])
        
        # Assign the corresponding value from the reference distribution
        norm = [rank[i] for i in t]
        
        # Preserve NaN values if keep_na is True
        if keep_na == True:
            # Replace normalized values with NaN where original values were NaN
            norm = [np.nan if np.isnan(a) else b for a, b in zip(df[col], norm)]
        
        # Update the column with normalized values
        df[col] = norm
    
    return df

# %% ../nbs/00_core.ipynb 13
def norm_loading_TMT(df):
    """
    Normalize TMT (Tandem Mass Tag) proteomics data to account for uneven sample loading.
    
    This function performs total sum normalization, specifically designed for 
    TMT-based multiplexed proteomics experiments where differences in total 
    protein abundance between samples may be due to technical variations rather
    than biological differences.
    
    Background:
    -----------
    In TMT proteomics experiments, multiple samples are labeled with different 
    isobaric tags and measured simultaneously. However, variations in:
      - Sample preparation
      - Protein extraction efficiency
      - Peptide/protein loading
      - Labeling efficiency
    can lead to systematic biases that affect all proteins within a sample.
    
    Algorithm:
    ----------
    1. Calculate the sum of all protein abundances for each sample (column)
    2. Compute a target value (mean of all column sums)
    3. Calculate normalization factors for each sample
    4. Scale each sample by its corresponding normalization factor
    
    Parameters:
    -----------
    df : pandas.DataFrame
        Input dataframe where:
        - Rows represent proteins or peptides
        - Columns represent TMT channels/samples
        - Values are intensity/abundance measurements
    
    Returns:
    --------
    pandas.DataFrame
        Normalized dataframe with the same dimensions as the input,
        where each sample's total protein abundance is equalized
    
    Notes:
    ------
    - This normalization assumes that differences in total protein abundance
      between samples are due to technical variation, not biological differences
    - This method is preferable to median normalization for TMT data because it
      accounts for the total reporter ion signal in each channel
    - For samples where biological differences in total protein content are
      expected, alternative normalization strategies should be considered
    """
    # Calculate the sum of all values in each column (total protein per sample)
    # This represents the total reporter ion signal in each TMT channel
    col_sum = df.sum(axis=0)
    # print(col_sum)
    
    # Calculate the target value (average of all sample totals)
    # This becomes our reference point to normalize all samples
    target = np.mean(col_sum)
    # print(target)
    
    # Calculate normalization factors for each sample
    # Samples with lower total protein will get scaled up (norm_fac > 1)
    # Samples with higher total protein will get scaled down (norm_fac < 1)
    norm_facs = target / col_sum
    # print(norm_facs)
    
    # Apply normalization factors to each sample (column)
    # This preserves relative protein abundances within each sample
    # while equalizing the total protein content across all samples
    data_norm = df.multiply(norm_facs, axis=1)
    
    return data_norm

# %% ../nbs/00_core.ipynb 15
def ires_norm(df, exps_columns):
    """
    Implement Internal Reference Scaling (IRS) normalization for combining multiple TMT experiments.
    
    This function normalizes and integrates data from multiple TMT experiments by:
    1. Computing the sum of each protein's intensity across all channels within each experiment
    2. Calculating the geometric mean of these sums across experiments (reference value)
    3. Deriving scaling factors to adjust each experiment to this reference
    4. Applying an additional total sum normalization to the combined dataset
    
    Background:
    -----------
    When analyzing multiple TMT experiments (plexes), systematic biases can exist between 
    runs that make direct comparison challenging. IRS normalization uses common proteins 
    across experiments as internal references to make data comparable across these 
    different multiplexed runs.
    
    Parameters:
    -----------
    df : pandas.DataFrame
        Input dataframe where:
        - Rows represent proteins or peptides
        - Columns include all TMT channels from all experiments
        - Values are intensity/abundance measurements
    
    exps_columns : list of lists
        A list containing lists of column names for each experiment
        Example: [[TMT1_126, TMT1_127, ...], [TMT2_126, TMT2_127, ...]]
        Each sublist contains all the TMT channel names for one experiment
    
    Returns:
    --------
    pandas.DataFrame
        Normalized and integrated dataframe combining all experiments, where:
        - Inter-experiment biases have been corrected
        - Total loading has been normalized
    
    Notes:
    ------
    - Missing values (NaN) should be handled before using this function
      (e.g., by imputation or replacing with means)
    - After normalization, you may want to restore original missing values
    - A small constant (1e-8) is added to avoid log(0) issues
    - This implementation combines the IRS approach with total sum normalization
    
    Example:
    --------
    irs_df = ires_norm(df.replace(0, np.nan).fillna(df.mean()), [tmt10_cols, tmt6_cols])
    # Restore original missing values after normalization
    irs_df[df.replace(0, np.nan).isna()] = np.nan
    """
    
    # Check if the DataFrame contains all required columns
    if not all(col in df.columns for cols in exps_columns for col in cols):
        raise ValueError("DataFrame does not contain all the required columns")
    
    # Create a list of DataFrames, one for each experiment
    df_list = [df[exp_cols] for exp_cols in exps_columns]
    
    # Calculate the row sum for each experiment (total protein abundance across channels)
    # This represents the total abundance of each protein in each experiment
    df_sums = pd.concat([exp.sum(axis=1, skipna=True) for exp in df_list], axis=1)
    df_sums.columns = [f'exp_{n+1}' for n in range(len(exps_columns))]
    
    # Compute geometric mean of sums across experiments
    # This serves as our reference value for each protein across all experiments
    # Add a small constant (1e-8) for numerical stability to avoid log(0) issues
    df_sums["gmean"] = np.exp(np.nanmean(np.log(df_sums + 1e-8), axis=1))
    
    # Calculate and apply scaling factors for each experiment
    for n, exp_cols in enumerate(exps_columns):
        # Create column name for the scaling factor
        scaling_col = f'exp_{n+1}_scaling'
        
        # Calculate scaling factor: geometric mean / experiment sum
        # This factor will adjust each experiment to match the reference level
        df_sums[scaling_col] = df_sums["gmean"] / df_sums[f'exp_{n+1}']
        
        # Apply scaling factor to all channels in the experiment
        # This preserves relative abundances within each experiment
        # while making absolute values comparable across experiments
        df_list[n] = df_list[n].multiply(df_sums[scaling_col].values, axis=0)
    
    # Combine all normalized experiments into a single DataFrame
    final_df = pd.concat(df_list, axis=1)
    
    # Apply additional total sum normalization to correct for loading differences
    # across all samples in the combined dataset
    # final_df = norm_loading_TMT(final_df)
    
    return final_df

# %% ../nbs/00_core.ipynb 19
#get only the gene id from
#the new TryTripDB format
def clean_id(temp_id):
    temp_id = temp_id.split(':')[0]
    if temp_id.count('.')>2:
        temp_id = '.'.join(temp_id.split('.')[0:3])
    return temp_id

# %% ../nbs/00_core.ipynb 20
def mod_hist_legend(ax, title=False):
    """
    Creates a cleaner legend for histogram plots by using line elements instead of patches.
    when using step
    Motivation:
    - Default histogram legends show rectangle patches which can be visually distracting
    - This function creates a more elegant legend with simple lines matching histogram edge colors
    - Positions the legend outside the plot to avoid overlapping with data
    
    Parameters:
    -----------
    ax : matplotlib.axes.Axes
        The axes object containing the histogram(s)
    title : str or bool, default=False
        Optional title for the legend. If False, no title is displayed
        
    Returns:
    --------
    None - modifies the axes object in place
    """
    # Extract the current handles and labels from the plot
    handles, labels = ax.get_legend_handles_labels()
    
    # Create new line handles that match the edge colors of histogram bars
    # This produces a cleaner, more minimal legend appearance
    new_handles = [matplotlib.lines.Line2D([], [], c=h.get_edgecolor()) for h in handles]
    
    # Create the legend with custom positioning
    # - Places legend outside the plot (to the right) to avoid obscuring the data
    # - Centers the legend vertically for better visual balance
    ax.legend(handles=new_handles, 
              labels=labels, 
              title=title,
              loc='center left', 
              bbox_to_anchor=(1, 0.5))

# %% ../nbs/00_core.ipynb 23
def clean_axes(ax, offset=10):
    """
    Customizes a matplotlib axes by removing top and right spines,
    and creating a broken axis effect where x and y axes don't touch.
    
    Parameters:
    -----------
    ax : matplotlib.axes.Axes
        The axes object to customize
    offset : int, default=10
        The amount of offset/gap between the x and y axes in points
        
    Returns:
    --------
    ax : matplotlib.axes.Axes
        The same axes object, modified in place
    """
    # Remove the top and right spines
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    
    # Make the remaining spines gray for a more subtle look
    ax.spines['left'].set_color('gray')
    ax.spines['bottom'].set_color('gray')
    
    # Create the broken axis effect
    # Move the bottom spine up by offset points
    #ax.spines['bottom'].set_position(('outward', offset))
    
    # Move the left spine right by offset points
    ax.spines['left'].set_position(('outward', offset))
    
    # Return the modified axes
    return ax

# %% ../nbs/00_core.ipynb 25
def parse_fasta_file(fasta_file):
    '''
    create a dictionary of protein id to gene product
    using fasta file from tritrypDB
    '''
    protein_dict = {}
    current_protein_id = None

    with open(fasta_file, 'r') as f:
        for line in f:
            if line.startswith('>'):
                protein_id = '.'.join(line.split('>')[1].split('.')[0:-3]).split(':')[0]
                gene_product_match = re.search(r'gene_product=([^|]+)', line)

                if  gene_product_match:
                    #protein_id = protein_id_match.group(1)
                    gene_product = gene_product_match.group(1)
                    protein_dict[protein_id] = gene_product.strip()
                    current_protein_id = protein_id
                else:
                    current_protein_id = None
    return protein_dict

def add_desc(data, prot_to_desc):
    desc = []
    for item in data.index.values:
        item_desc = []
        for prot in item.split(';'):
            clean_prot = prot.split(':')[0]
            item_desc.append(prot_to_desc.get(clean_prot,clean_prot))
        item_desc = ';'.join(item_desc)
        desc.append(item_desc)                        
    return desc

# %% ../nbs/00_core.ipynb 26
from sklearn.preprocessing import StandardScaler

def get_scaled_df(df):
    scaler = StandardScaler()
    tmp_df = np.log10(df).dropna().copy()
    tmp_df.index = [n .split(':') [0] for n in tmp_df.index.values]
    tmp_df = pd.DataFrame(scaler.fit_transform(tmp_df),
                          index=tmp_df.index,columns=tmp_df.columns)
    return tmp_df

