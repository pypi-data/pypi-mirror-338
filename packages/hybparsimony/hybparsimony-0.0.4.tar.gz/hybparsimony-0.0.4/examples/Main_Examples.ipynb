{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HYBparsimony"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***HYBparsimony*** for Python is a package **for searching accurate parsimonious models by combining feature selection (FS), model hyperparameter optimization (HO), and parsimonious model selection (PMS) based on a separate cost and complexity evaluation** ([slices-HAIS2022](https://github.com/jodivaso/hybparsimony/blob/master/docs/presentacion_HAIS2022_javi.pdf), [slices-HAIS2023](https://github.com/jodivaso/hybparsimony/blob/master/docs/presentacion_HAIS2023_javi.pdf))\n",
    "\n",
    "To improve the search for parsimony, the hybrid method combines GA mechanisms such as selection, crossover and mutation within a PSO-based optimization algorithm that includes a strategy in which the best position of each particle (thus also the best position of each neighborhood) is calculated taking into account not only the goodness-of-fit, but also the parsimony principle.\n",
    "\n",
    "In HYBparsimony, the percentage of variables to be replaced with GA at each iteration $t$ is selected by a decreasing exponential function:\n",
    " $pcrossover=max(0.80 \\cdot e^{(-\\Gamma \\cdot t)}, 0.10)$, that is adjusted by a $\\Gamma$ parameter (by default $\\Gamma$ is set to $0.50$). Thus, in the first iterations parsimony is promoted by GA mechanisms, i.e., replacing by crossover a high percentage of particles at the beginning. Subsequently, optimization with PSO becomes more relevant for the improvement of model accuracy. This differs from other hybrid methods in which the crossover is applied between the best individual position of each particle or other approaches in which the worst particles are also replaced by new particles, but at extreme positions.\n",
    "\n",
    "[Experiments with 100 datasets](https://github.com/jodivaso/hybparsimony/tree/master/examples/analysis) showed that, in general, and with a suitable $\\Gamma$, HYBparsimony allows to obtain better, more parsimonious and more robust models compared to other methods. It also reduces the number of iterations vs previous methods and, consequently, the computational effort.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Regression Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to search with *hybparsimony* package for a parsimonious (with low complexity) *KernelRidge* with *rbf* kernel model and for the *diabetes* dataset. *HYBparsimony* searches for the best input features and *KernelRidge* hyperparameters: $alpha$ and $gamma$. Models are evaluated by default with a 5-fold CV negative mean squared error (*Neg MSE*). Finally, root mean squared error (*RMSE*) is calculated with another test dataset to check the degree of model generalization.\n",
    "\n",
    "In this example, *rerank\\_error* is set to $0.001$, but other values could improve the balance between model complexity and accuracy. PMS considers the most parsimonious model with the fewest number of features. The default complexity is $M_c = 10^9{N_{FS}} + int_{comp}$  where ${N_{FS}}$ is the number of selected input features and $int_{comp}$ is the internal measure of model complexity, which depends on the algorithm used for training. In this example, $int_{comp}$ for *KernelRidge* is measured by the sum of the squared coefficients. Therefore, between two models with the same number of features, the smaller sum of the squared weights will determine the more parsimonious model (smaller weights reduce the propagation of perturbations and improve robustness).\n",
    "\n",
    "- **Note 1**: The datasets used in these examples are of small size to reduce elapsed times. These datasets have been selected in order to speed up the calculation process of the examples. With such small datasets it is necessary to use robust validation methods such as bootstrapping or repeated cross validation. It is also recommended to repeat the use of HYBparsimony with different random seeds in order to obtain more solid conclusions.\n",
    "- **Note 2***: In some examples the number of iterations ('maxiter') has been reduced as well as the use of weak validation to minimize computation times. Due to these reasons some results could not be reliable. To obtain reliable results with these datasets, it is recommended to use repeated cross-validation and increase 'maxiter'.\n",
    "- **Note 3**: The results of the examples may vary depending on the hardware available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "Detected a regression problem. Using 'neg_mean_squared_error' as default scoring function.\n",
      "Running iteration 0\n",
      "Best model -> Score = -0.510786 Complexity = 9,017,405,352.5 \n",
      "Iter = 0 -> MeanVal = -0.88274  ValBest = -0.510786   ComplexBest = 9,017,405,352.5 Time(min) = 0.015769\n",
      "\n",
      "Running iteration 1\n",
      "Best model -> Score = -0.499005 Complexity = 8,000,032,783.88 \n",
      "Iter = 1 -> MeanVal = -0.659969  ValBest = -0.499005   ComplexBest = 8,000,032,783.88 Time(min) = 0.014605\n",
      "\n",
      "Running iteration 2\n",
      "Best model -> Score = -0.498697 Complexity = 7,000,001,419.98 \n",
      "Iter = 2 -> MeanVal = -0.784296  ValBest = -0.498697   ComplexBest = 7,000,001,419.98 Time(min) = 0.010077\n",
      "\n",
      "Running iteration 3\n",
      "Best model -> Score = -0.498697 Complexity = 7,000,001,419.98 \n",
      "Iter = 3 -> MeanVal = -0.768574  ValBest = -0.498697   ComplexBest = 7,000,001,419.98 Time(min) = 0.004828\n",
      "\n",
      "Running iteration 4\n",
      "Best model -> Score = -0.492592 Complexity = 9,000,011,010.8 \n",
      "Iter = 4 -> MeanVal = -0.726017  ValBest = -0.492592   ComplexBest = 9,000,011,010.8 Time(min) = 0.004827\n",
      "\n",
      "Running iteration 5\n",
      "Best model -> Score = -0.492592 Complexity = 9,000,011,010.8 \n",
      "Iter = 5 -> MeanVal = -0.785012  ValBest = -0.515181   ComplexBest = 7,000,000,405.66 Time(min) = 0.007852\n",
      "\n",
      "Running iteration 6\n",
      "Best model -> Score = -0.492592 Complexity = 9,000,011,010.8 \n",
      "Iter = 6 -> MeanVal = -0.605981  ValBest = -0.507186   ComplexBest = 7,000,081,686.21 Time(min) = 0.009184\n",
      "\n",
      "Running iteration 7\n",
      "Best model -> Score = -0.492592 Complexity = 9,000,011,010.8 \n",
      "Iter = 7 -> MeanVal = -0.714653  ValBest = -0.499147   ComplexBest = 9,000,002,624.04 Time(min) = 0.009497\n",
      "\n",
      "Running iteration 8\n",
      "Best model -> Score = -0.492592 Complexity = 9,000,011,010.8 \n",
      "Iter = 8 -> MeanVal = -0.608436  ValBest = -0.504824   ComplexBest = 7,000,305,929.74 Time(min) = 0.009441\n",
      "\n",
      "Running iteration 9\n",
      "Best model -> Score = -0.492592 Complexity = 9,000,011,010.8 \n",
      "Iter = 9 -> MeanVal = -0.603445  ValBest = -0.502485   ComplexBest = 7,000,000,615.82 Time(min) = 0.009806\n",
      "\n",
      "Running iteration 10\n",
      "Best model -> Score = -0.49155 Complexity = 9,000,004,723.69 \n",
      "Iter = 10 -> MeanVal = -0.707835   ValBest = -0.49155   ComplexBest = 9,000,004,723.69 Time(min) = 0.008147\n",
      "\n",
      "Running iteration 11\n",
      "Best model -> Score = -0.49155 Complexity = 9,000,004,723.69 \n",
      "Iter = 11 -> MeanVal = -0.682189  ValBest = -0.492258   ComplexBest = 8,000,009,518.84 Time(min) = 0.005111\n",
      "\n",
      "Running iteration 12\n",
      "Best model -> Score = -0.49155 Complexity = 9,000,004,723.69 \n",
      "Iter = 12 -> MeanVal = -0.6184  ValBest = -0.492262   ComplexBest = 9,000,000,509.66 Time(min) = 0.005025\n",
      "\n",
      "Running iteration 13\n",
      "Best model -> Score = -0.49155 Complexity = 9,000,004,723.69 \n",
      "Iter = 13 -> MeanVal = -0.565354  ValBest = -0.494495   ComplexBest = 8,007,163,537.27 Time(min) = 0.005082\n",
      "\n",
      "Running iteration 14\n",
      "Best model -> Score = -0.49155 Complexity = 9,000,004,723.69 \n",
      "Iter = 14 -> MeanVal = -0.577446  ValBest = -0.494822   ComplexBest = 8,000,001,784.47 Time(min) = 0.005388\n",
      "\n",
      "Running iteration 15\n",
      "Best model -> Score = -0.489843 Complexity = 8,000,007,120.88 \n",
      "Iter = 15 -> MeanVal = -0.574708  ValBest = -0.489843   ComplexBest = 8,000,007,120.88 Time(min) = 0.005075\n",
      "\n",
      "Running iteration 16\n",
      "Best model -> Score = -0.489843 Complexity = 8,000,007,120.88 \n",
      "Iter = 16 -> MeanVal = -0.577152   ValBest = -0.4903    ComplexBest = 9,000,004,525.11 Time(min) = 0.006561\n",
      "\n",
      "Running iteration 17\n",
      "Best model -> Score = -0.489843 Complexity = 8,000,007,120.88 \n",
      "Iter = 17 -> MeanVal = -0.552603  ValBest = -0.491091   ComplexBest = 7,000,017,092.92 Time(min) = 0.009802\n",
      "\n",
      "Running iteration 18\n",
      "Best model -> Score = -0.489843 Complexity = 8,000,007,120.88 \n",
      "Iter = 18 -> MeanVal = -0.585651  ValBest = -0.490496   ComplexBest = 9,000,006,645.1 Time(min) = 0.010156\n",
      "\n",
      "Running iteration 19\n",
      "Best model -> Score = -0.489843 Complexity = 8,000,007,120.88 \n",
      "Iter = 19 -> MeanVal = -0.627911  ValBest = -0.495379   ComplexBest = 9,000,003,107.93 Time(min) = 0.009576\n",
      "\n",
      "Running iteration 20\n",
      "Best model -> Score = -0.489741 Complexity = 8,000,003,294.1 \n",
      "Iter = 20 -> MeanVal = -0.549074  ValBest = -0.489741   ComplexBest = 8,000,003,294.1 Time(min) = 0.009506\n",
      "\n",
      "Running iteration 21\n",
      "Best model -> Score = -0.489741 Complexity = 8,000,003,294.1 \n",
      "Iter = 21 -> MeanVal = -0.58809  ValBest = -0.490309   ComplexBest = 8,000,015,721.28 Time(min) = 0.009572\n",
      "\n",
      "Running iteration 22\n",
      "Best model -> Score = -0.489741 Complexity = 8,000,003,294.1 \n",
      "Iter = 22 -> MeanVal = -0.538231   ValBest = -0.49022   ComplexBest = 8,000,013,506.58 Time(min) = 0.006045\n",
      "\n",
      "Running iteration 23\n",
      "Best model -> Score = -0.489741 Complexity = 8,000,003,294.1 \n",
      "Iter = 23 -> MeanVal = -0.561922  ValBest = -0.489924   ComplexBest = 8,000,003,624.83 Time(min) = 0.005156\n",
      "\n",
      "Running iteration 24\n",
      "Time limit reached. Stopped.\n",
      "\n",
      "\n",
      "Best Model = KernelRidge(alpha=0.21928613615827208, gamma=0.00851452958173231, kernel='rbf')\n",
      "Selected features:['age' 'sex' 'bmi' 'bp' 's1' 's4' 's5' 's6']\n",
      "Complexity = 8,000,003,294.1\n",
      "5-CV MSE = 0.489741\n",
      "RMSE test = 0.682467\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from hybparsimony import HYBparsimony\n",
    "import os\n",
    "\n",
    "#####################################################\n",
    "#         Use sklearn regression algorithm          #\n",
    "#####################################################\n",
    "\n",
    "# Load 'diabetes' dataset\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "print(X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1234)\n",
    "\n",
    "\n",
    "# Standarize X and y (some algorithms require that)\n",
    "scaler_X = StandardScaler()\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_train = scaler_y.fit_transform(y_train.reshape(-1,1)).flatten()\n",
    "y_test = scaler_y.transform(y_test.reshape(-1,1)).flatten()\n",
    "algo = 'KernelRidge'\n",
    "HYBparsimony_model = HYBparsimony(algorithm=algo,\n",
    "                                features=diabetes.feature_names,\n",
    "                                rerank_error=0.001,\n",
    "                                n_jobs=1, #Kernel Ridge uses all cores\n",
    "                                verbose=1)\n",
    "HYBparsimony_model.fit(X_train, y_train, time_limit=0.20)\n",
    "# Check results with test dataset\n",
    "preds = HYBparsimony_model.predict(X_test)\n",
    "\n",
    "print(f'\\n\\nBest Model = {HYBparsimony_model.best_model}')\n",
    "print(f'Selected features:{HYBparsimony_model.selected_features}')\n",
    "print(f'Complexity = {round(HYBparsimony_model.best_complexity, 2):,}')\n",
    "print(f'5-CV MSE = {-round(HYBparsimony_model.best_score,6)}')\n",
    "print(f'RMSE test = {round(mean_squared_error(y_test, preds, squared=False),6)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with different algorithms...\n",
    "\n",
    "- ***Note:*** Increase *maxiter* to improve results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################\n",
      "Searching best:  Ridge\n",
      "Detected a regression problem. Using 'neg_mean_squared_error' as default scoring function.\n",
      "Running iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model -> Score = -0.49946 Complexity = 8,000,000,000.57 \n",
      "Iter = 0 -> MeanVal = -0.596448   ValBest = -0.49946   ComplexBest = 8,000,000,000.57 Time(min) = 0.01322\n",
      "\n",
      "Running iteration 1\n",
      "Best model -> Score = -0.49946 Complexity = 8,000,000,000.57 \n",
      "Iter = 1 -> MeanVal = -0.526338   ValBest = -0.49982   ComplexBest = 9,000,000,000.64 Time(min) = 0.004284\n",
      "\n",
      "Early stopping reached. Stopped.\n",
      "Ridge RMSE test 0.6954991719578381\n",
      "Selected features: ['sex' 'bmi' 'bp' 's1' 's2' 's3' 's5' 's6']\n",
      "Ridge(alpha=0.8391213911947143)\n",
      "#######################\n",
      "#######################\n",
      "Searching best:  Lasso\n",
      "Detected a regression problem. Using 'neg_mean_squared_error' as default scoring function.\n",
      "Running iteration 0\n",
      "Best model -> Score = -0.499801 Complexity = 9,000,000,000.52 \n",
      "Iter = 0 -> MeanVal = -0.778465  ValBest = -0.499801   ComplexBest = 9,000,000,000.52 Time(min) = 0.004618\n",
      "\n",
      "Running iteration 1\n",
      "Best model -> Score = -0.499801 Complexity = 9,000,000,000.52 \n",
      "Iter = 1 -> MeanVal = -0.593776  ValBest = -0.499801   ComplexBest = 9,000,000,000.52 Time(min) = 0.004541\n",
      "\n",
      "Early stopping reached. Stopped.\n",
      "Lasso RMSE test 0.6975262041906811\n",
      "Selected features: ['age' 'sex' 'bmi' 'bp' 's1' 's2' 's4' 's5' 's6']\n",
      "Lasso(alpha=4.854296278521452e-05)\n",
      "#######################\n",
      "#######################\n",
      "Searching best:  KernelRidge\n",
      "Detected a regression problem. Using 'neg_mean_squared_error' as default scoring function.\n",
      "Running iteration 0\n",
      "Best model -> Score = -0.510786 Complexity = 9,017,405,352.5 \n",
      "Iter = 0 -> MeanVal = -0.88274  ValBest = -0.510786   ComplexBest = 9,017,405,352.5 Time(min) = 0.016174\n",
      "\n",
      "Running iteration 1\n",
      "Best model -> Score = -0.499005 Complexity = 8,000,032,783.88 \n",
      "Iter = 1 -> MeanVal = -0.659969  ValBest = -0.499005   ComplexBest = 8,000,032,783.88 Time(min) = 0.00921\n",
      "\n",
      "KernelRidge RMSE test 0.6893557844133084\n",
      "Selected features: ['age' 'sex' 'bmi' 'bp' 's1' 's3' 's5' 's6']\n",
      "KernelRidge(alpha=0.07139062201784814, gamma=0.0015130607692474562,\n",
      "            kernel='rbf')\n",
      "#######################\n",
      "#######################\n",
      "Searching best:  KNeighborsRegressor\n",
      "Detected a regression problem. Using 'neg_mean_squared_error' as default scoring function.\n",
      "Running iteration 0\n",
      "Best model -> Score = -0.54155 Complexity = 8,000,041,666.67 \n",
      "Iter = 0 -> MeanVal = -0.62051   ValBest = -0.54155   ComplexBest = 8,000,041,666.67 Time(min) = 0.007545\n",
      "\n",
      "Running iteration 1\n",
      "Best model -> Score = -0.533396 Complexity = 9,000,062,500.0 \n",
      "Iter = 1 -> MeanVal = -0.573138  ValBest = -0.533396   ComplexBest = 9,000,062,500.0 Time(min) = 0.007186\n",
      "\n",
      "KNeighborsRegressor RMSE test 0.7263807801505571\n",
      "Selected features: ['age' 'sex' 'bmi' 'bp' 's1' 's2' 's3' 's5' 's6']\n",
      "KNeighborsRegressor(n_neighbors=16)\n",
      "#######################\n",
      "#######################\n",
      "Searching best:  MLPRegressor\n",
      "Detected a regression problem. Using 'neg_mean_squared_error' as default scoring function.\n",
      "Running iteration 0\n",
      "Best model -> Score = -0.505516 Complexity = 9,000,000,008.32 \n",
      "Iter = 0 -> MeanVal = -0.995194  ValBest = -0.505516   ComplexBest = 9,000,000,008.32 Time(min) = 0.182871\n",
      "\n",
      "Running iteration 1\n",
      "Best model -> Score = -0.500163 Complexity = 7,000,000,009.63 \n",
      "Iter = 1 -> MeanVal = -0.810527  ValBest = -0.500163   ComplexBest = 7,000,000,009.63 Time(min) = 0.111785\n",
      "\n",
      "MLPRegressor RMSE test 0.6683955210024939\n",
      "Selected features: ['age' 'sex' 'bmi' 'bp' 's1' 's4' 's5']\n",
      "MLPRegressor(activation='logistic', alpha=2.160843607383805,\n",
      "             hidden_layer_sizes=22, max_iter=5000, n_iter_no_change=20,\n",
      "             random_state=1234, solver='lbfgs', tol=1e-05)\n",
      "#######################\n",
      "#######################\n",
      "Searching best:  SVR\n",
      "Detected a regression problem. Using 'neg_mean_squared_error' as default scoring function.\n",
      "Running iteration 0\n",
      "Best model -> Score = -0.536835 Complexity = 9,000,000,315.0 \n",
      "Iter = 0 -> MeanVal = -0.902589  ValBest = -0.536835   ComplexBest = 9,000,000,315.0 Time(min) = 0.014235\n",
      "\n",
      "Running iteration 1\n",
      "Best model -> Score = -0.50464 Complexity = 7,000,000,308.0 \n",
      "Iter = 1 -> MeanVal = -0.617306   ValBest = -0.50464   ComplexBest = 7,000,000,308.0 Time(min) = 0.014492\n",
      "\n",
      "SVR RMSE test 0.6765703699717684\n",
      "Selected features: ['age' 'sex' 'bmi' 'bp' 's1' 's3' 's5']\n",
      "SVR(C=96.58257006419139, gamma=0.0016074955935315026)\n",
      "#######################\n",
      "#######################\n",
      "Searching best:  DecisionTreeRegressor\n",
      "Detected a regression problem. Using 'neg_mean_squared_error' as default scoring function.\n",
      "Running iteration 0\n",
      "Best model -> Score = -0.680303 Complexity = 10,000,000,004.0 \n",
      "Iter = 0 -> MeanVal = -0.868792  ValBest = -0.680303   ComplexBest = 10,000,000,004.0 Time(min) = 0.008202\n",
      "\n",
      "Running iteration 1\n",
      "Best model -> Score = -0.665165 Complexity = 9,000,000,013.0 \n",
      "Iter = 1 -> MeanVal = -0.738718  ValBest = -0.665165   ComplexBest = 9,000,000,013.0 Time(min) = 0.006923\n",
      "\n",
      "DecisionTreeRegressor RMSE test 0.8186927476666741\n",
      "Selected features: ['age' 'sex' 'bmi' 'bp' 's1' 's2' 's4' 's5' 's6']\n",
      "DecisionTreeRegressor(max_depth=4, min_samples_split=17)\n",
      "#######################\n",
      "#######################\n",
      "Searching best:  RandomForestRegressor\n",
      "Detected a regression problem. Using 'neg_mean_squared_error' as default scoring function.\n",
      "Running iteration 0\n",
      "Best model -> Score = -0.567666 Complexity = 10,000,000,021.3 \n",
      "Iter = 0 -> MeanVal = -0.584306  ValBest = -0.567666   ComplexBest = 10,000,000,021.3 Time(min) = 0.217702\n",
      "\n",
      "Running iteration 1\n",
      "Best model -> Score = -0.563765 Complexity = 9,000,000,024.0 \n",
      "Iter = 1 -> MeanVal = -0.580464  ValBest = -0.563765   ComplexBest = 9,000,000,024.0 Time(min) = 0.160237\n",
      "\n",
      "RandomForestRegressor RMSE test 0.7105678561471215\n",
      "Selected features: ['age' 'sex' 'bmi' 'bp' 's1' 's2' 's4' 's5' 's6']\n",
      "RandomForestRegressor(max_depth=14, min_samples_split=20, n_estimators=109)\n",
      "#######################\n",
      "                                          best_model   MSE_5CV      RMSE  NFS  \\\n",
      "4  MLPRegressor(activation='logistic', alpha=2.16...  0.500163  0.668396  7.0   \n",
      "5  SVR(C=96.58257006419139, gamma=0.0016074955935...  0.504640  0.676570  7.0   \n",
      "2  KernelRidge(alpha=0.07139062201784814, gamma=0...  0.499005  0.689356  8.0   \n",
      "0                    Ridge(alpha=0.8391213911947143)  0.499460  0.695499  8.0   \n",
      "1                 Lasso(alpha=4.854296278521452e-05)  0.499801  0.697526  9.0   \n",
      "7  (DecisionTreeRegressor(max_depth=14, max_featu...  0.563765  0.710568  9.0   \n",
      "3                KNeighborsRegressor(n_neighbors=16)  0.533396  0.726381  9.0   \n",
      "6  DecisionTreeRegressor(max_depth=4, min_samples...  0.665165  0.818693  9.0   \n",
      "\n",
      "                         selected_features  \n",
      "4          [age, sex, bmi, bp, s1, s4, s5]  \n",
      "5          [age, sex, bmi, bp, s1, s3, s5]  \n",
      "2      [age, sex, bmi, bp, s1, s3, s5, s6]  \n",
      "0       [sex, bmi, bp, s1, s2, s3, s5, s6]  \n",
      "1  [age, sex, bmi, bp, s1, s2, s4, s5, s6]  \n",
      "7  [age, sex, bmi, bp, s1, s2, s4, s5, s6]  \n",
      "3  [age, sex, bmi, bp, s1, s2, s3, s5, s6]  \n",
      "6  [age, sex, bmi, bp, s1, s2, s4, s5, s6]  \n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "#         Use different regression algorithms          #\n",
    "########################################################\n",
    "algorithms_reg = ['Ridge', 'Lasso', 'KernelRidge', 'KNeighborsRegressor', 'MLPRegressor', 'SVR',\n",
    "'DecisionTreeRegressor', 'RandomForestRegressor']\n",
    "res = []\n",
    "for algo in algorithms_reg:\n",
    "    print('#######################')\n",
    "    print('Searching best: ', algo)\n",
    "    HYBparsimony_model = HYBparsimony(algorithm=algo,\n",
    "                                    features=diabetes.feature_names,\n",
    "                                    maxiter=2, # 200 Extend to more iterations to improve results (time consuming)\n",
    "                                    # cv=RepeatedKFold(n_splits=5, n_repeats=10), #uncomment to improve validation (time consuming)\n",
    "                                    # n_jobs=20, # each job execute one fold\n",
    "                                    rerank_error=0.001,\n",
    "                                    verbose=1)\n",
    "    # Search the best hyperparameters and features \n",
    "    # (increasing 'time_limit' to improve RMSE with high consuming algorithms)\n",
    "    HYBparsimony_model.fit(X_train, y_train, time_limit=5)\n",
    "    # Check results with test dataset\n",
    "    preds = HYBparsimony_model.predict(X_test)\n",
    "    print(algo, \"RMSE test\", mean_squared_error(y_test, preds, squared=False))\n",
    "    print('Selected features:',HYBparsimony_model.selected_features)\n",
    "    print(HYBparsimony_model.best_model)\n",
    "    print('#######################')\n",
    "    # Append results\n",
    "    res.append(dict(algo=algo,\n",
    "                    MSE_5CV= -round(HYBparsimony_model.best_score,6),\n",
    "                    RMSE=round(mean_squared_error(y_test, preds, squared=False),6),\n",
    "                    NFS=HYBparsimony_model.best_complexity//1e9,\n",
    "                    selected_features = HYBparsimony_model.selected_features,\n",
    "                    best_model=HYBparsimony_model.best_model))\n",
    "\n",
    "res = pd.DataFrame(res).sort_values('RMSE')\n",
    "# Visualize results\n",
    "print(res[['best_model', 'MSE_5CV', 'RMSE', 'NFS', 'selected_features']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can improve results in RMSE and parsimony if we increase the time limit to 60 minutes, the maximum number of iterations to 1000, and use a more robust validation with a 10-repeated 5-fold crossvalidation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of code:\n",
    "```python\n",
    "HYBparsimony_model = HYBparsimony(algorithm=algo,\n",
    "                                   features=diabetes.feature_names,\n",
    "                                   rerank_error=0.001,\n",
    "                                   cv=RepeatedKFold(n_repeats=10, n_splits=5),\n",
    "                                   n_jobs=10, # each job executes one fold\n",
    "                                   maxiter=1000,\n",
    "                                   verbose=0)\n",
    "HYBparsimony_model.fit(X_train, y_train, time_limit=60)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: *n_jobs* represents the number of CPU cores used within the *cross_val_score()* function included in *default_cv_score()*. Also, it is important to mention that certain *scikit-learn* algorithms inherently employ parallelization as well. Thus, with some algorithms it will be necessary to consider the sharing of cores between the algorithm and the cross_val_score() function.\n",
    "\n",
    "The following table shows the best models found for each algorithm. In this case, **the model that best generalizes the problem is an ML regressor with only 6 features out of 10 and a single neuron in the hidden layer!**\n",
    "\n",
    "|Algorithm|MSE\\_10R5CV|RMSEtst|NFS|selected\\_features|best\\_model|\n",
    "|-|-|-|-|-|-|\n",
    "|**MLPRegressor**|0.493201|**0.671856**|**6**|['sex','bmi','bp','s1','s2','s5']|MLPRegressor(activation='logistic', alpha=0.010729877296924203, hidden_layer_sizes=1, max_iter=5000, n_iter_no_change=20, random_state=1234, solver='lbfgs', tol=1e-05)|\\\n",
    "|KernelRidge|0.483465|0.679036|7|['age','sex','bmi','bp','s3','s5','s6']|KernelRidge(alpha=0.3664462701238023, gamma=0.01808883688516421, kernel='rbf')|\\\n",
    "|SVR|0.487392|0.682699|8|['age','sex','bmi','bp','s1','s4','s5','s6']|SVR(C=0.8476135773996406, gamma=0.02324169209860404)|\\\n",
    "|KNeighborsRegressor|0.521326|0.687740|6|['sex','bmi','bp','s3','s5','s6']|KNeighborsRegressor(n\\_neighbors=11)|\\\n",
    "|Lasso|0.493825|0.696194|7|['sex','bmi','bp','s1','s2','s5','s6']|Lasso(alpha=0.0002735058905983914)|\\\n",
    "|Ridge|0.492570|0.696273|7|['sex','bmi','bp','s1','s2','s5','s6']|Ridge(alpha=0.1321381563140431)|\\\n",
    "|RandomForestRegressor|0.552005|0.703769|9|['age','sex','bmi','bp','s2','s3','s4','s5','s6']|RandomForestRegressor(max_depth=17, min_samples_split=25, n_estimators=473)|\\\n",
    "|DecisionTreeRegressor|0.628316|0.864194|5|['age','sex','bmi','s4','s6']|DecisionTreeRegressor(max_depth=2, min_samples_split=20)|\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to use *HYBparsimony* in a binary classification problem with *breast_cancer* dataset (30 inputs). By default, method uses *LogisticRegression* algorithm and *neg_log_loss* as scoring metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "Detected a binary-class problem. Using 'neg_log_loss' as default scoring function.\n",
      "Running iteration 0\n",
      "Best model -> Score = -0.091519 Complexity = 29,000,000,005.11 \n",
      "Iter = 0 -> MeanVal = -0.297449  ValBest = -0.091519   ComplexBest = 29,000,000,005.11 Time(min) = 0.016088\n",
      "\n",
      "Running iteration 1\n",
      "Best model -> Score = -0.085673 Complexity = 27,000,000,009.97 \n",
      "Iter = 1 -> MeanVal = -0.117216  ValBest = -0.085673   ComplexBest = 27,000,000,009.97 Time(min) = 0.010721\n",
      "\n",
      "Running iteration 2\n",
      "Best model -> Score = -0.08542 Complexity = 14,000,000,024.34 \n",
      "Iter = 2 -> MeanVal = -0.122737   ValBest = -0.08542   ComplexBest = 14,000,000,024.34 Time(min) = 0.010556\n",
      "\n",
      "Running iteration 3\n",
      "Best model -> Score = -0.085181 Complexity = 22,000,000,017.16 \n",
      "Iter = 3 -> MeanVal = -0.11499  ValBest = -0.085181   ComplexBest = 22,000,000,017.16 Time(min) = 0.011141\n",
      "\n",
      "Running iteration 4\n",
      "Best model -> Score = -0.085181 Complexity = 22,000,000,017.16 \n",
      "Iter = 4 -> MeanVal = -0.117198  ValBest = -0.086371   ComplexBest = 25,000,000,010.28 Time(min) = 0.010969\n",
      "\n",
      "Running iteration 5\n",
      "Best model -> Score = -0.08224 Complexity = 16,000,000,023.93 \n",
      "Iter = 5 -> MeanVal = -0.123552   ValBest = -0.08224   ComplexBest = 16,000,000,023.93 Time(min) = 0.010466\n",
      "\n",
      "Running iteration 6\n",
      "Best model -> Score = -0.079754 Complexity = 23,000,000,015.0 \n",
      "Iter = 6 -> MeanVal = -0.106795  ValBest = -0.079754   ComplexBest = 23,000,000,015.0 Time(min) = 0.013366\n",
      "\n",
      "Running iteration 7\n",
      "Best model -> Score = -0.07701 Complexity = 20,000,000,017.12 \n",
      "Iter = 7 -> MeanVal = -0.120019   ValBest = -0.07701   ComplexBest = 20,000,000,017.12 Time(min) = 0.012089\n",
      "\n",
      "Running iteration 8\n",
      "Best model -> Score = -0.07701 Complexity = 20,000,000,017.12 \n",
      "Iter = 8 -> MeanVal = -0.093967  ValBest = -0.080024   ComplexBest = 12,000,000,022.16 Time(min) = 0.012406\n",
      "\n",
      "Running iteration 9\n",
      "Best model -> Score = -0.076878 Complexity = 20,000,000,017.7 \n",
      "Iter = 9 -> MeanVal = -0.097546  ValBest = -0.076878   ComplexBest = 20,000,000,017.7 Time(min) = 0.011956\n",
      "\n",
      "Running iteration 10\n",
      "Best model -> Score = -0.076819 Complexity = 21,000,000,016.08 \n",
      "Iter = 10 -> MeanVal = -0.092614  ValBest = -0.076819   ComplexBest = 21,000,000,016.08 Time(min) = 0.012273\n",
      "\n",
      "Running iteration 11\n",
      "Best model -> Score = -0.076819 Complexity = 21,000,000,016.08 \n",
      "Iter = 11 -> MeanVal = -0.094542  ValBest = -0.081096   ComplexBest = 15,000,000,023.93 Time(min) = 0.01161\n",
      "\n",
      "Running iteration 12\n",
      "Best model -> Score = -0.076819 Complexity = 21,000,000,016.08 \n",
      "Iter = 12 -> MeanVal = -0.092353  ValBest = -0.077813   ComplexBest = 14,000,000,019.95 Time(min) = 0.010085\n",
      "\n",
      "Running iteration 13\n",
      "Best model -> Score = -0.076819 Complexity = 21,000,000,016.08 \n",
      "Iter = 13 -> MeanVal = -0.093167  ValBest = -0.078407   ComplexBest = 14,000,000,018.16 Time(min) = 0.012254\n",
      "\n",
      "Running iteration 14\n",
      "Best model -> Score = -0.076819 Complexity = 21,000,000,016.08 \n",
      "Iter = 14 -> MeanVal = -0.087177  ValBest = -0.077582   ComplexBest = 21,000,000,014.73 Time(min) = 0.009267\n",
      "\n",
      "Running iteration 15\n",
      "Best model -> Score = -0.076819 Complexity = 21,000,000,016.08 \n",
      "Iter = 15 -> MeanVal = -0.091306  ValBest = -0.078162   ComplexBest = 15,000,000,023.14 Time(min) = 0.00987\n",
      "\n",
      "Running iteration 16\n",
      "Best model -> Score = -0.076819 Complexity = 21,000,000,016.08 \n",
      "Iter = 16 -> MeanVal = -0.085794   ValBest = -0.07894   ComplexBest = 14,000,000,025.13 Time(min) = 0.011973\n",
      "\n",
      "Running iteration 17\n",
      "Best model -> Score = -0.075443 Complexity = 17,000,000,026.47 \n",
      "Iter = 17 -> MeanVal = -0.084607  ValBest = -0.075443   ComplexBest = 17,000,000,026.47 Time(min) = 0.01092\n",
      "\n",
      "Running iteration 18\n",
      "Best model -> Score = -0.075443 Complexity = 17,000,000,026.47 \n",
      "Iter = 18 -> MeanVal = -0.086748  ValBest = -0.077982   ComplexBest = 12,000,000,022.16 Time(min) = 0.012611\n",
      "\n",
      "Running iteration 19\n",
      "Best model -> Score = -0.075443 Complexity = 17,000,000,026.47 \n",
      "Iter = 19 -> MeanVal = -0.086437  ValBest = -0.080699   ComplexBest = 14,000,000,023.06 Time(min) = 0.012127\n",
      "\n",
      "Running iteration 20\n",
      "Best model -> Score = -0.075366 Complexity = 15,000,000,027.0 \n",
      "Iter = 20 -> MeanVal = -0.086453  ValBest = -0.075366   ComplexBest = 15,000,000,027.0 Time(min) = 0.011767\n",
      "\n",
      "Running iteration 21\n",
      "Best model -> Score = -0.07392 Complexity = 20,000,000,021.04 \n",
      "Iter = 21 -> MeanVal = -0.090097   ValBest = -0.07392   ComplexBest = 20,000,000,021.04 Time(min) = 0.012294\n",
      "\n",
      "Running iteration 22\n",
      "Best model -> Score = -0.073054 Complexity = 20,000,000,023.09 \n",
      "Iter = 22 -> MeanVal = -0.089881  ValBest = -0.073054   ComplexBest = 20,000,000,023.09 Time(min) = 0.011372\n",
      "\n",
      "Running iteration 23\n",
      "Best model -> Score = -0.073054 Complexity = 20,000,000,023.09 \n",
      "Iter = 23 -> MeanVal = -0.083651  ValBest = -0.073164   ComplexBest = 19,000,000,023.62 Time(min) = 0.012443\n",
      "\n",
      "Running iteration 24\n",
      "Best model -> Score = -0.072653 Complexity = 16,000,000,024.02 \n",
      "Iter = 24 -> MeanVal = -0.083594  ValBest = -0.072653   ComplexBest = 16,000,000,024.02 Time(min) = 0.012387\n",
      "\n",
      "Running iteration 25\n",
      "Best model -> Score = -0.072653 Complexity = 16,000,000,024.02 \n",
      "Iter = 25 -> MeanVal = -0.085285  ValBest = -0.073343   ComplexBest = 14,000,000,023.38 Time(min) = 0.012387\n",
      "\n",
      "Running iteration 26\n",
      "Best model -> Score = -0.071972 Complexity = 18,000,000,024.21 \n",
      "Iter = 26 -> MeanVal = -0.085777  ValBest = -0.071972   ComplexBest = 18,000,000,024.21 Time(min) = 0.012929\n",
      "\n",
      "Running iteration 27\n",
      "Best model -> Score = -0.071611 Complexity = 19,000,000,023.59 \n",
      "Iter = 27 -> MeanVal = -0.085935  ValBest = -0.071611   ComplexBest = 19,000,000,023.59 Time(min) = 0.012217\n",
      "\n",
      "Running iteration 28\n",
      "Best model -> Score = -0.069496 Complexity = 13,000,000,023.85 \n",
      "Iter = 28 -> MeanVal = -0.082958  ValBest = -0.069496   ComplexBest = 13,000,000,023.85 Time(min) = 0.010711\n",
      "\n",
      "Running iteration 29\n",
      "Best model -> Score = -0.069496 Complexity = 13,000,000,023.85 \n",
      "Iter = 29 -> MeanVal = -0.085811  ValBest = -0.071577   ComplexBest = 13,000,000,042.53 Time(min) = 0.011899\n",
      "\n",
      "Running iteration 30\n",
      "Best model -> Score = -0.069496 Complexity = 13,000,000,023.85 \n",
      "Iter = 30 -> MeanVal = -0.081131  ValBest = -0.070863   ComplexBest = 18,000,000,023.29 Time(min) = 0.012582\n",
      "\n",
      "Running iteration 31\n",
      "Best model -> Score = -0.067638 Complexity = 13,000,000,033.49 \n",
      "Iter = 31 -> MeanVal = -0.080697  ValBest = -0.067638   ComplexBest = 13,000,000,033.49 Time(min) = 0.012638\n",
      "\n",
      "Running iteration 32\n",
      "Best model -> Score = -0.067638 Complexity = 13,000,000,033.49 \n",
      "Iter = 32 -> MeanVal = -0.086215  ValBest = -0.069154   ComplexBest = 14,000,000,030.16 Time(min) = 0.01225\n",
      "\n",
      "Running iteration 33\n",
      "Best model -> Score = -0.067638 Complexity = 13,000,000,033.49 \n",
      "Iter = 33 -> MeanVal = -0.083789  ValBest = -0.070863   ComplexBest = 15,000,000,022.71 Time(min) = 0.012243\n",
      "\n",
      "Running iteration 34\n",
      "Best model -> Score = -0.067638 Complexity = 13,000,000,033.49 \n",
      "Iter = 34 -> MeanVal = -0.083299  ValBest = -0.072209   ComplexBest = 9,000,000,029.42 Time(min) = 0.01233\n",
      "\n",
      "Running iteration 35\n",
      "Best model -> Score = -0.067638 Complexity = 13,000,000,033.49 \n",
      "Iter = 35 -> MeanVal = -0.082192   ValBest = -0.06967   ComplexBest = 11,000,000,030.95 Time(min) = 0.012372\n",
      "\n",
      "Running iteration 36\n",
      "Best model -> Score = -0.067638 Complexity = 13,000,000,033.49 \n",
      "Iter = 36 -> MeanVal = -0.076987  ValBest = -0.068658   ComplexBest = 13,000,000,033.55 Time(min) = 0.011971\n",
      "\n",
      "Running iteration 37\n",
      "Best model -> Score = -0.067638 Complexity = 13,000,000,033.49 \n",
      "Iter = 37 -> MeanVal = -0.076757  ValBest = -0.068599   ComplexBest = 13,000,000,034.54 Time(min) = 0.012482\n",
      "\n",
      "Running iteration 38\n",
      "Best model -> Score = -0.067638 Complexity = 13,000,000,033.49 \n",
      "Iter = 38 -> MeanVal = -0.083113  ValBest = -0.070171   ComplexBest = 14,000,000,023.36 Time(min) = 0.011827\n",
      "\n",
      "Running iteration 39\n",
      "Best model -> Score = -0.067638 Complexity = 13,000,000,033.49 \n",
      "Iter = 39 -> MeanVal = -0.080043  ValBest = -0.070124   ComplexBest = 10,000,000,026.53 Time(min) = 0.012251\n",
      "\n",
      "Running iteration 40\n",
      "Best model -> Score = -0.067638 Complexity = 13,000,000,033.49 \n",
      "Iter = 40 -> MeanVal = -0.083121  ValBest = -0.070917   ComplexBest = 12,000,000,024.5 Time(min) = 0.011706\n",
      "\n",
      "Running iteration 41\n",
      "Time limit reached. Stopped.\n",
      "\n",
      "\n",
      "Best Model = LogisticRegression(C=3.770074526952063)\n",
      "Selected features:['mean texture' 'mean compactness' 'mean concave points' 'area error'\n",
      " 'compactness error' 'worst radius' 'worst texture' 'worst perimeter'\n",
      " 'worst area' 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst symmetry']\n",
      "Complexity = 13,000,000,033.49\n",
      "5-CV logloss = 0.067638\n",
      "logloss test = 0.076985\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import log_loss\n",
    "from hybparsimony import HYBparsimony\n",
    "\n",
    "# load 'breast_cancer' dataset\n",
    "breast_cancer = load_breast_cancer()\n",
    "X, y = breast_cancer.data, breast_cancer.target \n",
    "print(X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=1)\n",
    "\n",
    "# Standarize X and y (some algorithms require that)\n",
    "scaler_X = StandardScaler()\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "HYBparsimony_model = HYBparsimony(features=breast_cancer.feature_names,\n",
    "                                rerank_error=0.005,\n",
    "                                verbose=1)\n",
    "HYBparsimony_model.fit(X_train, y_train, time_limit=0.50)\n",
    "# Extract probs of class==1\n",
    "preds = HYBparsimony_model.predict_proba(X_test)[:,1]\n",
    "print(f'\\n\\nBest Model = {HYBparsimony_model.best_model}')\n",
    "print(f'Selected features:{HYBparsimony_model.selected_features}')\n",
    "print(f'Complexity = {round(HYBparsimony_model.best_complexity, 2):,}')\n",
    "print(f'5-CV logloss = {-round(HYBparsimony_model.best_score,6)}')\n",
    "print(f'logloss test = {round(log_loss(y_test, preds),6)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search the best model with different algorithms...\n",
    "\n",
    "- ***Note:*** Increase *maxiter* to improve results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################\n",
      "Searching best:  LogisticRegression\n",
      "Detected a binary-class problem. Using 'neg_log_loss' as default scoring function.\n",
      "Running iteration 0\n",
      "Best model -> Score = -0.091519 Complexity = 29,000,000,005.11 \n",
      "Iter = 0 -> MeanVal = -0.297449  ValBest = -0.091519   ComplexBest = 29,000,000,005.11 Time(min) = 0.012875\n",
      "\n",
      "Running iteration 1\n",
      "Best model -> Score = -0.085673 Complexity = 27,000,000,009.97 \n",
      "Iter = 1 -> MeanVal = -0.117216  ValBest = -0.085673   ComplexBest = 27,000,000,009.97 Time(min) = 0.009914\n",
      "\n",
      "LogisticRegression Logloss_Test= 0.080637\n",
      "Selected features: ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concave points error'\n",
      " 'symmetry error' 'fractal dimension error' 'worst radius'\n",
      " 'worst perimeter' 'worst area' 'worst smoothness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "LogisticRegression(C=0.5847346981014845)\n",
      "#######################\n",
      "#######################\n",
      "Searching best:  MLPClassifier\n",
      "Detected a binary-class problem. Using 'neg_log_loss' as default scoring function.\n",
      "Running iteration 0\n",
      "Best model -> Score = -0.080725 Complexity = 27,000,000,106.59 \n",
      "Iter = 0 -> MeanVal = -0.339892  ValBest = -0.080725   ComplexBest = 27,000,000,106.59 Time(min) = 0.027235\n",
      "\n",
      "Running iteration 1\n",
      "Best model -> Score = -0.070896 Complexity = 24,000,000,110.96 \n",
      "Iter = 1 -> MeanVal = -0.144731  ValBest = -0.070896   ComplexBest = 24,000,000,110.96 Time(min) = 0.034983\n",
      "\n",
      "MLPClassifier Logloss_Test= 0.083091\n",
      "Selected features: ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concave points'\n",
      " 'mean symmetry' 'mean fractal dimension' 'radius error' 'texture error'\n",
      " 'perimeter error' 'area error' 'smoothness error' 'compactness error'\n",
      " 'concavity error' 'symmetry error' 'worst radius' 'worst texture'\n",
      " 'worst perimeter' 'worst area' 'worst smoothness' 'worst concavity'\n",
      " 'worst fractal dimension']\n",
      "MLPClassifier(activation='logistic', alpha=0.26941768064287824,\n",
      "              hidden_layer_sizes=18, max_iter=5000, n_iter_no_change=20,\n",
      "              random_state=1234, solver='lbfgs', tol=1e-05)\n",
      "#######################\n",
      "#######################\n",
      "Searching best:  SVC\n",
      "Detected a binary-class problem. Using 'neg_log_loss' as default scoring function.\n",
      "Running iteration 0\n",
      "Best model -> Score = -0.102506 Complexity = 27,000,000,122.0 \n",
      "Iter = 0 -> MeanVal = -0.305842  ValBest = -0.102506   ComplexBest = 27,000,000,122.0 Time(min) = 0.028223\n",
      "\n",
      "Running iteration 1\n",
      "Best model -> Score = -0.092461 Complexity = 24,000,000,118.0 \n",
      "Iter = 1 -> MeanVal = -0.104228  ValBest = -0.092461   ComplexBest = 24,000,000,118.0 Time(min) = 0.020348\n",
      "\n",
      "SVC Logloss_Test= 0.103359\n",
      "Selected features: ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean concavity' 'mean concave points' 'mean symmetry' 'radius error'\n",
      " 'texture error' 'perimeter error' 'area error' 'smoothness error'\n",
      " 'compactness error' 'concave points error' 'symmetry error'\n",
      " 'fractal dimension error' 'worst radius' 'worst texture' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst fractal dimension']\n",
      "SVC(C=28.72267213984671, gamma=0.0001253364407341882, probability=True)\n",
      "#######################\n",
      "#######################\n",
      "Searching best:  DecisionTreeClassifier\n",
      "Detected a binary-class problem. Using 'neg_log_loss' as default scoring function.\n",
      "Running iteration 0\n",
      "Best model -> Score = -0.803871 Complexity = 26,000,000,006.0 \n",
      "Iter = 0 -> MeanVal = -1.68102  ValBest = -0.803871   ComplexBest = 26,000,000,006.0 Time(min) = 0.013854\n",
      "\n",
      "Running iteration 1\n",
      "Best model -> Score = -0.784302 Complexity = 23,000,000,006.0 \n",
      "Iter = 1 -> MeanVal = -1.454135  ValBest = -0.784302   ComplexBest = 23,000,000,006.0 Time(min) = 0.014012\n",
      "\n",
      "DecisionTreeClassifier Logloss_Test= 0.283898\n",
      "Selected features: ['mean texture' 'mean smoothness' 'mean concavity' 'mean concave points'\n",
      " 'mean symmetry' 'mean fractal dimension' 'radius error' 'texture error'\n",
      " 'area error' 'smoothness error' 'compactness error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst concavity' 'worst concave points'\n",
      " 'worst symmetry' 'worst fractal dimension']\n",
      "DecisionTreeClassifier(max_depth=3, min_samples_split=21)\n",
      "#######################\n",
      "#######################\n",
      "Searching best:  RandomForestClassifier\n",
      "Detected a binary-class problem. Using 'neg_log_loss' as default scoring function.\n",
      "Running iteration 0\n",
      "Best model -> Score = -0.11411 Complexity = 27,000,000,012.33 \n",
      "Iter = 0 -> MeanVal = -0.134982   ValBest = -0.11411   ComplexBest = 27,000,000,012.33 Time(min) = 0.239176\n",
      "\n",
      "Running iteration 1\n",
      "Best model -> Score = -0.110718 Complexity = 17,000,000,014.59 \n",
      "Iter = 1 -> MeanVal = -0.119905  ValBest = -0.110718   ComplexBest = 17,000,000,014.59 Time(min) = 0.246833\n",
      "\n",
      "RandomForestClassifier Logloss_Test= 0.179693\n",
      "Selected features: ['mean texture' 'mean perimeter' 'mean compactness' 'mean concavity'\n",
      " 'radius error' 'texture error' 'area error' 'compactness error'\n",
      " 'concavity error' 'concave points error' 'symmetry error' 'worst radius'\n",
      " 'worst texture' 'worst concavity' 'worst concave points' 'worst symmetry'\n",
      " 'worst fractal dimension']\n",
      "RandomForestClassifier(max_depth=7, min_samples_split=6, n_estimators=305)\n",
      "#######################\n",
      "#######################\n",
      "Searching best:  KNeighborsClassifier\n",
      "Detected a binary-class problem. Using 'neg_log_loss' as default scoring function.\n",
      "Running iteration 0\n",
      "Best model -> Score = -0.131817 Complexity = 27,000,034,482.76 \n",
      "Iter = 0 -> MeanVal = -0.213455  ValBest = -0.131817   ComplexBest = 27,000,034,482.76 Time(min) = 0.009066\n",
      "\n",
      "Running iteration 1\n",
      "Best model -> Score = -0.130211 Complexity = 21,000,047,619.05 \n",
      "Iter = 1 -> MeanVal = -0.175585  ValBest = -0.130211   ComplexBest = 21,000,047,619.05 Time(min) = 0.009483\n",
      "\n",
      "KNeighborsClassifier Logloss_Test= 0.159534\n",
      "Selected features: ['mean radius' 'mean texture' 'mean perimeter' 'mean smoothness'\n",
      " 'mean compactness' 'mean concave points' 'mean symmetry'\n",
      " 'mean fractal dimension' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'concavity error' 'concave points error'\n",
      " 'symmetry error' 'worst perimeter' 'worst area' 'worst smoothness'\n",
      " 'worst compactness' 'worst concave points' 'worst symmetry'\n",
      " 'worst fractal dimension']\n",
      "KNeighborsClassifier(n_neighbors=21)\n",
      "#######################\n",
      "                     algo  Logloss_10R5CV  Logloss_Test  NFS\n",
      "0      LogisticRegression        0.085673      0.080637   27\n",
      "1           MLPClassifier        0.070896      0.083091   24\n",
      "2                     SVC        0.092461      0.103359   24\n",
      "5    KNeighborsClassifier        0.130211      0.159534   21\n",
      "4  RandomForestClassifier        0.110718      0.179693   17\n",
      "3  DecisionTreeClassifier        0.784302      0.283898   23\n"
     ]
    }
   ],
   "source": [
    "algorithms_clas = ['LogisticRegression', 'MLPClassifier', \n",
    "                        'SVC', 'DecisionTreeClassifier',\n",
    "                        'RandomForestClassifier', 'KNeighborsClassifier',\n",
    "                        ]\n",
    "res = []\n",
    "for algo in algorithms_clas:\n",
    "    print('#######################')\n",
    "    print('Searching best: ', algo)\n",
    "    HYBparsimony_model = HYBparsimony(algorithm=algo,\n",
    "                                    features=breast_cancer.feature_names,\n",
    "                                    rerank_error=0.005,\n",
    "                                    maxiter=2, # extend to more iterations (time consuming)\n",
    "                                    # cv=RepeatedKFold(n_splits=5, n_repeats=10), #uncomment to improve validation (time consuming)\n",
    "                                    # n_jobs=20, # each job executes one fold\n",
    "                                    verbose=1)\n",
    "    # Search the best hyperparameters and features \n",
    "    # (increasing 'time_limit' to improve neg_log_loss with high consuming algorithms)\n",
    "    HYBparsimony_model.fit(X_train, y_train, time_limit=60)\n",
    "    # Check results with test dataset\n",
    "    preds = HYBparsimony_model.predict_proba(X_test)[:,1]\n",
    "    print(algo, \"Logloss_Test=\", round(log_loss(y_test, preds),6))\n",
    "    print('Selected features:',HYBparsimony_model.selected_features)\n",
    "    print(HYBparsimony_model.best_model)\n",
    "    print('#######################')\n",
    "    # Append results\n",
    "    res.append(dict(algo=algo,\n",
    "                    Logloss_10R5CV= -round(HYBparsimony_model.best_score,6),\n",
    "                    Logloss_Test = round(log_loss(y_test, preds),6),\n",
    "                    NFS=int(HYBparsimony_model.best_complexity//1e9),\n",
    "                    selected_features = HYBparsimony_model.selected_features,\n",
    "                    best_model=HYBparsimony_model.best_model))\n",
    "res = pd.DataFrame(res).sort_values('Logloss_Test')\n",
    "# Visualize results\n",
    "print(res[['algo', 'Logloss_10R5CV', 'Logloss_Test', 'NFS']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the number of classes is greather than 2, *HYBparsimony* selects *f1\\_macro* as scoring metric. In this example, we increase the number of particles to 20 with $npart=20$ and the $time\\\\_limit$ to 5 minutes. However, we also include an early stopping if best individual does not change in 20 iterations $early\\\\_stop=20$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 13)\n",
      "3\n",
      "Detected a multi-class problem. Using 'f1_macro' as default scoring function.\n",
      "Running iteration 0\n",
      "Best model -> Score = 0.981068 Complexity = 13,000,000,001.38 \n",
      "Iter = 0 -> MeanVal = 0.759953   ValBest = 0.981068   ComplexBest = 13,000,000,001.38 Time(min) = 0.063593\n",
      "\n",
      "Running iteration 1\n",
      "Best model -> Score = 0.985503 Complexity = 11,000,000,036.33 \n",
      "Iter = 1 -> MeanVal = 0.938299   ValBest = 0.985503   ComplexBest = 11,000,000,036.33 Time(min) = 0.03999\n",
      "\n",
      "Running iteration 2\n",
      "Best model -> Score = 0.98892 Complexity = 11,000,000,038.25 \n",
      "Iter = 2 -> MeanVal = 0.963618   ValBest = 0.98892    ComplexBest = 11,000,000,038.25 Time(min) = 0.03825\n",
      "\n",
      "Running iteration 3\n",
      "Best model -> Score = 0.98958 Complexity = 7,000,000,344.54 \n",
      "Iter = 3 -> MeanVal = 0.965505   ValBest = 0.98958    ComplexBest = 7,000,000,344.54 Time(min) = 0.039044\n",
      "\n",
      "Running iteration 4\n",
      "Best model -> Score = 0.98958 Complexity = 7,000,000,344.54 \n",
      "Iter = 4 -> MeanVal = 0.964892   ValBest = 0.989042   ComplexBest = 9,000,000,139.63 Time(min) = 0.039176\n",
      "\n",
      "Running iteration 5\n",
      "Best model -> Score = 0.99215 Complexity = 8,000,000,037.19 \n",
      "Iter = 5 -> MeanVal = 0.968755   ValBest = 0.99215    ComplexBest = 8,000,000,037.19 Time(min) = 0.039406\n",
      "\n",
      "Running iteration 6\n",
      "Best model -> Score = 0.992282 Complexity = 9,000,000,030.63 \n",
      "Iter = 6 -> MeanVal = 0.974364   ValBest = 0.992282   ComplexBest = 9,000,000,030.63 Time(min) = 0.039508\n",
      "\n",
      "Running iteration 7\n",
      "Best model -> Score = 0.992282 Complexity = 9,000,000,030.63 \n",
      "Iter = 7 -> MeanVal = 0.933253   ValBest = 0.990964   ComplexBest = 10,000,000,013.83 Time(min) = 0.039224\n",
      "\n",
      "Running iteration 8\n",
      "Best model -> Score = 0.993136 Complexity = 10,000,000,006.61 \n",
      "Iter = 8 -> MeanVal = 0.954792   ValBest = 0.993136   ComplexBest = 10,000,000,006.61 Time(min) = 0.038229\n",
      "\n",
      "Running iteration 9\n",
      "Best model -> Score = 0.993136 Complexity = 10,000,000,006.61 \n",
      "Iter = 9 -> MeanVal = 0.969228   ValBest = 0.992739   ComplexBest = 10,000,000,005.85 Time(min) = 0.037079\n",
      "\n",
      "Running iteration 10\n",
      "Best model -> Score = 0.993136 Complexity = 10,000,000,006.61 \n",
      "Iter = 10 -> MeanVal = 0.974959   ValBest = 0.992449   ComplexBest = 9,000,000,008.89 Time(min) = 0.037898\n",
      "\n",
      "Running iteration 11\n",
      "Best model -> Score = 0.993136 Complexity = 10,000,000,006.61 \n",
      "Iter = 11 -> MeanVal = 0.979041   ValBest = 0.992177   ComplexBest = 9,000,000,014.18 Time(min) = 0.037603\n",
      "\n",
      "Running iteration 12\n",
      "Best model -> Score = 0.993136 Complexity = 10,000,000,006.61 \n",
      "Iter = 12 -> MeanVal = 0.977748   ValBest = 0.992092   ComplexBest = 10,000,000,005.85 Time(min) = 0.037132\n",
      "\n",
      "Running iteration 13\n",
      "Best model -> Score = 0.993136 Complexity = 10,000,000,006.61 \n",
      "Iter = 13 -> MeanVal = 0.976917   ValBest = 0.991839   ComplexBest = 8,000,000,006.99 Time(min) = 0.037461\n",
      "\n",
      "Running iteration 14\n",
      "Best model -> Score = 0.993136 Complexity = 10,000,000,006.61 \n",
      "Iter = 14 -> MeanVal = 0.980287   ValBest = 0.992808   ComplexBest = 9,000,000,009.5 Time(min) = 0.038248\n",
      "\n",
      "Running iteration 15\n",
      "Best model -> Score = 0.993136 Complexity = 10,000,000,006.61 \n",
      "Iter = 15 -> MeanVal = 0.97237   ValBest = 0.992874   ComplexBest = 9,000,000,010.99 Time(min) = 0.038616\n",
      "\n",
      "Running iteration 16\n",
      "Best model -> Score = 0.993136 Complexity = 10,000,000,006.61 \n",
      "Iter = 16 -> MeanVal = 0.980541   ValBest = 0.992874   ComplexBest = 8,000,000,006.1 Time(min) = 0.037029\n",
      "\n",
      "Running iteration 17\n",
      "Best model -> Score = 0.993136 Complexity = 10,000,000,006.61 \n",
      "Iter = 17 -> MeanVal = 0.977844   ValBest = 0.992143   ComplexBest = 8,000,000,006.1 Time(min) = 0.037491\n",
      "\n",
      "Running iteration 18\n",
      "Best model -> Score = 0.994971 Complexity = 8,000,000,006.24 \n",
      "Iter = 18 -> MeanVal = 0.977124   ValBest = 0.994971   ComplexBest = 8,000,000,006.24 Time(min) = 0.036176\n",
      "\n",
      "Running iteration 19\n",
      "Best model -> Score = 0.994971 Complexity = 8,000,000,006.24 \n",
      "Iter = 19 -> MeanVal = 0.975307   ValBest = 0.992597   ComplexBest = 9,000,000,008.16 Time(min) = 0.036606\n",
      "\n",
      "Running iteration 20\n",
      "Best model -> Score = 0.994971 Complexity = 8,000,000,006.24 \n",
      "Iter = 20 -> MeanVal = 0.942464   ValBest = 0.992747   ComplexBest = 8,000,000,005.17 Time(min) = 0.036234\n",
      "\n",
      "Running iteration 21\n",
      "Best model -> Score = 0.994971 Complexity = 8,000,000,006.24 \n",
      "Iter = 21 -> MeanVal = 0.981499   ValBest = 0.99274    ComplexBest = 9,000,000,005.82 Time(min) = 0.037676\n",
      "\n",
      "Running iteration 22\n",
      "Best model -> Score = 0.994971 Complexity = 8,000,000,006.24 \n",
      "Iter = 22 -> MeanVal = 0.981864   ValBest = 0.993251   ComplexBest = 8,000,000,008.09 Time(min) = 0.037203\n",
      "\n",
      "Running iteration 23\n",
      "Best model -> Score = 0.994971 Complexity = 8,000,000,006.24 \n",
      "Iter = 23 -> MeanVal = 0.97913   ValBest = 0.992906   ComplexBest = 9,000,000,014.48 Time(min) = 0.036222\n",
      "\n",
      "Running iteration 24\n",
      "Best model -> Score = 0.994971 Complexity = 8,000,000,006.24 \n",
      "Iter = 24 -> MeanVal = 0.982498   ValBest = 0.992924   ComplexBest = 9,000,000,021.15 Time(min) = 0.036612\n",
      "\n",
      "Running iteration 25\n",
      "Best model -> Score = 0.994971 Complexity = 8,000,000,006.24 \n",
      "Iter = 25 -> MeanVal = 0.984058   ValBest = 0.993001   ComplexBest = 8,000,000,031.01 Time(min) = 0.035651\n",
      "\n",
      "Running iteration 26\n",
      "Best model -> Score = 0.994971 Complexity = 8,000,000,006.24 \n",
      "Iter = 26 -> MeanVal = 0.980005   ValBest = 0.994364   ComplexBest = 8,000,000,003.25 Time(min) = 0.03636\n",
      "\n",
      "Running iteration 27\n",
      "Best model -> Score = 0.99615 Complexity = 8,000,000,014.48 \n",
      "Iter = 27 -> MeanVal = 0.980154   ValBest = 0.99615    ComplexBest = 8,000,000,014.48 Time(min) = 0.035959\n",
      "\n",
      "Running iteration 28\n",
      "Best model -> Score = 0.99615 Complexity = 8,000,000,014.48 \n",
      "Iter = 28 -> MeanVal = 0.98119   ValBest = 0.992339   ComplexBest = 8,000,000,019.88 Time(min) = 0.036539\n",
      "\n",
      "Running iteration 29\n",
      "Best model -> Score = 0.99615 Complexity = 8,000,000,014.48 \n",
      "Iter = 29 -> MeanVal = 0.977374   ValBest = 0.99193    ComplexBest = 9,000,000,010.69 Time(min) = 0.037106\n",
      "\n",
      "Running iteration 30\n",
      "Best model -> Score = 0.99615 Complexity = 8,000,000,014.48 \n",
      "Iter = 30 -> MeanVal = 0.983235   ValBest = 0.99259    ComplexBest = 7,000,000,017.47 Time(min) = 0.037248\n",
      "\n",
      "Running iteration 31\n",
      "Best model -> Score = 0.99615 Complexity = 8,000,000,014.48 \n",
      "Iter = 31 -> MeanVal = 0.963627   ValBest = 0.992669   ComplexBest = 8,000,000,013.75 Time(min) = 0.036123\n",
      "\n",
      "Running iteration 32\n",
      "Best model -> Score = 0.99615 Complexity = 8,000,000,014.48 \n",
      "Iter = 32 -> MeanVal = 0.977936   ValBest = 0.992498   ComplexBest = 9,000,000,006.81 Time(min) = 0.0358\n",
      "\n",
      "Running iteration 33\n",
      "Best model -> Score = 0.99615 Complexity = 8,000,000,014.48 \n",
      "Iter = 33 -> MeanVal = 0.976033   ValBest = 0.992408   ComplexBest = 8,000,000,010.12 Time(min) = 0.037175\n",
      "\n",
      "Running iteration 34\n",
      "Best model -> Score = 0.99615 Complexity = 8,000,000,014.48 \n",
      "Iter = 34 -> MeanVal = 0.979166   ValBest = 0.992821   ComplexBest = 8,000,000,011.56 Time(min) = 0.037154\n",
      "\n",
      "Running iteration 35\n",
      "Best model -> Score = 0.99615 Complexity = 8,000,000,014.48 \n",
      "Iter = 35 -> MeanVal = 0.970909   ValBest = 0.992512   ComplexBest = 8,000,000,011.56 Time(min) = 0.037398\n",
      "\n",
      "Running iteration 36\n",
      "Best model -> Score = 0.99615 Complexity = 8,000,000,014.48 \n",
      "Iter = 36 -> MeanVal = 0.981434   ValBest = 0.993393   ComplexBest = 8,000,000,010.11 Time(min) = 0.035933\n",
      "\n",
      "Running iteration 37\n",
      "Best model -> Score = 0.99615 Complexity = 8,000,000,014.48 \n",
      "Iter = 37 -> MeanVal = 0.982428   ValBest = 0.992773   ComplexBest = 8,000,000,005.88 Time(min) = 0.035747\n",
      "\n",
      "Running iteration 38\n",
      "Best model -> Score = 0.99615 Complexity = 8,000,000,014.48 \n",
      "Iter = 38 -> MeanVal = 0.969823   ValBest = 0.992736   ComplexBest = 9,000,000,004.97 Time(min) = 0.036204\n",
      "\n",
      "Running iteration 39\n",
      "Best model -> Score = 0.99615 Complexity = 8,000,000,014.48 \n",
      "Iter = 39 -> MeanVal = 0.977395   ValBest = 0.992948   ComplexBest = 9,000,000,008.48 Time(min) = 0.036277\n",
      "\n",
      "Running iteration 40\n",
      "Best model -> Score = 0.99615 Complexity = 8,000,000,014.48 \n",
      "Iter = 40 -> MeanVal = 0.983263   ValBest = 0.993266   ComplexBest = 8,000,000,012.93 Time(min) = 0.036676\n",
      "\n",
      "Running iteration 41\n",
      "Best model -> Score = 0.99615 Complexity = 8,000,000,014.48 \n",
      "Iter = 41 -> MeanVal = 0.973513   ValBest = 0.991709   ComplexBest = 8,000,000,010.27 Time(min) = 0.035143\n",
      "\n",
      "Running iteration 42\n",
      "Best model -> Score = 0.99615 Complexity = 8,000,000,014.48 \n",
      "Iter = 42 -> MeanVal = 0.97073   ValBest = 0.992417   ComplexBest = 9,000,000,007.64 Time(min) = 0.036371\n",
      "\n",
      "Running iteration 43\n",
      "Best model -> Score = 0.99615 Complexity = 8,000,000,014.48 \n",
      "Iter = 43 -> MeanVal = 0.983723   ValBest = 0.994644   ComplexBest = 8,000,000,008.9 Time(min) = 0.036605\n",
      "\n",
      "Running iteration 44\n",
      "Best model -> Score = 0.99615 Complexity = 8,000,000,014.48 \n",
      "Iter = 44 -> MeanVal = 0.979896   ValBest = 0.99285    ComplexBest = 9,000,000,008.81 Time(min) = 0.036312\n",
      "\n",
      "Running iteration 45\n",
      "Best model -> Score = 0.99615 Complexity = 8,000,000,014.48 \n",
      "Iter = 45 -> MeanVal = 0.984447   ValBest = 0.992284   ComplexBest = 8,000,000,009.54 Time(min) = 0.036378\n",
      "\n",
      "Running iteration 46\n",
      "Best model -> Score = 0.99615 Complexity = 8,000,000,014.48 \n",
      "Iter = 46 -> MeanVal = 0.979013   ValBest = 0.992943   ComplexBest = 8,000,000,007.89 Time(min) = 0.035504\n",
      "\n",
      "Early stopping reached. Stopped.\n",
      "\n",
      "\n",
      "Best Model = LogisticRegression(C=1.1242464804883552)\n",
      "Selected features:['alcohol' 'ash' 'alcalinity_of_ash' 'flavanoids' 'nonflavanoid_phenols'\n",
      " 'color_intensity' 'hue' 'proline']\n",
      "Complexity = 8,000,000,014.48\n",
      "10R5-CV f1_macro = 0.99615\n",
      "f1_macro test = 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.metrics import f1_score\n",
    "from hybparsimony import HYBparsimony\n",
    "\n",
    "# load 'wine' dataset \n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target \n",
    "print(X.shape)\n",
    "# 3 classes\n",
    "print(len(np.unique(y)))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=1)\n",
    "\n",
    "# Standarize X and y (some algorithms require that)\n",
    "scaler_X = StandardScaler()\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "HYBparsimony_model = HYBparsimony(features=wine.feature_names,\n",
    "                                cv=RepeatedKFold(n_splits=5, n_repeats=10),\n",
    "                                npart = 20,\n",
    "                                early_stop=20,\n",
    "                                rerank_error=0.001,\n",
    "                                n_jobs=10, #Use 10 cores (1 core runs 1 fold)\n",
    "                                verbose=1)\n",
    "HYBparsimony_model.fit(X_train, y_train, time_limit=5.0)\n",
    "preds = HYBparsimony_model.predict(X_test)\n",
    "print(f'\\n\\nBest Model = {HYBparsimony_model.best_model}')\n",
    "print(f'Selected features:{HYBparsimony_model.selected_features}')\n",
    "print(f'Complexity = {round(HYBparsimony_model.best_complexity, 2):,}')\n",
    "print(f'10R5-CV f1_macro = {round(HYBparsimony_model.best_score,6)}')\n",
    "print(f'f1_macro test = {round(f1_score(y_test, preds, average=\"macro\"),6)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Custom Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*HYBparsimony* uses by default sklearn's [*cross_val_score*](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function as follows:\n",
    "\n",
    "```python\n",
    "def default_cv_score(estimator, X, y):\n",
    "  return cross_val_score(estimator, X, y, cv=self.cv, scoring=self.scoring)\n",
    "```\n",
    "\n",
    "By default $cv=5$, and $scoring$ is defined as *MSE* for regression problems, *log_loss* for binary classification problems, and *f1_macro* for multiclass problems. However, it is possible to choose [another scoring metric](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) defined in *scikit-learn* library or design [your own](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring). Also, the user can define a custom evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_breast_cancer, load_wine\n",
    "from sklearn.model_selection import cross_val_score, RepeatedKFold\n",
    "from hybparsimony import HYBparsimony\n",
    "from sklearn.metrics import fbeta_score, make_scorer, cohen_kappa_score, log_loss, accuracy_score\n",
    "import os\n",
    "\n",
    "# load 'breast_cancer' dataset\n",
    "breast_cancer = load_breast_cancer()\n",
    "X, y = breast_cancer.data, breast_cancer.target \n",
    "print(X.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=1)\n",
    "\n",
    "# Standarize X and y (some algorithms require that)\n",
    "scaler_X = StandardScaler()\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_test = scaler_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Example A: Using 10 folds and 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'accuracy' as scoring function.\n",
      "Running iteration 0\n",
      "Best model -> Score = 0.975845 Complexity = 29,000,000,005.11 \n",
      "Iter = 0 -> MeanVal = 0.889407   ValBest = 0.975845   ComplexBest = 29,000,000,005.11 Time(min) = 0.021194\n",
      "\n",
      "Running iteration 1\n",
      "Best model -> Score = 0.980338 Complexity = 25,000,000,009.16 \n",
      "Iter = 1 -> MeanVal = 0.96771   ValBest = 0.980338   ComplexBest = 25,000,000,009.16 Time(min) = 0.016287\n",
      "\n",
      "Running iteration 2\n",
      "Best model -> Score = 0.980338 Complexity = 25,000,000,009.16 \n",
      "Iter = 2 -> MeanVal = 0.96609   ValBest = 0.978116   ComplexBest = 14,000,000,047.55 Time(min) = 0.014893\n",
      "\n",
      "Running iteration 3\n",
      "Best model -> Score = 0.980338 Complexity = 25,000,000,009.16 \n",
      "Iter = 3 -> MeanVal = 0.962306   ValBest = 0.978116   ComplexBest = 22,000,000,011.33 Time(min) = 0.012363\n",
      "\n",
      "Running iteration 4\n",
      "Best model -> Score = 0.980338 Complexity = 25,000,000,009.16 \n",
      "Iter = 4 -> MeanVal = 0.969018   ValBest = 0.98029    ComplexBest = 21,000,000,012.79 Time(min) = 0.012556\n",
      "\n",
      "Running iteration 5\n",
      "Best model -> Score = 0.984686 Complexity = 23,000,000,013.87 \n",
      "Iter = 5 -> MeanVal = 0.972847   ValBest = 0.984686   ComplexBest = 23,000,000,013.87 Time(min) = 0.015185\n",
      "\n",
      "Running iteration 6\n",
      "Time limit reached. Stopped.\n",
      "\n",
      "\n",
      "Best Model = LogisticRegression(C=0.9707259751512046)\n",
      "Selected features:['mean radius' 'mean texture' 'mean compactness' 'mean concave points'\n",
      " 'mean fractal dimension' 'radius error' 'texture error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'fractal dimension error' 'worst radius'\n",
      " 'worst texture' 'worst perimeter' 'worst area' 'worst smoothness'\n",
      " 'worst compactness' 'worst concavity' 'worst concave points'\n",
      " 'worst symmetry' 'worst fractal dimension']\n",
      "Complexity = 23,000,000,013.87\n",
      "10R5-CV Accuracy = 0.984686\n",
      "Accuracy test = 0.964912\n"
     ]
    }
   ],
   "source": [
    "HYBparsimony_model = HYBparsimony(features=breast_cancer.feature_names,\n",
    "                                    scoring='accuracy',\n",
    "                                    cv=10,\n",
    "                                    n_jobs=10, #Use 10 cores (1 core run 1 fold)\n",
    "                                    rerank_error=0.001,\n",
    "                                    verbose=1)\n",
    "\n",
    "HYBparsimony_model.fit(X_train, y_train, time_limit=0.1)\n",
    "preds = HYBparsimony_model.predict(X_test)\n",
    "print(f'\\n\\nBest Model = {HYBparsimony_model.best_model}')\n",
    "print(f'Selected features:{HYBparsimony_model.selected_features}')\n",
    "print(f'Complexity = {round(HYBparsimony_model.best_complexity, 2):,}')\n",
    "print(f'10R5-CV Accuracy = {round(HYBparsimony_model.best_score,6)}')\n",
    "print(f'Accuracy test = {round(accuracy_score(y_test, preds),6)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Example B: Using 10-repeated 5-fold CV and 'Kappa' score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 13)\n",
      "3\n",
      "Using 'make_scorer(cohen_kappa_score)' as scoring function.\n",
      "Running iteration 0\n",
      "Best model -> Score = 0.962774 Complexity = 13,000,000,009.17 \n",
      "Iter = 0 -> MeanVal = 0.706077   ValBest = 0.962774   ComplexBest = 13,000,000,009.17 Time(min) = 0.029052\n",
      "\n",
      "Running iteration 1\n",
      "Best model -> Score = 0.9719 Complexity = 13,000,000,002.52 \n",
      "Iter = 1 -> MeanVal = 0.939917    ValBest = 0.9719    ComplexBest = 13,000,000,002.52 Time(min) = 0.029599\n",
      "\n",
      "Running iteration 2\n",
      "Best model -> Score = 0.972549 Complexity = 9,000,000,026.48 \n",
      "Iter = 2 -> MeanVal = 0.942237   ValBest = 0.972549   ComplexBest = 9,000,000,026.48 Time(min) = 0.02718\n",
      "\n",
      "Running iteration 3\n",
      "Time limit reached. Stopped.\n",
      "\n",
      "\n",
      "Best Model = LogisticRegression(C=3.136879012883974)\n",
      "Selected features:['alcohol' 'malic_acid' 'ash' 'flavanoids' 'nonflavanoid_phenols'\n",
      " 'proanthocyanins' 'hue' 'od280/od315_of_diluted_wines' 'proline']\n"
     ]
    }
   ],
   "source": [
    "# load 'wine' dataset \n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target \n",
    "print(X.shape)\n",
    "# 3 classes\n",
    "print(len(np.unique(y)))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=1)\n",
    "\n",
    "# Standarize X and y (some algorithms require that)\n",
    "scaler_X = StandardScaler()\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "\n",
    "metric_kappa = make_scorer(cohen_kappa_score, greater_is_better=True)\n",
    "HYBparsimony_model = HYBparsimony(features=wine.feature_names,\n",
    "                                scoring=metric_kappa,\n",
    "                                cv=RepeatedKFold(n_splits=5, n_repeats=10),\n",
    "                                n_jobs=10, #Use 10 cores (one core=one fold)\n",
    "                                rerank_error=0.001,\n",
    "                                verbose=1)\n",
    "HYBparsimony_model.fit(X_train, y_train, time_limit=0.1)\n",
    "print(f'\\n\\nBest Model = {HYBparsimony_model.best_model}')\n",
    "print(f'Selected features:{HYBparsimony_model.selected_features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Example C: Using a weighted 'log_loss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "Using 'make_scorer(my_custom_loss_func, greater_is_better=False, needs_proba=True)' as scoring function.\n",
      "Running iteration 0\n",
      "Best model -> Score = -0.078242 Complexity = 24,000,000,013.5 \n",
      "Iter = 0 -> MeanVal = -0.257253  ValBest = -0.078242   ComplexBest = 24,000,000,013.5 Time(min) = 0.036192\n",
      "\n",
      "Running iteration 1\n",
      "Best model -> Score = -0.073602 Complexity = 25,000,000,009.77 \n",
      "Iter = 1 -> MeanVal = -0.102049  ValBest = -0.073602   ComplexBest = 25,000,000,009.77 Time(min) = 0.015747\n",
      "\n",
      "Running iteration 2\n",
      "Best model -> Score = -0.073134 Complexity = 23,000,000,014.89 \n",
      "Iter = 2 -> MeanVal = -0.098238  ValBest = -0.073134   ComplexBest = 23,000,000,014.89 Time(min) = 0.013923\n",
      "\n",
      "Running iteration 3\n",
      "Best model -> Score = -0.069526 Complexity = 17,000,000,024.25 \n",
      "Iter = 3 -> MeanVal = -0.101725  ValBest = -0.069526   ComplexBest = 17,000,000,024.25 Time(min) = 0.015503\n",
      "\n",
      "Running iteration 4\n",
      "Best model -> Score = -0.069526 Complexity = 17,000,000,024.25 \n",
      "Iter = 4 -> MeanVal = -0.08713  ValBest = -0.071124   ComplexBest = 14,000,000,019.65 Time(min) = 0.013846\n",
      "\n",
      "Running iteration 5\n",
      "Best model -> Score = -0.067925 Complexity = 17,000,000,038.02 \n",
      "Iter = 5 -> MeanVal = -0.086964  ValBest = -0.067925   ComplexBest = 17,000,000,038.02 Time(min) = 0.013271\n",
      "\n",
      "Running iteration 6\n",
      "Best model -> Score = -0.067704 Complexity = 16,000,000,086.52 \n",
      "Iter = 6 -> MeanVal = -0.099679  ValBest = -0.067704   ComplexBest = 16,000,000,086.52 Time(min) = 0.013342\n",
      "\n",
      "Running iteration 7\n",
      "Best model -> Score = -0.063769 Complexity = 14,000,000,049.46 \n",
      "Iter = 7 -> MeanVal = -0.091679  ValBest = -0.063769   ComplexBest = 14,000,000,049.46 Time(min) = 0.016577\n",
      "\n",
      "Running iteration 8\n",
      "Best model -> Score = -0.063769 Complexity = 14,000,000,049.46 \n",
      "Iter = 8 -> MeanVal = -0.074642  ValBest = -0.067277   ComplexBest = 18,000,000,035.87 Time(min) = 0.01444\n",
      "\n",
      "Running iteration 9\n",
      "Best model -> Score = -0.06327 Complexity = 15,000,000,025.23 \n",
      "Iter = 9 -> MeanVal = -0.079702   ValBest = -0.06327   ComplexBest = 15,000,000,025.23 Time(min) = 0.015958\n",
      "\n",
      "Running iteration 10\n",
      "Best model -> Score = -0.06327 Complexity = 15,000,000,025.23 \n",
      "Iter = 10 -> MeanVal = -0.079064   ValBest = -0.06651   ComplexBest = 14,000,000,057.27 Time(min) = 0.013537\n",
      "\n",
      "Running iteration 11\n",
      "Best model -> Score = -0.06327 Complexity = 15,000,000,025.23 \n",
      "Iter = 11 -> MeanVal = -0.074218  ValBest = -0.063531   ComplexBest = 14,000,000,042.91 Time(min) = 0.013119\n",
      "\n",
      "Running iteration 12\n",
      "Time limit reached. Stopped.\n",
      "\n",
      "\n",
      "Best Model = LogisticRegression(C=2.011610055252894)\n",
      "Selected features:['mean texture' 'mean compactness' 'mean concavity' 'mean concave points'\n",
      " 'mean fractal dimension' 'radius error' 'texture error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst area' 'worst smoothness'\n",
      " 'worst symmetry']\n"
     ]
    }
   ],
   "source": [
    "# Assign a double weight to class one\n",
    "def my_custom_loss_func(y_true, y_pred):\n",
    "    sample_weight = np.ones_like(y_true)\n",
    "    sample_weight[y_true==1] = 2.0\n",
    "    return log_loss(y_true, y_pred, sample_weight=sample_weight)\n",
    "\n",
    "# load 'breast_cancer' dataset\n",
    "breast_cancer = load_breast_cancer()\n",
    "X, y = breast_cancer.data, breast_cancer.target \n",
    "print(X.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=1)\n",
    "\n",
    "# Standarize X and y (some algorithms require that)\n",
    "scaler_X = StandardScaler()\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "# Lower is better and 'log_loss' needs probabilities\n",
    "custom_score = make_scorer(my_custom_loss_func, greater_is_better=False, needs_proba=True)\n",
    "HYBparsimony_model = HYBparsimony(features=breast_cancer.feature_names,\n",
    "                                scoring=custom_score,\n",
    "                                rerank_error=0.001,\n",
    "                                verbose=1)\n",
    "HYBparsimony_model.fit(X_train, y_train, time_limit=0.20)\n",
    "print(f'\\n\\nBest Model = {HYBparsimony_model.best_model}')\n",
    "print(f'Selected features:{HYBparsimony_model.selected_features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Example D: Using a 'custom evaluation' function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running iteration 0\n",
      "Best model -> Score = 0.975824 Complexity = 29,000,000,005.11 \n",
      "Iter = 0 -> MeanVal = 0.884835   ValBest = 0.975824   ComplexBest = 29,000,000,005.11 Time(min) = 0.03483\n",
      "\n",
      "Running iteration 1\n",
      "Best model -> Score = 0.982418 Complexity = 25,000,000,013.27 \n",
      "Iter = 1 -> MeanVal = 0.965714   ValBest = 0.982418   ComplexBest = 25,000,000,013.27 Time(min) = 0.013752\n",
      "\n",
      "Running iteration 2\n",
      "Best model -> Score = 0.982418 Complexity = 25,000,000,013.27 \n",
      "Iter = 2 -> MeanVal = 0.966154   ValBest = 0.973626   ComplexBest = 20,000,000,012.68 Time(min) = 0.01234\n",
      "\n",
      "Running iteration 3\n",
      "Best model -> Score = 0.982418 Complexity = 25,000,000,013.27 \n",
      "Iter = 3 -> MeanVal = 0.966007   ValBest = 0.978022   ComplexBest = 20,000,000,013.53 Time(min) = 0.010652\n",
      "\n",
      "Running iteration 4\n",
      "Best model -> Score = 0.982418 Complexity = 25,000,000,013.27 \n",
      "Iter = 4 -> MeanVal = 0.963956   ValBest = 0.978022   ComplexBest = 16,000,000,022.01 Time(min) = 0.00968\n",
      "\n",
      "Running iteration 5\n",
      "Best model -> Score = 0.982418 Complexity = 25,000,000,013.27 \n",
      "Iter = 5 -> MeanVal = 0.965128   ValBest = 0.975824   ComplexBest = 16,000,000,014.02 Time(min) = 0.008211\n",
      "\n",
      "Running iteration 6\n",
      "Best model -> Score = 0.982418 Complexity = 25,000,000,013.27 \n",
      "Iter = 6 -> MeanVal = 0.970403   ValBest = 0.978022   ComplexBest = 22,000,000,017.24 Time(min) = 0.010469\n",
      "\n",
      "Running iteration 7\n",
      "Best model -> Score = 0.982418 Complexity = 25,000,000,013.27 \n",
      "Iter = 7 -> MeanVal = 0.969817   ValBest = 0.98022    ComplexBest = 22,000,000,013.86 Time(min) = 0.012529\n",
      "\n",
      "Running iteration 8\n",
      "Best model -> Score = 0.982418 Complexity = 17,000,000,013.79 \n",
      "Iter = 8 -> MeanVal = 0.970989   ValBest = 0.982418   ComplexBest = 17,000,000,013.79 Time(min) = 0.010208\n",
      "\n",
      "Running iteration 9\n",
      "Best model -> Score = 0.982418 Complexity = 17,000,000,013.79 \n",
      "Iter = 9 -> MeanVal = 0.97011   ValBest = 0.98022    ComplexBest = 23,000,000,017.22 Time(min) = 0.010617\n",
      "\n",
      "Running iteration 10\n",
      "Best model -> Score = 0.982418 Complexity = 17,000,000,013.79 \n",
      "Iter = 10 -> MeanVal = 0.969963   ValBest = 0.98022    ComplexBest = 20,000,000,018.18 Time(min) = 0.008894\n",
      "\n",
      "Running iteration 11\n",
      "Best model -> Score = 0.982418 Complexity = 17,000,000,013.79 \n",
      "Iter = 11 -> MeanVal = 0.97011   ValBest = 0.982418   ComplexBest = 20,000,000,015.31 Time(min) = 0.010887\n",
      "\n",
      "Running iteration 12\n",
      "Best model -> Score = 0.982418 Complexity = 17,000,000,013.79 \n",
      "Iter = 12 -> MeanVal = 0.970842   ValBest = 0.978022   ComplexBest = 22,000,000,015.2 Time(min) = 0.009707\n",
      "\n",
      "Running iteration 13\n",
      "Best model -> Score = 0.984615 Complexity = 23,000,000,010.76 \n",
      "Iter = 13 -> MeanVal = 0.973919   ValBest = 0.984615   ComplexBest = 23,000,000,010.76 Time(min) = 0.009424\n",
      "\n",
      "Running iteration 14\n",
      "Best model -> Score = 0.984615 Complexity = 22,000,000,011.59 \n",
      "Iter = 14 -> MeanVal = 0.974799   ValBest = 0.984615   ComplexBest = 22,000,000,011.59 Time(min) = 0.009125\n",
      "\n",
      "Running iteration 15\n",
      "Best model -> Score = 0.984615 Complexity = 20,000,000,011.71 \n",
      "Iter = 15 -> MeanVal = 0.970989   ValBest = 0.984615   ComplexBest = 20,000,000,011.71 Time(min) = 0.009646\n",
      "\n",
      "Running iteration 16\n",
      "Time limit reached. Stopped.\n",
      "\n",
      "\n",
      "Best Model = LogisticRegression(C=0.7244581797460722)\n",
      "Selected features:['mean texture' 'mean area' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean fractal dimension' 'radius error'\n",
      " 'area error' 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'fractal dimension error' 'worst radius'\n",
      " 'worst perimeter' 'worst area' 'worst smoothness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry']\n",
      "Complexity = 20,000,000,011.71\n",
      "10R5-CV Accuracy = 0.984615\n",
      "Accuracy test = 0.95614\n"
     ]
    }
   ],
   "source": [
    "def custom_fun(estimator, X, y):\n",
    "    return cross_val_score(estimator, X, y, scoring=\"accuracy\", n_jobs=10)\n",
    "\n",
    "HYBparsimony_model = HYBparsimony(features=breast_cancer.feature_names,\n",
    "                                custom_eval_fun=custom_fun,\n",
    "                                rerank_error=0.001,\n",
    "                                verbose=1)\n",
    "\n",
    "\n",
    "HYBparsimony_model.fit(X_train, y_train, time_limit=0.20)\n",
    "preds = HYBparsimony_model.predict(X_test)\n",
    "print(f'\\n\\nBest Model = {HYBparsimony_model.best_model}')\n",
    "print(f'Selected features:{HYBparsimony_model.selected_features}')\n",
    "print(f'Complexity = {round(HYBparsimony_model.best_complexity, 2):,}')\n",
    "print(f'10R5-CV Accuracy = {round(HYBparsimony_model.best_score,6)}')\n",
    "print(f'Accuracy test = {round(accuracy_score(y_test, preds),6)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Custom Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*HYBparsimony* has predefined the most common scikit-learn algorithms as well as functions to measure [their complexity](https://github.com/jodivaso/hybparsimony/blob/master/hybparsimony/util/complexity.py) and the [hyperparameter ranges](https://github.com/jodivaso/hybparsimony/blob/master/hybparsimony/util/models.py) to search on. However, all this can be customized. \n",
    "\n",
    "In the following example, the dictionary *MLPRegressor_new* is defined. It consists of the following properties:\n",
    "- *estimator* any machine learning algorithm compatible with scikit-learn.\n",
    "- *complexity* the function that measures the complexity of the model.\n",
    "- The hyperparameters of the algorithm. In this case, they can be fixed values (defined by Population.CONSTANT) such as '*solver*', '*activation*', etc.; or a search range $[min, max]$ defined by *{\"range\":(min, max), \"type\": Population.X}* and which type can be of three values: integer (Population.INTEGER), float (Population.FLOAT) or in powers of 10 (Population.POWER), i.e. $10^{[min, max]}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected a regression problem. Using 'neg_mean_squared_error' as default scoring function.\n",
      "Running iteration 0\n",
      "Best model -> Score = -0.553939 Complexity = 10,000,000,006.93 \n",
      "Iter = 0 -> MeanVal = -0.881378  ValBest = -0.553939   ComplexBest = 10,000,000,006.93 Time(min) = 0.096929\n",
      "\n",
      "Running iteration 1\n",
      "Best model -> Score = -0.553939 Complexity = 10,000,000,006.93 \n",
      "Iter = 1 -> MeanVal = -0.665962  ValBest = -0.559135   ComplexBest = 9,000,000,005.57 Time(min) = 0.079647\n",
      "\n",
      "Early stopping reached. Stopped.\n",
      "\n",
      "\n",
      "Best Model = MLPRegressor(activation='tanh', alpha=0.00023061450085985692,\n",
      "             early_stopping=True, hidden_layer_sizes=4,\n",
      "             learning_rate='adaptive', n_iter_no_change=20, random_state=1234,\n",
      "             tol=1e-05)\n",
      "Selected features:['age' 'sex' 'bmi' 'bp' 's1' 's2' 's3' 's4' 's5' 's6']\n",
      "Complexity = 10,000,000,006.93\n",
      "5-CV MSE = 0.553939\n",
      "RMSE test = 0.767647\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from hybparsimony import HYBparsimony, Population\n",
    "\n",
    "\n",
    "# Load 'diabetes' dataset\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "X, y = diabetes.data, diabetes.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1234)\n",
    "\n",
    "# Standarize X and y\n",
    "scaler_X = StandardScaler()\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "scaler_y = StandardScaler()\n",
    "y_train = scaler_y.fit_transform(y_train.reshape(-1,1)).flatten()\n",
    "y_test = scaler_y.transform(y_test.reshape(-1,1)).flatten()\n",
    "\n",
    "def mlp_new_complexity(model, nFeatures, **kwargs):\n",
    "    weights = [np.concatenate(model.intercepts_)]\n",
    "    for wm in model.coefs_:\n",
    "        weights.append(wm.flatten())\n",
    "    weights = np.concatenate(weights) \n",
    "    int_comp = np.min((1E09-1,np.sum(weights**2)))\n",
    "    return nFeatures*1E09 + int_comp\n",
    "\n",
    "MLPRegressor_new = {\"estimator\": MLPRegressor, # The estimator\n",
    "                \"complexity\": mlp_new_complexity, # The complexity\n",
    "                \"hidden_layer_sizes\": {\"range\": (1, 5), \"type\": Population.INTEGER},\n",
    "                \"alpha\": {\"range\": (-5, 5), \"type\": Population.POWER},\n",
    "                \"solver\": {\"value\": \"adam\", \"type\": Population.CONSTANT},\n",
    "                \"learning_rate\": {\"value\": \"adaptive\", \"type\": Population.CONSTANT},\n",
    "                \"early_stopping\": {\"value\": True, \"type\": Population.CONSTANT},\n",
    "                \"validation_fraction\": {\"value\": 0.10, \"type\": Population.CONSTANT},\n",
    "                \"activation\": {\"value\": \"tanh\", \"type\": Population.CONSTANT},\n",
    "                \"n_iter_no_change\": {\"value\": 20, \"type\": Population.CONSTANT},\n",
    "                \"tol\": {\"value\": 1e-5, \"type\": Population.CONSTANT},\n",
    "                \"random_state\": {\"value\": 1234, \"type\": Population.CONSTANT},\n",
    "                \"max_iter\": {\"value\": 200, \"type\": Population.CONSTANT}\n",
    "                }\n",
    "HYBparsimony_model = HYBparsimony(algorithm=MLPRegressor_new,\n",
    "                                features=diabetes.feature_names,\n",
    "                                cv=RepeatedKFold(n_splits=5, n_repeats=10),\n",
    "                                n_jobs= 25, #Use 25 cores (one core=one fold)\n",
    "                                maxiter=2, # Extend to more generations (time consuming)\n",
    "                                npart = 10,\n",
    "                                rerank_error=0.001,\n",
    "                                verbose=1)\n",
    "\n",
    "# Search the best hyperparameters and features \n",
    "# (increasing 'time_limit' to improve RMSE with high consuming algorithms)\n",
    "HYBparsimony_model.fit(X_train, y_train, time_limit=1.00)\n",
    "preds = HYBparsimony_model.predict(X_test)\n",
    "print(f'\\n\\nBest Model = {HYBparsimony_model.best_model}')\n",
    "print(f'Selected features:{HYBparsimony_model.selected_features}')\n",
    "print(f'Complexity = {round(HYBparsimony_model.best_complexity, 2):,}')\n",
    "print(f'5-CV MSE = {-round(HYBparsimony_model.best_score,6)}')\n",
    "print(f'RMSE test = {round(mean_squared_error(y_test, preds, squared=False),6)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Using AutoGluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This notebook](https://github.com/jodivaso/hybparsimony/blob/master/examples/Autogluon_with_SHDD.ipynb) shows **how to reduce the input features from 85 to 44 (51.7%)** of an [AutoGluon](https://auto.gluon.ai/stable/index.html) model for the COIL2000 dataset downloaded from [openml.com](https://www.openml.org/). The difference in the 'log_loss' (with a test dataset) of the model trained with the 85 features versus the 44 features **is only 0.000312**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divas�n, J., Pernia-Espinoza, A., Martinez-de-Pison, F.J. (2023). [HYB-PARSIMONY: A hybrid approach combining Particle Swarm Optimization and Genetic Algorithms to find parsimonious models in high-dimensional datasets](https://authors.elsevier.com/sd/article/S0925-2312(23)00963-3). Neurocomputing, 560, 126840.\n",
    "2023, Elsevier. [https://doi.org/10.1016/j.neucom.2023.126840](https://doi.org/10.1016/j.neucom.2023.126840)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
