{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gjentakelser i et utvalg norsk lyrikk\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Innledning \n",
    "\n",
    "I et stort utvalg dikt fra 1890-tallet finner vi at et overveldende flertall, hele 83%, er strofiske. Av disse igjen er halvparten på 4 linjer. \n",
    "\n",
    "Ettersom diktenes strofer er annotert med enderimsmønstre hver for seg, og ikke alle strofene i ett og samme dikt har fått samme rimmønster, kan vi se på fordelingen av enderimsmønstre over alle strofer, og finner at hele 1400 forskjellige rimmønstre fordeler seg utover strofene i litt over 1700 dikt. Mange av disse mønstrene forekommer bare én gang og kan dermed egentlig ikke kalles rimmønstre i tradisjonell forstand, men springer ut av annotasjonene som er blitt gjort ved hjelp av automatisk fonemisk transkripsjon og en regelbasert algoritme som annoterer verselinjer med en ny bokstav for hver linje som ikke rimer med noen foregående verselinjer i samme strofe. \n",
    "\n",
    "De vanligste mønstrene blant strofer med enderim er abcb (1169 av alle strofene, i 451 eller ca. 25% av alle diktene), abab (757 strofer, der 315 av diktene har minst en strofe med dette mønsteret), og abac (i 432 strofer, og i 234 av diktene), mens aabb kun forekommer i 312 strofer (ca X %).\n",
    "\n",
    "Om vi lar mønsteret som forekommer hyppigst i hvert dikt få bli diktets enderimsmønster, finner vi at fordelingen endrer seg litt: \n",
    "\n",
    "I 208 dikt har flest strofer enderim som følger abcb, mens det nest vanligste er abab (188 dikt). I 74 dikt er aabb det vanligste mønsteret, og abac gjelder for 62 dikt. \n",
    "\n",
    "I 751 av diktene forekommer det mest frekvente enderimsmønsteret i bare en av strofene → vi trenger mer informasjon om hvor mange strofer disse diktene har, og hva slags andre type scheme som finnes i hvert av diktene og om de er strofiske eller ikke (for å etterprøve metoden og transkripsjonen), MEN det kan være et spor for å finne alle de irregulære frie versene. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Materiale\n",
    "\n",
    "(1/2 side)\n",
    "\n",
    "NORN Dikt: metadata og oversikt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metode \n",
    "\n",
    "(maks 2 sider)\n",
    "beskrivelse av \n",
    "\n",
    "1. enderimsannotering \n",
    "2. alliterasjon \n",
    "3. anafor (ref. Rhetorical Figure Detection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse\n",
    "(6 sider, maks 8 inkl. visualisering)\n",
    "- enkeltdikt + regelbaserte algoritmer\n",
    "- visualiseringer: \n",
    "    - strofiske dikt: bilde av diktene oppå hverandre, jf. CHR 2024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konklusjon\n",
    "(1/2 side)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KODE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enderim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import poetry_analysis as pa\n",
    "\n",
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Definer datamappen\n",
    "DATADIR =  Path('/home/ingeridd/prosjekter/dikt-utforskning/data')\n",
    "\n",
    "DATADIR.exists()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annoter enderim for tekst og transkripsjoner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det første vi gjør er å lese inn diktene og annotere enderimene, og laste resultatene inn i en dataramme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lister med diktfilene\n",
    "poem_txt_files = list(DATADIR.glob('**/*.txt'))\n",
    "poem_json_files = list(DATADIR.glob('**/*østnorsk.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poetry_analysis import rhyme_detection as rd\n",
    "\n",
    "def load_rhyme_df(filepath: Path) -> pd.DataFrame:\n",
    "    filename = filepath.stem\n",
    "    pronfile = filepath.parent / f\"{filename}_østnorsk.json\"\n",
    "\n",
    "    if not pronfile.exists():\n",
    "        raise FileNotFoundError(f\"Pronunciation file {pronfile} does not exist.\")\n",
    "    \n",
    "    df_pron = pd.DataFrame(rd.tag_poem_file(pronfile)).explode(\"verses\").reset_index(drop=True)\n",
    "\n",
    "    df_text = pd.DataFrame(rd.tag_poem_file(filepath)).explode(\"verses\").reset_index(drop=True)\n",
    "\n",
    "    text_verses = df_text.verses.apply(pd.Series)\n",
    "    transcribed_verses = df_pron.verses.apply(pd.Series)\n",
    "\n",
    "    verses = transcribed_verses.drop(columns=[\"text\", \"tokens\"]).merge(text_verses[[\"text\", \"tokens\", \"last_token\", \"rhyme_tag\", \"rhymes_with\", \"rhyme_score\"]], left_index=True, right_index=True, suffixes=(\"_syll\", \"_text\"))\n",
    "\n",
    "    df = df_pron[[\"stanza_id\", \"rhyme_scheme\"]].merge(verses, left_index=True, right_index=True)\n",
    "    df = df.rename(columns={\"stanza_id\": \"stanza\", \"rhyme_scheme\": \"rhyme\", \"verse_id\": \"verse\"})\n",
    "\n",
    "    df[\"filename\"] = filename\n",
    "    df[\"poem\"] = filename.split(\"_\")[0]\n",
    "    ordered_columns = [\n",
    "        \"filename\", \"poem\",\n",
    "        'stanza', 'rhyme', 'verse', \n",
    "        'rhymes_with_text', 'rhymes_with_syll', \n",
    "        'rhyme_score_text', 'rhyme_score_syll', \n",
    "        'rhyme_tag_text', 'rhyme_tag_syll', \n",
    "        'last_token_text',  'last_token_syll', \n",
    "        'text', 'transcription', \n",
    "        'tokens', 'syllables',\n",
    "    ]\n",
    "    return df[ordered_columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m dfs = [\u001b[43mload_rhyme_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtextfile\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m textfile \u001b[38;5;129;01min\u001b[39;00m poem_txt_files]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mload_rhyme_df\u001b[39m\u001b[34m(filepath)\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPronunciation file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpronfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m df_pron = pd.DataFrame(rd.tag_poem_file(pronfile)).explode(\u001b[33m\"\u001b[39m\u001b[33mverses\u001b[39m\u001b[33m\"\u001b[39m).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m df_text = pd.DataFrame(\u001b[43mrd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtag_poem_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m).explode(\u001b[33m\"\u001b[39m\u001b[33mverses\u001b[39m\u001b[33m\"\u001b[39m).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     14\u001b[39m text_verses = df_text.verses.apply(pd.Series)\n\u001b[32m     15\u001b[39m transcribed_verses = df_pron.verses.apply(pd.Series)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/prosjekter/dikt-utforskning/.venv/lib/python3.12/site-packages/poetry_analysis/rhyme_detection.py:334\u001b[39m, in \u001b[36mtag_poem_file\u001b[39m\u001b[34m(poem_file, write_to_file)\u001b[39m\n\u001b[32m    330\u001b[39m     orthographic = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    332\u001b[39m logging.debug(\u001b[33m\"\u001b[39m\u001b[33mTagging poem: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, poem_id)\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m file_annotations = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtag_stanzas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstanzas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morthographic\u001b[49m\u001b[43m=\u001b[49m\u001b[43morthographic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m write_to_file:\n\u001b[32m    337\u001b[39m     outputfile = filepath.parent / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath.stem\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_rhyme_scheme.json\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/prosjekter/dikt-utforskning/.venv/lib/python3.12/site-packages/poetry_analysis/rhyme_detection.py:301\u001b[39m, in \u001b[36mtag_stanzas\u001b[39m\u001b[34m(stanzas, orthographic)\u001b[39m\n\u001b[32m    299\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Iterate over stanzas and tag verses with a rhyme scheme.\"\"\"\u001b[39;00m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, stanza \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(stanzas):\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m     tagged = \u001b[43mtag_rhyming_verses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstanza\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morthographic\u001b[49m\u001b[43m=\u001b[49m\u001b[43morthographic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m     rhyme_scheme = collate_rhyme_scheme(tagged)\n\u001b[32m    304\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m {\n\u001b[32m    305\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstanza_id\u001b[39m\u001b[33m\"\u001b[39m: idx,\n\u001b[32m    306\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrhyme_scheme\u001b[39m\u001b[33m\"\u001b[39m: rhyme_scheme,\n\u001b[32m    307\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mverses\u001b[39m\u001b[33m\"\u001b[39m: [verse.dict \u001b[38;5;28;01mfor\u001b[39;00m verse \u001b[38;5;129;01min\u001b[39;00m tagged],\n\u001b[32m    308\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/prosjekter/dikt-utforskning/.venv/lib/python3.12/site-packages/poetry_analysis/rhyme_detection.py:226\u001b[39m, in \u001b[36mtag_rhyming_verses\u001b[39m\u001b[34m(verses, orthographic)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m orthographic:\n\u001b[32m    225\u001b[39m     tokens = tokenize(verseline)\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m     last = tokens[-\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m utils.is_punctuation(tokens[-\u001b[32m1\u001b[39m]) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    227\u001b[39m     current_verse = Verse(\n\u001b[32m    228\u001b[39m         id_=idx,\n\u001b[32m    229\u001b[39m         text=verseline,\n\u001b[32m    230\u001b[39m         tokens=tokens,\n\u001b[32m    231\u001b[39m         last_token=last.casefold(),\n\u001b[32m    232\u001b[39m     )\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "dfs = [load_rhyme_df(textfile) for textfile in poem_txt_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Hent ut de mest brukte rimmønstrene for alle strofene \n",
    "scheme_counter = Counter([pattern for schemes in rhymepatterns.values() for pattern in schemes])\n",
    "\n",
    "scheme_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se nærmere på antall dikt som har et bestemt rimmønster\n",
    "most_common_schemes = [poem_id for poem_id, schemes in rhymepatterns.items() if \"abcd\" in schemes]\n",
    "len(most_common_schemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hent ut rimmønsteret som forekommer hyppigst i hvert dikt, og frekvensen til det rimmønsteret\n",
    "majority_schemes =  [{\"poem_id\":poem_id, \"scheme\": Counter(schemes).most_common(1)} for poem_id, schemes in rhymepatterns.items()]\n",
    "\n",
    "df = pd.DataFrame(majority_schemes)\n",
    "\n",
    "df[\"pattern\"] = df.scheme.explode()\n",
    "df[[\"scheme\", \"count\"]] = df[\"pattern\"].apply(pd.Series)\n",
    "\n",
    "df.drop(columns=[\"pattern\"], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tell antall dikt de ulike mønstrene er vanligst i\n",
    "df.scheme.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hent ut dikt som ikke har enderim, med mønsteret \"abcd\" (hver linje har en ny bokstav, dvs. ingen linjer rimer med noen foregående i samme strofe)\n",
    "# Filtrer vekk diktene der dette mønsteret ikke forekommer mer enn én gang\n",
    "df[(df.scheme == \"abcd\") & (df[\"count\"] > 1)].sort_values(by=\"count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Legg til kolonne med info om antall strofer of hvilke andre schemes som er brukt i diktet\n",
    "\n",
    "# Hent ut diktene der majoritetsrimmønsteret kun forekommer 1 gang, dvs. at det ikke er noe klart rimmønster\n",
    "df[df[\"count\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hent frekvensen til alle rimmønstrene i hvert dikt\n",
    "counted = [{\"poem_id\":poem_id, \"scheme\": Counter(schemes)} for poem_id, schemes in rhymepatterns.items()]\n",
    "\n",
    "df = pd.DataFrame(counted)\n",
    "\n",
    "pd.merge(df[\"poem_id\"], df[\"scheme\"].apply(pd.Series), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.scheme.str.len().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.scheme.str.len().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.scheme.str.len() == 330]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hvilke ord rimer oftest? \n",
    "\n",
    "Vi har annotasjonene fra rimtaggeren liggende som json-filer, og leser dem inn på nytt for å hente ut siste ord i hver linje for hvert dikt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poetry_analysis import rhyme_detection as rd \n",
    "from poetry_analysis import utils\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_stanzas_from_transcription(transcription: dict) -> list:\n",
    "    \"\"\"Parse a dict of transcribed verse lines and return a list of stanzas.\"\"\"\n",
    "    n_lines = len(transcription.keys()) - 2  # subtract the text_id and dialect keys\n",
    "    logging.debug(\"Number of lines in poem: %s\", n_lines)\n",
    "    poem = []\n",
    "    stanza = []\n",
    "    for n in range(n_lines):\n",
    "        verse = transcription.get(f\"line_{n}\")\n",
    "        if (verse is not None) and (len(verse) > 0):\n",
    "            syllables = utils.syllabify(verse)\n",
    "            stanza.append(syllables)\n",
    "        else:\n",
    "            if len(stanza) == 0:\n",
    "                continue\n",
    "            poem.append(stanza)\n",
    "            stanza = []\n",
    "    if len(poem) == 0 and len(stanza) > 0:\n",
    "        poem.append(stanza)\n",
    "    return poem\n",
    "\n",
    "\n",
    "def annotate_rhyming_patterns(poem: dict) -> list:\n",
    "    rhyming_patterns = []\n",
    "    stanzas = get_stanzas_from_transcription(poem)    \n",
    "    \n",
    "    for stanza in enumerate(stanzas):\n",
    "        tagged = rd.tag_rhyming_verses(stanza)\n",
    "        rhyme_scheme = rd.collate_rhyme_scheme(tagged)\n",
    "        rhyming_patterns.append(rhyme_scheme)\n",
    "    return rhyming_patterns\n",
    "\n",
    "\n",
    "\n",
    "def fetch_end_words(poem: dict) -> list:\n",
    "    \"\"\"Hent ordene på slutten av hver linje i hver strofe i diktet. Hver strofe er en liste av ord.\"\"\"\n",
    "    n_lines = len(poem) - 2 # subtract the text_id and dialect keys\n",
    "    end_words = []\n",
    "    stanza_words = []\n",
    "    for idx in range(n_lines):\n",
    "        line = poem.get(f\"line_{idx}\")\n",
    "        if (line is not None) and (len(line) > 0):\n",
    "            words = [word[0] for word in line]\n",
    "            stanza_words.append(words[-1])\n",
    "        else:\n",
    "            if not stanza_words: \n",
    "                continue\n",
    "            end_words.append(stanza_words)\n",
    "            stanza_words = []\n",
    "    return end_words\n",
    "\n",
    "# Hent ut rimordene fra diktene \n",
    "def fetch_rhyming_words(folder: Path) -> dict:\n",
    "    rhyming_words = {}    \n",
    "    for filename in folder.glob(\"**/*_østnorsk.json\"):\n",
    "        poem_id = filename.stem.split(\"_\")[0]\n",
    "        poem = json.loads(filename.read_text())\n",
    "        \n",
    "        poem_title = poem.get(\"text_id\")\n",
    "        try: \n",
    "            patterns = rhymepatterns[poem_id]\n",
    "        except KeyError:\n",
    "            try: \n",
    "                patterns = annotate_rhyming_patterns(poem)\n",
    "            except TypeError:\n",
    "                logging.error(\"No rhyme patterns found for poem %s\", poem_title)\n",
    "                logging.error(poem)\n",
    "                continue\n",
    "        end_words = fetch_end_words(poem)\n",
    "        rhyming_words[poem_id] = {\"title\": poem_title, \"patterns\": patterns, \"end_words\": end_words}\n",
    "    return rhyming_words\n",
    "\n",
    "\n",
    "rhyming_words = fetch_rhyming_words(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rhyming_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = Counter()\n",
    "\n",
    "for poem_id, poem in rhyming_words.items():\n",
    "    patterns = poem[\"patterns\"]\n",
    "    end_words = poem[\"end_words\"]\n",
    "\n",
    "    for pattern, word in zip(patterns, end_words):\n",
    "         # Dette er 1 strofe\n",
    "        #if pattern not in [\"abac\", \"abab\", \"aabb\", \"abcb\"]:\n",
    "        #    continue\n",
    "        rhyme_pairs = {}\n",
    "        for letter, w in zip(pattern, word):\n",
    "            # Dette er 1 linje \n",
    "           # print(poem_id, letter, w)\n",
    "            if letter in rhyme_pairs:\n",
    "                rhyme_pairs[letter].append(w)\n",
    "            else:\n",
    "                rhyme_pairs[letter] = [w]\n",
    "        #print(rhyme_pairs)\n",
    "        rhyme_pairs = [tuple(v) for _, v in rhyme_pairs.items() if len(v) > 1]\n",
    "    \n",
    "        word_pairs.update(rhyme_pairs)\n",
    "\n",
    "word_pairs.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antall rimordgrupper\n",
    "len(word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se på rimordgrupper som har flere enn 2 ord \n",
    "with Path(\"output/rimordgrupper.csv\").open(\"w\") as fp: \n",
    "    fp.write(\"word_group\\n\")\n",
    "    for key in word_pairs.keys():\n",
    "        if len(key) > 2:\n",
    "            fp.write(f\"{','.join(key)}\\n\")\n",
    "\n",
    "#[key for key in word_pairs.keys() if len(key) > 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(\"output/word_pairs.csv\").open(\"w\") as fp: \n",
    "    for key, value in word_pairs.most_common():\n",
    "        fp.write(f\"{value},{','.join(key)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
