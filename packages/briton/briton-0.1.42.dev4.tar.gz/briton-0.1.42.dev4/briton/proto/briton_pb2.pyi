"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""

import builtins
import collections.abc
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import sys
import typing

if sys.version_info >= (3, 10):
    import typing as typing_extensions
else:
    import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor

class _DataType:
    ValueType = typing.NewType("ValueType", builtins.int)
    V: typing_extensions.TypeAlias = ValueType

class _DataTypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_DataType.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    DT_INVALID: _DataType.ValueType  # 0
    DT_INT4: _DataType.ValueType  # 1
    DT_INT8: _DataType.ValueType  # 2
    DT_UINT8: _DataType.ValueType  # 3
    DT_INT32: _DataType.ValueType  # 4
    DT_INT64: _DataType.ValueType  # 5
    DT_FLOAT16: _DataType.ValueType  # 10
    """kHALF"""
    DT_BFLOAT16: _DataType.ValueType  # 11
    """kBF16"""
    DT_FLOAT32: _DataType.ValueType  # 12
    """kFLOAT"""
    DT_FP8: _DataType.ValueType  # 13
    DT_BOOL: _DataType.ValueType  # 20

class DataType(_DataType, metaclass=_DataTypeEnumTypeWrapper):
    """These correspond to nvinfer1::DataType"""

DT_INVALID: DataType.ValueType  # 0
DT_INT4: DataType.ValueType  # 1
DT_INT8: DataType.ValueType  # 2
DT_UINT8: DataType.ValueType  # 3
DT_INT32: DataType.ValueType  # 4
DT_INT64: DataType.ValueType  # 5
DT_FLOAT16: DataType.ValueType  # 10
"""kHALF"""
DT_BFLOAT16: DataType.ValueType  # 11
"""kBF16"""
DT_FLOAT32: DataType.ValueType  # 12
"""kFLOAT"""
DT_FP8: DataType.ValueType  # 13
DT_BOOL: DataType.ValueType  # 20
global___DataType = DataType

class _FinishReason:
    ValueType = typing.NewType("ValueType", builtins.int)
    V: typing_extensions.TypeAlias = ValueType

class _FinishReasonEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_FinishReason.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    NOT_FINISHED: _FinishReason.ValueType  # 0
    """/ @brief The request is not finished."""
    END_ID: _FinishReason.ValueType  # 1
    """/ @brief The request finished because the end id was generated."""
    STOP_WORDS: _FinishReason.ValueType  # 2
    """/ @brief The request finished because a stop word was generated."""
    LENGTH: _FinishReason.ValueType  # 3
    """/ @brief The request finished because the maximum number of tokens was reached."""
    TIMED_OUT: _FinishReason.ValueType  # 4
    """/ @brief The request finished because it got timed out (via the mAllotedTime parameter)"""
    CANCELLED: _FinishReason.ValueType  # 5
    """/ @brief The request was cancelled by calling cancelRequest."""

class FinishReason(_FinishReason, metaclass=_FinishReasonEnumTypeWrapper):
    """/ @brief The reason why the model stopped generating tokens for a request."""

NOT_FINISHED: FinishReason.ValueType  # 0
"""/ @brief The request is not finished."""
END_ID: FinishReason.ValueType  # 1
"""/ @brief The request finished because the end id was generated."""
STOP_WORDS: FinishReason.ValueType  # 2
"""/ @brief The request finished because a stop word was generated."""
LENGTH: FinishReason.ValueType  # 3
"""/ @brief The request finished because the maximum number of tokens was reached."""
TIMED_OUT: FinishReason.ValueType  # 4
"""/ @brief The request finished because it got timed out (via the mAllotedTime parameter)"""
CANCELLED: FinishReason.ValueType  # 5
"""/ @brief The request was cancelled by calling cancelRequest."""
global___FinishReason = FinishReason

@typing.final
class Tensor(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    @typing.final
    class Shape(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor

        DIM_FIELD_NUMBER: builtins.int
        @property
        def dim(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]: ...
        def __init__(
            self,
            *,
            dim: collections.abc.Iterable[builtins.int] | None = ...,
        ) -> None: ...
        def ClearField(self, field_name: typing.Literal["dim", b"dim"]) -> None: ...

    SHAPE_FIELD_NUMBER: builtins.int
    DTYPE_FIELD_NUMBER: builtins.int
    DATA_FIELD_NUMBER: builtins.int
    dtype: global___DataType.ValueType
    data: builtins.bytes
    @property
    def shape(self) -> global___Tensor.Shape: ...
    def __init__(
        self,
        *,
        shape: global___Tensor.Shape | None = ...,
        dtype: global___DataType.ValueType = ...,
        data: builtins.bytes = ...,
    ) -> None: ...
    def HasField(self, field_name: typing.Literal["shape", b"shape"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing.Literal["data", b"data", "dtype", b"dtype", "shape", b"shape"]) -> None: ...

global___Tensor = Tensor

@typing.final
class Batch(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    REQUEST_IDS_FIELD_NUMBER: builtins.int
    @property
    def request_ids(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]: ...
    def __init__(
        self,
        *,
        request_ids: collections.abc.Iterable[builtins.int] | None = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing.Literal["request_ids", b"request_ids"]) -> None: ...

global___Batch = Batch

@typing.final
class LookaheadDecodingConfig(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    WINDOW_SIZE_FIELD_NUMBER: builtins.int
    NGRAM_SIZE_FIELD_NUMBER: builtins.int
    VERIFICATION_SET_SIZE_FIELD_NUMBER: builtins.int
    window_size: builtins.int
    ngram_size: builtins.int
    verification_set_size: builtins.int
    def __init__(
        self,
        *,
        window_size: builtins.int = ...,
        ngram_size: builtins.int = ...,
        verification_set_size: builtins.int = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing.Literal["ngram_size", b"ngram_size", "verification_set_size", b"verification_set_size", "window_size", b"window_size"]) -> None: ...

global___LookaheadDecodingConfig = LookaheadDecodingConfig

@typing.final
class GuidedDecodingParams(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    class _GuideType:
        ValueType = typing.NewType("ValueType", builtins.int)
        V: typing_extensions.TypeAlias = ValueType

    class _GuideTypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[GuidedDecodingParams._GuideType.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        JSON: GuidedDecodingParams._GuideType.ValueType  # 0
        JSON_SCHEMA: GuidedDecodingParams._GuideType.ValueType  # 1
        REGEX: GuidedDecodingParams._GuideType.ValueType  # 2
        EBNF_GRAMMAR: GuidedDecodingParams._GuideType.ValueType  # 3

    class GuideType(_GuideType, metaclass=_GuideTypeEnumTypeWrapper): ...
    JSON: GuidedDecodingParams.GuideType.ValueType  # 0
    JSON_SCHEMA: GuidedDecodingParams.GuideType.ValueType  # 1
    REGEX: GuidedDecodingParams.GuideType.ValueType  # 2
    EBNF_GRAMMAR: GuidedDecodingParams.GuideType.ValueType  # 3

    GUIDE_TYPE_FIELD_NUMBER: builtins.int
    GUIDE_FIELD_NUMBER: builtins.int
    guide_type: global___GuidedDecodingParams.GuideType.ValueType
    guide: builtins.str
    def __init__(
        self,
        *,
        guide_type: global___GuidedDecodingParams.GuideType.ValueType = ...,
        guide: builtins.str | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing.Literal["_guide", b"_guide", "guide", b"guide"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing.Literal["_guide", b"_guide", "guide", b"guide", "guide_type", b"guide_type"]) -> None: ...
    def WhichOneof(self, oneof_group: typing.Literal["_guide", b"_guide"]) -> typing.Literal["guide"] | None: ...

global___GuidedDecodingParams = GuidedDecodingParams

@typing.final
class InferenceRequest(google.protobuf.message.Message):
    """The request message containing the user's name."""

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    REQUEST_ID_FIELD_NUMBER: builtins.int
    INPUT_TEXT_FIELD_NUMBER: builtins.int
    INPUT_IDS_FIELD_NUMBER: builtins.int
    REQUEST_OUTPUT_LEN_FIELD_NUMBER: builtins.int
    END_ID_FIELD_NUMBER: builtins.int
    PAD_ID_FIELD_NUMBER: builtins.int
    BEAM_WIDTH_FIELD_NUMBER: builtins.int
    TEMPERATURE_FIELD_NUMBER: builtins.int
    RUNTIME_TOP_K_FIELD_NUMBER: builtins.int
    RUNTIME_TOP_P_FIELD_NUMBER: builtins.int
    LEN_PENALTY_FIELD_NUMBER: builtins.int
    REPETITION_PENALTY_FIELD_NUMBER: builtins.int
    PRESENCE_PENALTY_FIELD_NUMBER: builtins.int
    FREQUENCY_PENALTY_FIELD_NUMBER: builtins.int
    LOGPROBS_FIELD_NUMBER: builtins.int
    TOP_LOGPROBS_FIELD_NUMBER: builtins.int
    BAD_WORDS_FIELD_NUMBER: builtins.int
    STOP_WORDS_FIELD_NUMBER: builtins.int
    LORA_TASK_ID_FIELD_NUMBER: builtins.int
    LORA_WEIGHTS_FIELD_NUMBER: builtins.int
    LORA_CONFIG_FIELD_NUMBER: builtins.int
    RANDOM_SEED_FIELD_NUMBER: builtins.int
    DRAFT_INPUT_IDS_FIELD_NUMBER: builtins.int
    BATCH_FIELD_NUMBER: builtins.int
    LOOKAHEAD_DECODING_CONFIG_FIELD_NUMBER: builtins.int
    GUIDED_DECODING_PARAMS_FIELD_NUMBER: builtins.int
    request_id: builtins.int
    input_text: builtins.str
    """One of input_text or input_ids should be provided.
    If input_ids is provided then output_ids are returned.
    """
    request_output_len: builtins.int
    end_id: builtins.int
    pad_id: builtins.int
    beam_width: builtins.int
    """Logit selection related"""
    temperature: builtins.float
    runtime_top_k: builtins.int
    runtime_top_p: builtins.float
    len_penalty: builtins.float
    repetition_penalty: builtins.float
    presence_penalty: builtins.float
    frequency_penalty: builtins.float
    logprobs: builtins.bool
    top_logprobs: builtins.int
    lora_task_id: builtins.int
    random_seed: builtins.int
    @property
    def input_ids(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]:
        """TODO(pankaj) Implement this. Not supported now, input_text
        should be provided.
        """

    @property
    def bad_words(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.str]: ...
    @property
    def stop_words(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.str]: ...
    @property
    def lora_weights(self) -> global___Tensor: ...
    @property
    def lora_config(self) -> global___Tensor: ...
    @property
    def draft_input_ids(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]: ...
    @property
    def batch(self) -> global___Batch:
        """If request is part of batch, then try to wait for other requests"""

    @property
    def lookahead_decoding_config(self) -> global___LookaheadDecodingConfig: ...
    @property
    def guided_decoding_params(self) -> global___GuidedDecodingParams: ...
    def __init__(
        self,
        *,
        request_id: builtins.int = ...,
        input_text: builtins.str | None = ...,
        input_ids: collections.abc.Iterable[builtins.int] | None = ...,
        request_output_len: builtins.int | None = ...,
        end_id: builtins.int | None = ...,
        pad_id: builtins.int | None = ...,
        beam_width: builtins.int | None = ...,
        temperature: builtins.float | None = ...,
        runtime_top_k: builtins.int | None = ...,
        runtime_top_p: builtins.float | None = ...,
        len_penalty: builtins.float | None = ...,
        repetition_penalty: builtins.float | None = ...,
        presence_penalty: builtins.float | None = ...,
        frequency_penalty: builtins.float | None = ...,
        logprobs: builtins.bool | None = ...,
        top_logprobs: builtins.int | None = ...,
        bad_words: collections.abc.Iterable[builtins.str] | None = ...,
        stop_words: collections.abc.Iterable[builtins.str] | None = ...,
        lora_task_id: builtins.int | None = ...,
        lora_weights: global___Tensor | None = ...,
        lora_config: global___Tensor | None = ...,
        random_seed: builtins.int | None = ...,
        draft_input_ids: collections.abc.Iterable[builtins.int] | None = ...,
        batch: global___Batch | None = ...,
        lookahead_decoding_config: global___LookaheadDecodingConfig | None = ...,
        guided_decoding_params: global___GuidedDecodingParams | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing.Literal["_batch", b"_batch", "_beam_width", b"_beam_width", "_end_id", b"_end_id", "_frequency_penalty", b"_frequency_penalty", "_guided_decoding_params", b"_guided_decoding_params", "_input_text", b"_input_text", "_len_penalty", b"_len_penalty", "_logprobs", b"_logprobs", "_lookahead_decoding_config", b"_lookahead_decoding_config", "_lora_config", b"_lora_config", "_lora_task_id", b"_lora_task_id", "_lora_weights", b"_lora_weights", "_pad_id", b"_pad_id", "_presence_penalty", b"_presence_penalty", "_random_seed", b"_random_seed", "_repetition_penalty", b"_repetition_penalty", "_request_output_len", b"_request_output_len", "_runtime_top_k", b"_runtime_top_k", "_runtime_top_p", b"_runtime_top_p", "_temperature", b"_temperature", "_top_logprobs", b"_top_logprobs", "batch", b"batch", "beam_width", b"beam_width", "end_id", b"end_id", "frequency_penalty", b"frequency_penalty", "guided_decoding_params", b"guided_decoding_params", "input_text", b"input_text", "len_penalty", b"len_penalty", "logprobs", b"logprobs", "lookahead_decoding_config", b"lookahead_decoding_config", "lora_config", b"lora_config", "lora_task_id", b"lora_task_id", "lora_weights", b"lora_weights", "pad_id", b"pad_id", "presence_penalty", b"presence_penalty", "random_seed", b"random_seed", "repetition_penalty", b"repetition_penalty", "request_output_len", b"request_output_len", "runtime_top_k", b"runtime_top_k", "runtime_top_p", b"runtime_top_p", "temperature", b"temperature", "top_logprobs", b"top_logprobs"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing.Literal["_batch", b"_batch", "_beam_width", b"_beam_width", "_end_id", b"_end_id", "_frequency_penalty", b"_frequency_penalty", "_guided_decoding_params", b"_guided_decoding_params", "_input_text", b"_input_text", "_len_penalty", b"_len_penalty", "_logprobs", b"_logprobs", "_lookahead_decoding_config", b"_lookahead_decoding_config", "_lora_config", b"_lora_config", "_lora_task_id", b"_lora_task_id", "_lora_weights", b"_lora_weights", "_pad_id", b"_pad_id", "_presence_penalty", b"_presence_penalty", "_random_seed", b"_random_seed", "_repetition_penalty", b"_repetition_penalty", "_request_output_len", b"_request_output_len", "_runtime_top_k", b"_runtime_top_k", "_runtime_top_p", b"_runtime_top_p", "_temperature", b"_temperature", "_top_logprobs", b"_top_logprobs", "bad_words", b"bad_words", "batch", b"batch", "beam_width", b"beam_width", "draft_input_ids", b"draft_input_ids", "end_id", b"end_id", "frequency_penalty", b"frequency_penalty", "guided_decoding_params", b"guided_decoding_params", "input_ids", b"input_ids", "input_text", b"input_text", "len_penalty", b"len_penalty", "logprobs", b"logprobs", "lookahead_decoding_config", b"lookahead_decoding_config", "lora_config", b"lora_config", "lora_task_id", b"lora_task_id", "lora_weights", b"lora_weights", "pad_id", b"pad_id", "presence_penalty", b"presence_penalty", "random_seed", b"random_seed", "repetition_penalty", b"repetition_penalty", "request_id", b"request_id", "request_output_len", b"request_output_len", "runtime_top_k", b"runtime_top_k", "runtime_top_p", b"runtime_top_p", "stop_words", b"stop_words", "temperature", b"temperature", "top_logprobs", b"top_logprobs"]) -> None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_batch", b"_batch"]) -> typing.Literal["batch"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_beam_width", b"_beam_width"]) -> typing.Literal["beam_width"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_end_id", b"_end_id"]) -> typing.Literal["end_id"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_frequency_penalty", b"_frequency_penalty"]) -> typing.Literal["frequency_penalty"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_guided_decoding_params", b"_guided_decoding_params"]) -> typing.Literal["guided_decoding_params"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_input_text", b"_input_text"]) -> typing.Literal["input_text"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_len_penalty", b"_len_penalty"]) -> typing.Literal["len_penalty"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_logprobs", b"_logprobs"]) -> typing.Literal["logprobs"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_lookahead_decoding_config", b"_lookahead_decoding_config"]) -> typing.Literal["lookahead_decoding_config"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_lora_config", b"_lora_config"]) -> typing.Literal["lora_config"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_lora_task_id", b"_lora_task_id"]) -> typing.Literal["lora_task_id"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_lora_weights", b"_lora_weights"]) -> typing.Literal["lora_weights"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_pad_id", b"_pad_id"]) -> typing.Literal["pad_id"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_presence_penalty", b"_presence_penalty"]) -> typing.Literal["presence_penalty"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_random_seed", b"_random_seed"]) -> typing.Literal["random_seed"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_repetition_penalty", b"_repetition_penalty"]) -> typing.Literal["repetition_penalty"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_request_output_len", b"_request_output_len"]) -> typing.Literal["request_output_len"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_runtime_top_k", b"_runtime_top_k"]) -> typing.Literal["runtime_top_k"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_runtime_top_p", b"_runtime_top_p"]) -> typing.Literal["runtime_top_p"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_temperature", b"_temperature"]) -> typing.Literal["temperature"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_top_logprobs", b"_top_logprobs"]) -> typing.Literal["top_logprobs"] | None: ...

global___InferenceRequest = InferenceRequest

@typing.final
class InferenceAnswerPart(google.protobuf.message.Message):
    """The response message containing the greetings."""

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    REQUEST_ID_FIELD_NUMBER: builtins.int
    OUTPUT_TEXT_FIELD_NUMBER: builtins.int
    OUTPUT_IDS_FIELD_NUMBER: builtins.int
    FINISH_REASON_FIELD_NUMBER: builtins.int
    TOP_LOGPROBS_FIELD_NUMBER: builtins.int
    request_id: builtins.int
    output_text: builtins.str
    finish_reason: global___FinishReason.ValueType
    """The reason why token generation stopped"""
    @property
    def output_ids(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]: ...
    @property
    def top_logprobs(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___TopLogProbs]: ...
    def __init__(
        self,
        *,
        request_id: builtins.int = ...,
        output_text: builtins.str = ...,
        output_ids: collections.abc.Iterable[builtins.int] | None = ...,
        finish_reason: global___FinishReason.ValueType | None = ...,
        top_logprobs: collections.abc.Iterable[global___TopLogProbs] | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing.Literal["_finish_reason", b"_finish_reason", "finish_reason", b"finish_reason"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing.Literal["_finish_reason", b"_finish_reason", "finish_reason", b"finish_reason", "output_ids", b"output_ids", "output_text", b"output_text", "request_id", b"request_id", "top_logprobs", b"top_logprobs"]) -> None: ...
    def WhichOneof(self, oneof_group: typing.Literal["_finish_reason", b"_finish_reason"]) -> typing.Literal["finish_reason"] | None: ...

global___InferenceAnswerPart = InferenceAnswerPart

@typing.final
class TopLogProbs(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    @typing.final
    class LogprobsEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor

        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: builtins.int
        value: builtins.float
        def __init__(
            self,
            *,
            key: builtins.int = ...,
            value: builtins.float = ...,
        ) -> None: ...
        def ClearField(self, field_name: typing.Literal["key", b"key", "value", b"value"]) -> None: ...

    LOGPROB_FIELD_NUMBER: builtins.int
    LOGPROBS_FIELD_NUMBER: builtins.int
    logprob: builtins.float
    """The log probability of the token."""
    @property
    def logprobs(self) -> google.protobuf.internal.containers.ScalarMap[builtins.int, builtins.float]:
        """The log probabilities of the top k tokens."""

    def __init__(
        self,
        *,
        logprob: builtins.float = ...,
        logprobs: collections.abc.Mapping[builtins.int, builtins.float] | None = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing.Literal["logprob", b"logprob", "logprobs", b"logprobs"]) -> None: ...

global___TopLogProbs = TopLogProbs

@typing.final
class AddedToken(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    CONTENT_FIELD_NUMBER: builtins.int
    SINGLE_WORD_FIELD_NUMBER: builtins.int
    LSTRIP_FIELD_NUMBER: builtins.int
    RSTRIP_FIELD_NUMBER: builtins.int
    NORMALIZED_FIELD_NUMBER: builtins.int
    SPECIAL_FIELD_NUMBER: builtins.int
    content: builtins.str
    single_word: builtins.bool
    lstrip: builtins.bool
    rstrip: builtins.bool
    normalized: builtins.bool
    special: builtins.bool
    def __init__(
        self,
        *,
        content: builtins.str | None = ...,
        single_word: builtins.bool | None = ...,
        lstrip: builtins.bool | None = ...,
        rstrip: builtins.bool | None = ...,
        normalized: builtins.bool | None = ...,
        special: builtins.bool | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing.Literal["_content", b"_content", "_lstrip", b"_lstrip", "_normalized", b"_normalized", "_rstrip", b"_rstrip", "_single_word", b"_single_word", "_special", b"_special", "content", b"content", "lstrip", b"lstrip", "normalized", b"normalized", "rstrip", b"rstrip", "single_word", b"single_word", "special", b"special"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing.Literal["_content", b"_content", "_lstrip", b"_lstrip", "_normalized", b"_normalized", "_rstrip", b"_rstrip", "_single_word", b"_single_word", "_special", b"_special", "content", b"content", "lstrip", b"lstrip", "normalized", b"normalized", "rstrip", b"rstrip", "single_word", b"single_word", "special", b"special"]) -> None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_content", b"_content"]) -> typing.Literal["content"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_lstrip", b"_lstrip"]) -> typing.Literal["lstrip"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_normalized", b"_normalized"]) -> typing.Literal["normalized"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_rstrip", b"_rstrip"]) -> typing.Literal["rstrip"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_single_word", b"_single_word"]) -> typing.Literal["single_word"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_special", b"_special"]) -> typing.Literal["special"] | None: ...

global___AddedToken = AddedToken

@typing.final
class AddedTokens(google.protobuf.message.Message):
    """These mirror the added tokens concept in HF PreTrainedTokenizerFast. Most
    tokens are defined in tokenizer.json and are automatically picked up by the
    rust tokenizer. But those defined outside, as in tokenizer_config.json and
    special_tokens_map.json, need to be added manually. Those additional tokens
    are supplied at startup here.
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    TOKENS_FIELD_NUMBER: builtins.int
    @property
    def tokens(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___AddedToken]: ...
    def __init__(
        self,
        *,
        tokens: collections.abc.Iterable[global___AddedToken] | None = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing.Literal["tokens", b"tokens"]) -> None: ...

global___AddedTokens = AddedTokens

@typing.final
class XGrammarConfig(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    ENCODED_VOCAB_FIELD_NUMBER: builtins.int
    TOKENIZER_STR_FIELD_NUMBER: builtins.int
    STOP_TOKEN_IDS_FIELD_NUMBER: builtins.int
    tokenizer_str: builtins.str
    @property
    def encoded_vocab(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.str]: ...
    @property
    def stop_token_ids(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]: ...
    def __init__(
        self,
        *,
        encoded_vocab: collections.abc.Iterable[builtins.str] | None = ...,
        tokenizer_str: builtins.str = ...,
        stop_token_ids: collections.abc.Iterable[builtins.int] | None = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing.Literal["encoded_vocab", b"encoded_vocab", "stop_token_ids", b"stop_token_ids", "tokenizer_str", b"tokenizer_str"]) -> None: ...

global___XGrammarConfig = XGrammarConfig

@typing.final
class BritonConfig(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    class _BatchSchedulerPolicy:
        ValueType = typing.NewType("ValueType", builtins.int)
        V: typing_extensions.TypeAlias = ValueType

    class _BatchSchedulerPolicyEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[BritonConfig._BatchSchedulerPolicy.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        MAX_UTILIZATION: BritonConfig._BatchSchedulerPolicy.ValueType  # 0
        GUARANTEED_NO_EVICT: BritonConfig._BatchSchedulerPolicy.ValueType  # 1

    class BatchSchedulerPolicy(_BatchSchedulerPolicy, metaclass=_BatchSchedulerPolicyEnumTypeWrapper): ...
    MAX_UTILIZATION: BritonConfig.BatchSchedulerPolicy.ValueType  # 0
    GUARANTEED_NO_EVICT: BritonConfig.BatchSchedulerPolicy.ValueType  # 1

    ENGINE_PATH_FIELD_NUMBER: builtins.int
    HF_TOKENIZER_FIELD_NUMBER: builtins.int
    PORT_FIELD_NUMBER: builtins.int
    BATCH_SCHEDULER_POLICY_FIELD_NUMBER: builtins.int
    ENABLE_TRT_OVERLAP_FIELD_NUMBER: builtins.int
    MAX_TOKENS_IN_PAGED_KV_CACHE_FIELD_NUMBER: builtins.int
    KV_CACHE_FREE_GPU_MEM_FRACTION_FIELD_NUMBER: builtins.int
    MEDUSA_DECODING_MODE_FIELD_NUMBER: builtins.int
    ENABLE_CHUNKED_CONTEXT_FIELD_NUMBER: builtins.int
    ENABLE_KV_CACHE_REUSE_FIELD_NUMBER: builtins.int
    KV_CACHE_HOST_MEMORY_BYTES_FIELD_NUMBER: builtins.int
    LORA_CACHE_MAX_ADAPTER_SIZE_FIELD_NUMBER: builtins.int
    LORA_CACHE_OPTIMAL_ADAPTER_SIZE_FIELD_NUMBER: builtins.int
    LORA_CACHE_GPU_MEMORY_FRACTION_FIELD_NUMBER: builtins.int
    LORA_CACHE_HOST_MEMORY_BYTES_FIELD_NUMBER: builtins.int
    MAX_BATCH_WAIT_MS_FIELD_NUMBER: builtins.int
    ADDED_TOKENS_FIELD_NUMBER: builtins.int
    MAX_NUM_TOKENS_FIELD_NUMBER: builtins.int
    LOOKAHEAD_DECODING_CONFIG_FIELD_NUMBER: builtins.int
    MAX_BATCH_SIZE_FIELD_NUMBER: builtins.int
    XGRAMMAR_CONFIG_FIELD_NUMBER: builtins.int
    engine_path: builtins.str
    hf_tokenizer: builtins.str
    port: builtins.int
    batch_scheduler_policy: global___BritonConfig.BatchSchedulerPolicy.ValueType
    enable_trt_overlap: builtins.bool
    max_tokens_in_paged_kv_cache: builtins.int
    kv_cache_free_gpu_mem_fraction: builtins.float
    medusa_decoding_mode: builtins.bool
    enable_chunked_context: builtins.bool
    enable_kv_cache_reuse: builtins.bool
    kv_cache_host_memory_bytes: builtins.int
    lora_cache_max_adapter_size: builtins.int
    lora_cache_optimal_adapter_size: builtins.int
    lora_cache_gpu_memory_fraction: builtins.float
    lora_cache_host_memory_bytes: builtins.int
    max_batch_wait_ms: builtins.int
    """TODO(pankaj) This field is a placeholder for future use. Currently it is
    not used.

    Wait up to this time for all batch requests to arrive. Normally requests
    should arrive pretty quickly, this is to tackle exceptional cases. So this
    value can be relatively large, say a few ms.
    """
    max_num_tokens: builtins.int
    max_batch_size: builtins.int
    @property
    def added_tokens(self) -> global___AddedTokens:
        """Added tokens to the tokenizer"""

    @property
    def lookahead_decoding_config(self) -> global___LookaheadDecodingConfig: ...
    @property
    def xgrammar_config(self) -> global___XGrammarConfig: ...
    def __init__(
        self,
        *,
        engine_path: builtins.str = ...,
        hf_tokenizer: builtins.str = ...,
        port: builtins.int | None = ...,
        batch_scheduler_policy: global___BritonConfig.BatchSchedulerPolicy.ValueType | None = ...,
        enable_trt_overlap: builtins.bool | None = ...,
        max_tokens_in_paged_kv_cache: builtins.int | None = ...,
        kv_cache_free_gpu_mem_fraction: builtins.float | None = ...,
        medusa_decoding_mode: builtins.bool | None = ...,
        enable_chunked_context: builtins.bool | None = ...,
        enable_kv_cache_reuse: builtins.bool | None = ...,
        kv_cache_host_memory_bytes: builtins.int | None = ...,
        lora_cache_max_adapter_size: builtins.int | None = ...,
        lora_cache_optimal_adapter_size: builtins.int | None = ...,
        lora_cache_gpu_memory_fraction: builtins.float | None = ...,
        lora_cache_host_memory_bytes: builtins.int | None = ...,
        max_batch_wait_ms: builtins.int | None = ...,
        added_tokens: global___AddedTokens | None = ...,
        max_num_tokens: builtins.int | None = ...,
        lookahead_decoding_config: global___LookaheadDecodingConfig | None = ...,
        max_batch_size: builtins.int | None = ...,
        xgrammar_config: global___XGrammarConfig | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing.Literal["_added_tokens", b"_added_tokens", "_batch_scheduler_policy", b"_batch_scheduler_policy", "_enable_chunked_context", b"_enable_chunked_context", "_enable_kv_cache_reuse", b"_enable_kv_cache_reuse", "_enable_trt_overlap", b"_enable_trt_overlap", "_kv_cache_free_gpu_mem_fraction", b"_kv_cache_free_gpu_mem_fraction", "_kv_cache_host_memory_bytes", b"_kv_cache_host_memory_bytes", "_lookahead_decoding_config", b"_lookahead_decoding_config", "_lora_cache_gpu_memory_fraction", b"_lora_cache_gpu_memory_fraction", "_lora_cache_host_memory_bytes", b"_lora_cache_host_memory_bytes", "_lora_cache_max_adapter_size", b"_lora_cache_max_adapter_size", "_lora_cache_optimal_adapter_size", b"_lora_cache_optimal_adapter_size", "_max_batch_size", b"_max_batch_size", "_max_batch_wait_ms", b"_max_batch_wait_ms", "_max_num_tokens", b"_max_num_tokens", "_max_tokens_in_paged_kv_cache", b"_max_tokens_in_paged_kv_cache", "_medusa_decoding_mode", b"_medusa_decoding_mode", "_port", b"_port", "_xgrammar_config", b"_xgrammar_config", "added_tokens", b"added_tokens", "batch_scheduler_policy", b"batch_scheduler_policy", "enable_chunked_context", b"enable_chunked_context", "enable_kv_cache_reuse", b"enable_kv_cache_reuse", "enable_trt_overlap", b"enable_trt_overlap", "kv_cache_free_gpu_mem_fraction", b"kv_cache_free_gpu_mem_fraction", "kv_cache_host_memory_bytes", b"kv_cache_host_memory_bytes", "lookahead_decoding_config", b"lookahead_decoding_config", "lora_cache_gpu_memory_fraction", b"lora_cache_gpu_memory_fraction", "lora_cache_host_memory_bytes", b"lora_cache_host_memory_bytes", "lora_cache_max_adapter_size", b"lora_cache_max_adapter_size", "lora_cache_optimal_adapter_size", b"lora_cache_optimal_adapter_size", "max_batch_size", b"max_batch_size", "max_batch_wait_ms", b"max_batch_wait_ms", "max_num_tokens", b"max_num_tokens", "max_tokens_in_paged_kv_cache", b"max_tokens_in_paged_kv_cache", "medusa_decoding_mode", b"medusa_decoding_mode", "port", b"port", "xgrammar_config", b"xgrammar_config"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing.Literal["_added_tokens", b"_added_tokens", "_batch_scheduler_policy", b"_batch_scheduler_policy", "_enable_chunked_context", b"_enable_chunked_context", "_enable_kv_cache_reuse", b"_enable_kv_cache_reuse", "_enable_trt_overlap", b"_enable_trt_overlap", "_kv_cache_free_gpu_mem_fraction", b"_kv_cache_free_gpu_mem_fraction", "_kv_cache_host_memory_bytes", b"_kv_cache_host_memory_bytes", "_lookahead_decoding_config", b"_lookahead_decoding_config", "_lora_cache_gpu_memory_fraction", b"_lora_cache_gpu_memory_fraction", "_lora_cache_host_memory_bytes", b"_lora_cache_host_memory_bytes", "_lora_cache_max_adapter_size", b"_lora_cache_max_adapter_size", "_lora_cache_optimal_adapter_size", b"_lora_cache_optimal_adapter_size", "_max_batch_size", b"_max_batch_size", "_max_batch_wait_ms", b"_max_batch_wait_ms", "_max_num_tokens", b"_max_num_tokens", "_max_tokens_in_paged_kv_cache", b"_max_tokens_in_paged_kv_cache", "_medusa_decoding_mode", b"_medusa_decoding_mode", "_port", b"_port", "_xgrammar_config", b"_xgrammar_config", "added_tokens", b"added_tokens", "batch_scheduler_policy", b"batch_scheduler_policy", "enable_chunked_context", b"enable_chunked_context", "enable_kv_cache_reuse", b"enable_kv_cache_reuse", "enable_trt_overlap", b"enable_trt_overlap", "engine_path", b"engine_path", "hf_tokenizer", b"hf_tokenizer", "kv_cache_free_gpu_mem_fraction", b"kv_cache_free_gpu_mem_fraction", "kv_cache_host_memory_bytes", b"kv_cache_host_memory_bytes", "lookahead_decoding_config", b"lookahead_decoding_config", "lora_cache_gpu_memory_fraction", b"lora_cache_gpu_memory_fraction", "lora_cache_host_memory_bytes", b"lora_cache_host_memory_bytes", "lora_cache_max_adapter_size", b"lora_cache_max_adapter_size", "lora_cache_optimal_adapter_size", b"lora_cache_optimal_adapter_size", "max_batch_size", b"max_batch_size", "max_batch_wait_ms", b"max_batch_wait_ms", "max_num_tokens", b"max_num_tokens", "max_tokens_in_paged_kv_cache", b"max_tokens_in_paged_kv_cache", "medusa_decoding_mode", b"medusa_decoding_mode", "port", b"port", "xgrammar_config", b"xgrammar_config"]) -> None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_added_tokens", b"_added_tokens"]) -> typing.Literal["added_tokens"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_batch_scheduler_policy", b"_batch_scheduler_policy"]) -> typing.Literal["batch_scheduler_policy"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_enable_chunked_context", b"_enable_chunked_context"]) -> typing.Literal["enable_chunked_context"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_enable_kv_cache_reuse", b"_enable_kv_cache_reuse"]) -> typing.Literal["enable_kv_cache_reuse"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_enable_trt_overlap", b"_enable_trt_overlap"]) -> typing.Literal["enable_trt_overlap"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_kv_cache_free_gpu_mem_fraction", b"_kv_cache_free_gpu_mem_fraction"]) -> typing.Literal["kv_cache_free_gpu_mem_fraction"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_kv_cache_host_memory_bytes", b"_kv_cache_host_memory_bytes"]) -> typing.Literal["kv_cache_host_memory_bytes"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_lookahead_decoding_config", b"_lookahead_decoding_config"]) -> typing.Literal["lookahead_decoding_config"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_lora_cache_gpu_memory_fraction", b"_lora_cache_gpu_memory_fraction"]) -> typing.Literal["lora_cache_gpu_memory_fraction"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_lora_cache_host_memory_bytes", b"_lora_cache_host_memory_bytes"]) -> typing.Literal["lora_cache_host_memory_bytes"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_lora_cache_max_adapter_size", b"_lora_cache_max_adapter_size"]) -> typing.Literal["lora_cache_max_adapter_size"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_lora_cache_optimal_adapter_size", b"_lora_cache_optimal_adapter_size"]) -> typing.Literal["lora_cache_optimal_adapter_size"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_max_batch_size", b"_max_batch_size"]) -> typing.Literal["max_batch_size"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_max_batch_wait_ms", b"_max_batch_wait_ms"]) -> typing.Literal["max_batch_wait_ms"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_max_num_tokens", b"_max_num_tokens"]) -> typing.Literal["max_num_tokens"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_max_tokens_in_paged_kv_cache", b"_max_tokens_in_paged_kv_cache"]) -> typing.Literal["max_tokens_in_paged_kv_cache"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_medusa_decoding_mode", b"_medusa_decoding_mode"]) -> typing.Literal["medusa_decoding_mode"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_port", b"_port"]) -> typing.Literal["port"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_xgrammar_config", b"_xgrammar_config"]) -> typing.Literal["xgrammar_config"] | None: ...

global___BritonConfig = BritonConfig
