Metadata-Version: 2.4
Name: congrad
Version: 0.0.5
Summary: Generic conjugate gradient solver that works with your favorite backend
Author-email: "P. Michael Kielstra" <pmkielstra@math.berkeley.edu>
Project-URL: Homepage, https://github.com/PMKielstra/PyConGrad
Project-URL: Issues, https://github.com/PMKielstra/PyConGrad/issues
Classifier: Programming Language :: Python :: 3
Classifier: License :: CC0 1.0 Universal (CC0 1.0) Public Domain Dedication
Classifier: Operating System :: OS Independent
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: tqdm
Provides-Extra: torch
Requires-Dist: torch; extra == "torch"
Provides-Extra: numpy
Requires-Dist: numpy; extra == "numpy"
Provides-Extra: cupy
Requires-Dist: cupy; extra == "cupy"
Dynamic: license-file

# PyConGrad
A somewhat optimized generic batch conjugate gradient algorithm that works with PyTorch, NumPy, CuPy, and whatever other backend you like as long as you code it up.

Heavily inspired by [sbarratt/torch_cg](https://github.com/sbarratt/torch_cg) and uses similar function signatures.

## Installation and Usage

```bash
$ pip install congrad # If you already have either NumPy, PyTorch, CuPy, or your backend of choice installed
$ pip install congrad[numpy] # To enforce NumPy dependency
$ pip install congrad[torch] # To enforce PyTorch dependency
$ pip install congrad[cupy] # To enforce CuPy dependency
```

```python
from congrad.numpy import cg_batch # Or congrad.torch or congrad.cupy

X = np.random.rand(100, 100)
A = X @ X.T + 0.001 * np.eye(100)
b = np.random.rand(100, 10) # PyConGrad expects matrix equations, so the rightmost dimension is for batching.

def A_batch(x):
    return np.matmul(A, x)

solution, solve_info = cg_batch(A_batch, b)
```

For more information, documentation, and detailed instructions, see the `examples` folder.  In particular, notebook 2 shows you how to use a different batch dimension if you want to batch over vectors instead of matrices.
