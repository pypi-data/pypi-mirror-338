"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn
import triton
import triton.language as tl
from typing import List, Optional, Tuple
from ..utils import is_accelerate_available, is_torch_available

if is_torch_available():
    ...
if is_accelerate_available():
    ...
logger = ...
@triton.jit
def act_quant_kernel(x_ptr, y_ptr, s_ptr, BLOCK_SIZE: tl.constexpr): # -> None:
    ...

def act_quant(x: torch.Tensor, block_size: int = ...) -> Tuple[torch.Tensor, torch.Tensor]:
    ...

def w8a8_block_fp8_matmul_triton(A: torch.Tensor, B: torch.Tensor, As: torch.Tensor, Bs: torch.Tensor, block_size: List[int], output_dtype: torch.dtype = ...) -> torch.Tensor:
    """This function performs matrix multiplication with block-wise
    quantization.
    It takes two input tensors `A` and `B` with scales `As` and `Bs`.
    The output is returned in the specified `output_dtype`.
    Args:
        A: The input tensor, e.g., activation.
        B: The input tensor, e.g., weight.
        As: The per-token-group quantization scale for `A`.
        Bs: The per-block quantization scale for `B`.
        block_size: The block size for per-block quantization. It should
        be 2-dim, e.g., [128, 128].
        output_dytpe: The dtype of the returned tensor.
    Returns:
        torch.Tensor: The result of matmul.
    """
    ...

@torch.compile
def w8a8_block_fp8_matmul_compile(input_q: torch.Tensor, weight_q: torch.Tensor, input_scale: torch.Tensor, weight_scale: torch.Tensor, block_size: Optional[Tuple[int, int]] = ..., output_dtype: torch.dtype = ...) -> torch.Tensor:
    """
    Performs blocked matrix multiplication with FP8 quantized matrices.

    Args:
        input_q: Quantized input tensor with 1x128 block quantization
        weight_q: Quantized weight tensor with 128x128 block quantization
        input_scale: Scaling factors for input blocks
        weight_scale: Scaling factors for weight blocks
        block_size: Tuple of (M, N) for weight block dimensions
        output_dtype: Desired output dtype
    """
    ...

class FP8Linear(nn.Module):
    dtype = ...
    def __init__(self, in_features: int, out_features: int, bias: bool = ..., dtype=..., block_size: Optional[Tuple[int, int]] = ..., device=..., activation_scheme=...) -> None:
        ...
    
    def forward(self, input: torch.Tensor) -> torch.Tensor:
        ...
    


def replace_with_fp8_linear(model, modules_to_not_convert=..., quantization_config=...):
    """Helper function to replace model layers with FP8 versions."""
    ...

