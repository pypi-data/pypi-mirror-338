"""
This type stub file was generated by pyright.
"""

from functools import lru_cache
from typing import Optional
from torch import nn
from ..utils import is_torch_greater_or_equal
from torch.distributed.tensor import Placement

ALL_LAYERNORM_LAYERS = ...
logger = ...
_torch_distributed_available = ...
if is_torch_greater_or_equal("2.5") and _torch_distributed_available:
    ...
def get_packed_weights(param, empty_param, device_mesh, rank, dim):
    """
    When weights are packed (gate_up_proj), we need to make sure each shard gets its correct share.
    So if you have: gate_proj       ( 16, 5120, 8190)
    and             up_proj         ( 16, 5120, 8190)
    packed as       gate_up_proj    ( 16, 5120, 2 * 8190)
    And you shard along the last dimension, you need to interleave the gate and up values:

    Now, if we shard along the last dimension across TP_size (Tensor Parallelism size), we must interleave the values from gate and up projections correctly.

    Let's take TP_size = 4 for an example:

    Packed tensor `gate_up_proj`
    ---------------------------------------------------------------
    [ G0  G1  G2  G3 | G4  G5  G6  G7 | ... | U0  U1  U2  U3 | U4  U5  U6  U7 | ... ]
     ↑─────────────↑   ↑─────────────↑        ↑─────────────↑  ↑─────────────↑
       Gate Slice 0      Gate Slice 1            Up Slice 0       Up Slice 1

    Explanation:
    - The first half of the tensor (left of the center) holds the gate_proj values.
    - The second half (right of the center) holds the up_proj values.
    - For TP=4, we divide each half into 4 slices. In this example, we show two slices for brevity.
    - Each shard receives one slice from the gate part and the corresponding slice from the up part.

    For instance:
    • Shard 0 gets: [ Gate Slice 0, Up Slice 0 ] = [ G0, G1, G2, G3, U0, U1, U2, U3 ]
    • Shard 1 gets: [ Gate Slice 1, Up Slice 1 ] = [ G4, G5, G6, G7, U4, U5, U6, U7 ]
    • … and so on.

    This ensures that each shard receives an equal portion of both gate and up projections, maintaining consistency across tensor parallelism.
    """
    ...

def get_tensor_shard(param, empty_param, device_mesh, rank, dim):
    ...

def distribute_module(module: nn.Module, device_mesh=..., input_fn=..., output_fn=...) -> nn.Module:
    """
    Copy pasted from torch's function but we remove the communications (partitionning)
    as well as buffer registering that is similarly not efficient.
    """
    ...

class TensorParallelLayer:
    """
    General tensor parallel layer for transformers.
    """
    use_dtensor = ...
    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):
        ...
    
    def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:
        ...
    


class GatherParallel(TensorParallelLayer):
    """
    Simple class used to define the hooks to add to a layer when we just want to gather the outputs
    """
    def __init__(self, *, input_layouts: Optional[Placement] = ..., output_layouts: Optional[Placement] = ..., use_local_output: bool = ...) -> None:
        ...
    


class IsolatedParallel(TensorParallelLayer):
    """
    This class is used to isolate computation in a TP layer from the rest of the world.
    Parameters need to be LOCAL, so not dtensors
    """
    def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:
        ...
    


class ColwiseParallel(TensorParallelLayer):
    """
    General tensor parallel layer for transformers.
    """
    def __init__(self, *, input_layouts: Optional[Placement] = ..., output_layouts: Optional[Placement] = ..., use_local_output: bool = ..., use_dtensor=...) -> None:
        ...
    
    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh): # -> Parameter:
        ...
    


class PackedColwiseParallel(ColwiseParallel):
    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh): # -> Parameter:
        ...
    


class RowwiseParallel(TensorParallelLayer):
    """
    Partition a compatible nn.Module in a row-wise fashion. Currently supports nn.Linear and nn.Embedding.
    Users can compose it with ColwiseParallel to achieve the sharding of more complicated modules.
    (i.e. MLP, Attention)

    Keyword Args:
        input_layouts (Placement, optional):
            The DTensor layout of input tensor for the nn.Module, this is used to annotate the input tensor to
            become a DTensor. If not specified, we assume the input tensor to be sharded on the last dimension.
        output_layouts (Placement, optional):
            The DTensor layout of the output for the nn.Module, this is used to ensure the output of the nn.Module
            with the user desired layout. If not specified, the output tensor is replicated.
        use_local_output (bool, optional):
            Whether to use local :class:`torch.Tensor` instead of :class:`DTensor` for the module output, default: True.
    Returns:
        A :class:`ParallelStyle` object that represents Rowwise sharding of the nn.Module.
    """
    def __init__(self, *, input_layouts: Optional[Placement] = ..., output_layouts: Optional[Placement] = ..., use_local_output: bool = ..., use_dtensor=...) -> None:
        ...
    
    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh): # -> Parameter:
        ...
    
    def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:
        ...
    


class PackedRowwiseParallel(RowwiseParallel):
    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh): # -> Parameter:
        ...
    


SUPPORTED_TP_STYLES = ...
@lru_cache
def translate_to_torch_parallel_style(style: str): # -> ColwiseParallel | RowwiseParallel | IsolatedParallel | GatherParallel | PackedRowwiseParallel:
    """
    In model configurations, we use a neutral type (string) to specify parallel
    styles, here we translate them into torch.distributed tensor-parallel
    types.
    """
    ...

def add_tensor_parallel_hooks_to_module(model, module, tp_plan, layer_name, current_module_plan, device_mesh): # -> None:
    """
    Add hooks to the module holding the layer. Meaning:
    ```
    class MyModel(nn.Module):
        def __init__(self):
            self.layer = nn.Linear(10, 10)
    ```
    has state_dict like:
    ```
    {
        "layer.weight": torch.Tensor,
        "layer.bias": torch.Tensor
    }
    ```
    we add hooks to `MyModel` as well as `layer` to make sure that the tensors are correctly sharded and gathered.
    """
    ...

def shard_and_distribute_module(model, param, empty_param, parameter_name, param_casting_dtype, is_contiguous, rank, device_mesh): # -> Parameter:
    r"""
    Main uses cases:
    - column / rowise parallelism, you just shard all the weights of the layer (weight and bias)
    - packed layers: you slice the weights, then shard like above
    - custom operation:
        - you want to add an all-gather at the end of a local layer.
        - you want to have a layer that is isolated from the rest of the world (because torch.DTensor does not work well with `.view` for instance)

    """
    ...

