"""
This type stub file was generated by pyright.
"""

import numpy as np
import torch
from collections.abc import Iterable
from functools import lru_cache
from typing import Any, Optional, TypedDict, Union
from .image_processing_utils import BaseImageProcessor, BatchFeature
from .image_utils import ChannelDimension, ImageInput, PILImageResampling, SizeDict
from .processing_utils import Unpack
from .utils import TensorType, add_start_docstrings, is_torch_available, is_torchvision_available, is_vision_available

if is_vision_available():
    ...
if is_torch_available():
    ...
if is_torchvision_available():
    ...
logger = ...
@lru_cache(maxsize=10)
def validate_fast_preprocess_arguments(do_rescale: Optional[bool] = ..., rescale_factor: Optional[float] = ..., do_normalize: Optional[bool] = ..., image_mean: Optional[Union[float, list[float]]] = ..., image_std: Optional[Union[float, list[float]]] = ..., do_pad: Optional[bool] = ..., size_divisibility: Optional[int] = ..., do_center_crop: Optional[bool] = ..., crop_size: Optional[SizeDict] = ..., do_resize: Optional[bool] = ..., size: Optional[SizeDict] = ..., resample: Optional[PILImageResampling] = ..., return_tensors: Optional[Union[str, TensorType]] = ..., data_format: Optional[ChannelDimension] = ...): # -> None:
    """
    Checks validity of typically used arguments in an `ImageProcessorFast` `preprocess` method.
    Raises `ValueError` if arguments incompatibility is caught.
    """
    ...

def safe_squeeze(tensor: torch.Tensor, axis: Optional[int] = ...) -> torch.Tensor:
    """
    Squeezes a tensor, but only if the axis specified has dim 1.
    """
    ...

def max_across_indices(values: Iterable[Any]) -> list[Any]:
    """
    Return the maximum value across all indices of an iterable of values.
    """
    ...

def get_max_height_width(images: list[torch.Tensor]) -> tuple[int]:
    """
    Get the maximum height and width across all images in a batch.
    """
    ...

def divide_to_patches(image: Union[np.array, torch.Tensor], patch_size: int) -> list[Union[np.array, torch.Tensor]]:
    """
    Divides an image into patches of a specified size.

    Args:
        image (`Union[np.array, "torch.Tensor"]`):
            The input image.
        patch_size (`int`):
            The size of each patch.
    Returns:
        list: A list of Union[np.array, "torch.Tensor"] representing the patches.
    """
    ...

class DefaultFastImageProcessorKwargs(TypedDict, total=False):
    do_resize: Optional[bool]
    size: Optional[dict[str, int]]
    default_to_square: Optional[bool]
    resample: Optional[Union[PILImageResampling, F.InterpolationMode]]
    do_center_crop: Optional[bool]
    crop_size: Optional[dict[str, int]]
    do_rescale: Optional[bool]
    rescale_factor: Optional[Union[int, float]]
    do_normalize: Optional[bool]
    image_mean: Optional[Union[float, list[float]]]
    image_std: Optional[Union[float, list[float]]]
    do_convert_rgb: Optional[bool]
    return_tensors: Optional[Union[str, TensorType]]
    data_format: Optional[ChannelDimension]
    input_data_format: Optional[Union[str, ChannelDimension]]
    device: Optional[torch.device]
    ...


BASE_IMAGE_PROCESSOR_FAST_DOCSTRING = ...
BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS = ...
@add_start_docstrings("Constructs a fast base image processor.", BASE_IMAGE_PROCESSOR_FAST_DOCSTRING)
class BaseImageProcessorFast(BaseImageProcessor):
    resample = ...
    image_mean = ...
    image_std = ...
    size = ...
    default_to_square = ...
    crop_size = ...
    do_resize = ...
    do_center_crop = ...
    do_rescale = ...
    rescale_factor = ...
    do_normalize = ...
    do_convert_rgb = ...
    return_tensors = ...
    data_format = ...
    input_data_format = ...
    device = ...
    model_input_names = ...
    valid_kwargs = DefaultFastImageProcessorKwargs
    unused_kwargs = ...
    def __init__(self, **kwargs: Unpack[DefaultFastImageProcessorKwargs]) -> None:
        ...
    
    def resize(self, image: torch.Tensor, size: SizeDict, interpolation: F.InterpolationMode = ..., antialias: bool = ..., **kwargs) -> torch.Tensor:
        """
        Resize an image to `(size["height"], size["width"])`.

        Args:
            image (`torch.Tensor`):
                Image to resize.
            size (`SizeDict`):
                Dictionary in the format `{"height": int, "width": int}` specifying the size of the output image.
            resample (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):
                `InterpolationMode` filter to use when resizing the image e.g. `InterpolationMode.BICUBIC`.

        Returns:
            `torch.Tensor`: The resized image.
        """
        ...
    
    def rescale(self, image: torch.Tensor, scale: float, **kwargs) -> torch.Tensor:
        """
        Rescale an image by a scale factor. image = image * scale.

        Args:
            image (`torch.Tensor`):
                Image to rescale.
            scale (`float`):
                The scaling factor to rescale pixel values by.

        Returns:
            `torch.Tensor`: The rescaled image.
        """
        ...
    
    def normalize(self, image: torch.Tensor, mean: Union[float, Iterable[float]], std: Union[float, Iterable[float]], **kwargs) -> torch.Tensor:
        """
        Normalize an image. image = (image - image_mean) / image_std.

        Args:
            image (`torch.Tensor`):
                Image to normalize.
            mean (`torch.Tensor`, `float` or `Iterable[float]`):
                Image mean to use for normalization.
            std (`torch.Tensor`, `float` or `Iterable[float]`):
                Image standard deviation to use for normalization.

        Returns:
            `torch.Tensor`: The normalized image.
        """
        ...
    
    def rescale_and_normalize(self, images: torch.Tensor, do_rescale: bool, rescale_factor: float, do_normalize: bool, image_mean: Union[float, list[float]], image_std: Union[float, list[float]]) -> torch.Tensor:
        """
        Rescale and normalize images.
        """
        ...
    
    def center_crop(self, image: torch.Tensor, size: dict[str, int], **kwargs) -> torch.Tensor:
        """
        Center crop an image to `(size["height"], size["width"])`. If the input size is smaller than `crop_size` along
        any edge, the image is padded with 0's and then center cropped.

        Args:
            image (`"torch.Tensor"`):
                Image to center crop.
            size (`Dict[str, int]`):
                Size of the output image.

        Returns:
            `torch.Tensor`: The center cropped image.
        """
        ...
    
    def convert_to_rgb(self, image: ImageInput) -> ImageInput:
        """
        Converts an image to RGB format. Only converts if the image is of type PIL.Image.Image, otherwise returns the image
        as is.
        Args:
            image (ImageInput):
                The image to convert.

        Returns:
            ImageInput: The converted image.
        """
        ...
    
    def filter_out_unused_kwargs(self, kwargs: dict): # -> dict[Any, Any]:
        """
        Filter out the unused kwargs from the kwargs dictionary.
        """
        ...
    
    @add_start_docstrings(BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS)
    def preprocess(self, images: ImageInput, **kwargs: Unpack[DefaultFastImageProcessorKwargs]) -> BatchFeature:
        ...
    
    def to_dict(self): # -> Dict[str, Any]:
        ...
    


class SemanticSegmentationMixin:
    def post_process_semantic_segmentation(self, outputs, target_sizes: list[tuple] = ...): # -> list[Any]:
        """
        Converts the output of [`MobileNetV2ForSemanticSegmentation`] into semantic segmentation maps. Only supports PyTorch.

        Args:
            outputs ([`MobileNetV2ForSemanticSegmentation`]):
                Raw outputs of the model.
            target_sizes (`List[Tuple]` of length `batch_size`, *optional*):
                List of tuples corresponding to the requested final size (height, width) of each prediction. If unset,
                predictions will not be resized.

        Returns:
            semantic_segmentation: `List[torch.Tensor]` of length `batch_size`, where each item is a semantic
            segmentation map of shape (height, width) corresponding to the target_sizes entry (if `target_sizes` is
            specified). Each entry of each `torch.Tensor` correspond to a semantic class id.
        """
        ...
    


