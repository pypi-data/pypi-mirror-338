Metadata-Version: 2.4
Name: llm-wrapper-yl
Version: 0.1.3
Summary: A flexible wrapper for LLM providers with caching, async support, and cost tracking
Project-URL: Homepage, https://github.com/yoonholee/llm-wrapper
Project-URL: Repository, https://github.com/yoonholee/llm-wrapper.git
Author-email: Yoonho Lee <yoonho@stanford.edu>
License: MIT
License-File: LICENSE
Requires-Python: >=3.11
Requires-Dist: diskcache>=5.6.3
Requires-Dist: nest-asyncio>=1.6.0
Requires-Dist: openai>=1.65.1
Requires-Dist: requests>=2.31.0
Requires-Dist: tenacity>=8.5.0
Requires-Dist: together>=1.5.4
Requires-Dist: tqdm>=4.67.1
Provides-Extra: test
Requires-Dist: pytest>=8.3.5; extra == 'test'
Description-Content-Type: text/markdown

# LLM Wrapper

A flexible wrapper for LLM providers with caching, async support, and cost logging.

<https://pypi.org/project/llm-wrapper-yl/>

## Installation

```bash
pip install llm-wrapper-yl
```

## Quick Start

```python
from llm_wrapper import Provider

# Initialize the provider
provider = Provider(model="gpt-4o-mini")

# Generate a single response
response = provider.generate("What is the capital of France?")

# Generate multiple responses with caching
responses = provider.generate(
    ["What is 2+2?", "What is the weather?"],
    system_prompt="You are a helpful assistant.",
    temperature=0.7
)

# For the second call, you will retrieve responses from cache
responses = provider.generate(
    ["What is 2+2?", "What is the weather?"],
    system_prompt="You are a helpful assistant.",
    temperature=0.7
)
```

## Usage with Local Models

```python
from llm_wrapper import Provider

# Initialize the provider with a local model
provider = Provider(
    model="your-model-name",
    host="your-server-host",
    port=8000
)

# Generate responses
responses = provider.generate(["Your prompt here"])
```

## Testing

```bash
# Install local package with test dependencies
uv pip install '.[test]'
# Run tests with output
python -m pytest -s tests
```

## Deploying

```bash
hatch build
twine upload dist/*
```
