seed_everything: true
trainer:
  accelerator: gpu
  strategy: ddp_find_unused_parameters_true
  devices: 8
  num_nodes: 4
  precision: 16-mixed
  max_steps: 1500000
  check_val_every_n_epoch: null
  val_check_interval: 5005 ## one imagenet epoch length
  num_sanity_val_steps: -1
  log_every_n_steps: 100
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: "../../checkpoints/vqgan/test"
        save_top_k: -1 # save all checkpoints
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
  logger:
    class_path: lightning.pytorch.loggers.TensorBoardLogger
    init_args:
      save_dir: "../../results/vqgan/"
      version: "test"
      name:

model:
  class_path: OpenImageTokenizer.Open_MAGVIT2.models.lfqgan_pretrain.VQModel
  init_args:
    ddconfig:
      double_z: False
      z_channels: 14 #18
      resolution: 128
      in_channels: 3
      out_ch: 3
      ch: 128
      ch_mult: [1,1,2,2,4]  # num_down = len(ch_mult)-1
      num_res_blocks: 4

    lossconfig:
      target: OpenImageTokenizer.Open_MAGVIT2.modules.losses.vqperceptual.VQLPIPSWithDiscriminator
      params:
        disc_conditional: False
        disc_in_channels: 3
        disc_start: 0 # from 0 epoch #70000 is ok
        disc_num_layers: 4
        disc_weight: 0.8
        gen_loss_weight: 0.1 #using 0.1 for more training stability
        lecam_loss_weight: 0.05
        codebook_weight: 0.1 #can be lowered to 0.05
        commit_weight: 0.25
        codebook_enlarge_ratio: 0
        codebook_enlarge_steps: 2000
        disc_loss: hinge
        disc_num_channels: 3
        disc_num_stages: 3
        disc_hidden_channels: 128
        blur_resample: True
        blur_kernel_size: 4
        use_blur: True

    n_embed: 16384 #262144
    embed_dim: 14 #18
    learning_rate: 1e-4
    sample_minimization_weight: 1.0
    batch_maximization_weight: 1.0
    scheduler_type: "None"
    use_ema: True
    use_shared_epoch: True
    sche_type: cos
    wpe: 0.01 ## learning rate decay to zero
    wp: 1 ##one epoch for linear warmup
    wp0: 0.0 ##for warmup #from zero to lr
    max_iter: 1500000
    wp_iter: 5000

data:
  class_path: main.DataModuleFromConfig
  init_args:
    batch_size: 8
    num_workers: 16
    train:
      target: OpenImageTokenizer.Open_MAGVIT2.data.pretrain.LAIONCombineTrain
      params:
        config:
          size: 256
          subset:
          filter_path: ["../../data/laion-aesthetic-v2_filter_keys.json", "../../data/JourneyDB_filter_keys.json", "../../data/laion-aesthetic_v1_filter_keys.json", "../../data/laion-hd_sub_filter_keys_2.json"]
          sample_json_path: ["../../data/laion-coco_samples.json", "../../data/cc15m_samples_2.json", "../../data/laion-aesthetic-v2_samples.json", "../../data/JourneyDB_samples.json", "../../data/laion-aesthetic_v1_samples.json", "../../data/laion-hd_sub_samples_2.json"] 
          sample_coco_urls: ../../data/laion-coco_sample_urls_20M.txt #please specify your path
          sample_hd_urls: ../../data/laion-hd_sample_urls_30M_2.txt ##please specify your path
          data_dir: ["../../data/LAION-COCO-Recaption", "../../data/CC12M/webdataset/gcc12m_shards", "../../data/Laion-aesthetic-v2/data", "../../data/CC3M/webdataset/gcc3m_shards", "../../data/public_datasets/JourneyDB/wds", "../../data/laion-aesthetics-12M/webdataset_train", "../../public_datasets/laion-hd/webdataset_train/"]
          image_key: [jpg, jpeg.jpg, "jpg.jpg"]
          enable_image: True
    validation:
      target: OpenImageTokenizer.Open_MAGVIT2.data.imagenet.ImageNetValidation
      params:
        config:
          size: 256
          subset:
    test:
      target: OpenImageTokenizer.Open_MAGVIT2.data.imagenet.ImageNetValidation
      params:
        config:
          size: 256
          subset:

ckpt_path: null # to resume