Metadata-Version: 2.4
Name: maskit-learn
Version: 0.1.3
Summary: [MASK]it -Efficient pre-trained encoder adaption leveraging the [MASK]
Author-email: Eugenia Alleva <eugenia.alleva@gmail.com>
License: Apache 2.0
Requires-Python: >=3.10.16
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: transformers
Requires-Dist: torch
Provides-Extra: dev
Requires-Dist: pytest; extra == "dev"
Requires-Dist: black; extra == "dev"
Requires-Dist: flake8; extra == "dev"
Dynamic: license-file

# [MASK]-It - Lightweight framework for efficient encoder model fine-tuning

## Install
requires python 3.10 or above
```
pip install maskit-learn
```

## Usage
### Define task
```aiignore
# task and model
classes = ['happy', 'sad']
verbalizer_map = {'happy':['happy', 'fun'],
                  'sad':['sad', 'cry']}
model_name = 'google-bert/bert-base-uncased'
```
### Select pre-trained model
```aiignore
from maskit.model import maskitModel
model = maskitModel(model_name=model_name,
                    verbalizer_map=verbalizer_map)
```
### Load dataset
```aiignore
from torch.utils.data import DataLoader
from maskit.dataset import maskitDataset
text = ['I am so happy today that I cannot stay still','I am very very sad unfortunately']
labels = [1,0]
template = '{text}. This sentence is: [MASK]'
dataset = maskitDataset(text, labels, model_name, template)
dataloader = DataLoader(dataset=dataset, batch_size=2)
```

### Inference
```aiignore
batch = next(iter(dataloader))
output = model(**batch)
```

