{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First install the repo and requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip --quiet install git+https://github.com/wilson-labs/cola.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033a7033",
   "metadata": {},
   "source": [
    "# Philosophy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf0e4de",
   "metadata": {},
   "source": [
    "The framework of automatic differentiation has revolutionized machine learning.\n",
    "Although the rules that govern derivatives have long been known, automatically computing derivatives was a nontrivial process that required\n",
    "(1) efficient implementations of base-case primitive derivatives,\n",
    "(2) software abstractions (autograd and computation graphs) to compose these primitives into complex computations, and\n",
    "(3) a mechanism for users to modify or extend compositional rules to new functions.\n",
    "Once libraries such as PyTorch, Chainer, Tensorflow, JAX, and others\n",
    "figured out the correct abstractions, the impact was enormous.\n",
    "Efforts that previously went into deriving and implementing gradients could be repurposed into developing new models.\n",
    "\n",
    "In `CoLA`, we automate another notorious bottleneck for ML methods: performing large-scale linear algebra (e.g. matrix solves, eigenvalue problems, nullspace computations).\n",
    "These ubiquitous operations are at the heart of principal component analysis, Gaussian processes, normalizing flows, equivariant neural networks, and many other applications.\n",
    "As with automatic differentiation, structure-aware linear algebra is ripe for automation.\n",
    "We introduce a general numerical framework that dramatically simplifies implementation efforts while achieving a high degree of computational efficiency.\n",
    "In code, we represent structure matrices as \n",
    "`LinearOperator` objects which adhere to the same API as standard dense matrices.\n",
    "For example, a user can call inverse or eig on any `LinearOperator`, and under the hood our framework derives a computationally efficient algorithm built from roughly 70 compositional _dispatch rules_. If little is known about the `LinearOperator`, the derived algorithm reverts to a general-purpose base case (e.g. Gaussian elimination or GMRES for linear solves). Conversely, if the `LinearOperator` is known to be the Kronecker product of a lower triangular matrix and a positive definite Toeplitz matrix, for example, the derived algorithm uses specialty algorithms for Kronecker, triangular, and positive definite matrices. Through this compositional pattern matching,\n",
    "our framework can match or outperform special-purpose implementations across numerous applications despite relying on only 25 base `LinearOperator` types.\n",
    "\n",
    "In the table below we show the presence of dispatch rules (blue square) in our framework across different linear algebra operations (inverse, eig, diagonal, transpose, exp, determinant) and different types of `LinearOperators`. Some of the dispatch rules can be derived from a combination of previous ones (red squares)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e49fa56",
   "metadata": {},
   "source": [
    "![Alt Text](../CoLA_Table1.png)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
