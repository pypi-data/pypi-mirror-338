length of y:
1308
training regressor
{'regressor__estimator__n_estimators': [100], 'regressor__estimator__max_features': [4], 'regressor__estimator__max_depth': [50], 'regressor__estimator__min_samples_leaf': [0.5], 'regressor__estimator__max_samples': [0.5]}
Fitting 3 folds for each of 1 candidates, totalling 3 fits
exported model to: ./tests/ModelOutput/2-phase/model/rf/Emiliania_huxleyi_reg.sav
exported scoring to: ./tests/ModelOutput/2-phase/scoring/rf/Emiliania_huxleyi_reg.sav
reg rRMSE: 198%
reg rMAE: 150%
reg R2: -0.2
training classifier
length of y_clf:
1308
[1. 1. 1. ... 1. 1. 0.]
Fitting 3 folds for each of 1 candidates, totalling 3 fits
exported model to:./tests/ModelOutput/2-phase/model/rf/Emiliania_huxleyi_clf.sav
exported scoring to: ./tests/ModelOutput/2-phase/scoring/rf/Emiliania_huxleyi_clf.sav
[0.87219634 0.86072291 0.89937026]
clf balanced accuracy 0.88
training zero-inflated regressor
exported model to: ./tests/ModelOutput/2-phase/model/rf/Emiliania_huxleyi_zir.sav
exported scoring to: ./tests/ModelOutput/2-phase/scoring/rf/Emiliania_huxleyi_zir.sav
zir rRMSE: 146%
zir rMAE: 76%
zir R2: 0.22
execution time: 7.152557373046875e-07 seconds
training regressor
{'regressor__estimator__learning_rate': [0.05], 'regressor__estimator__n_estimators': [69], 'regressor__estimator__max_depth': [7], 'regressor__estimator__subsample': [0.8], 'regressor__estimator__colsample_bytree': [0.5], 'regressor__estimator__gamma': [1], 'regressor__estimator__reg_alpha': [0.1]}
Fitting 3 folds for each of 1 candidates, totalling 3 fits
exported model to: ./tests/ModelOutput/2-phase/model/xgb/Emiliania_huxleyi_reg.sav
exported scoring to: ./tests/ModelOutput/2-phase/scoring/xgb/Emiliania_huxleyi_reg.sav
reg rRMSE: 206%
reg rMAE: 154%
reg R2: -0.31
training classifier
length of y_clf:
1308
[1. 1. 1. ... 1. 1. 0.]
Fitting 3 folds for each of 1 candidates, totalling 3 fits
exported model to:./tests/ModelOutput/2-phase/model/xgb/Emiliania_huxleyi_clf.sav
exported scoring to: ./tests/ModelOutput/2-phase/scoring/xgb/Emiliania_huxleyi_clf.sav
[0.67844203 0.62840752 0.65243271]
clf balanced accuracy 0.65
training zero-inflated regressor
exported model to: ./tests/ModelOutput/2-phase/model/xgb/Emiliania_huxleyi_zir.sav
exported scoring to: ./tests/ModelOutput/2-phase/scoring/xgb/Emiliania_huxleyi_zir.sav
zir rRMSE: 173%
zir rMAE: 89%
zir R2: -0.09
execution time: 1.1920928955078125e-06 seconds
training regressor
{'regressor__estimator__max_samples': [0.2], 'regressor__estimator__max_features': [0.2], 'regressor__estimator__estimator__leaf_size': [25], 'regressor__estimator__estimator__n_neighbors': [3]}
Fitting 3 folds for each of 1 candidates, totalling 3 fits
exported model to: ./tests/ModelOutput/2-phase/model/knn/Emiliania_huxleyi_reg.sav
exported scoring to: ./tests/ModelOutput/2-phase/scoring/knn/Emiliania_huxleyi_reg.sav
reg rRMSE: 188%
reg rMAE: 146%
reg R2: -0.09
training classifier
length of y_clf:
1308
[1. 1. 1. ... 1. 1. 0.]
Fitting 3 folds for each of 1 candidates, totalling 3 fits
exported model to:./tests/ModelOutput/2-phase/model/knn/Emiliania_huxleyi_clf.sav
exported scoring to: ./tests/ModelOutput/2-phase/scoring/knn/Emiliania_huxleyi_clf.sav
[0.60571946 0.62405107 0.62508627]
clf balanced accuracy 0.62
training zero-inflated regressor
exported model to: ./tests/ModelOutput/2-phase/model/knn/Emiliania_huxleyi_zir.sav
exported scoring to: ./tests/ModelOutput/2-phase/scoring/knn/Emiliania_huxleyi_zir.sav
zir rRMSE: 182%
zir rMAE: 96%
zir R2: -0.2
execution time: 4.76837158203125e-07 seconds
initialized prediction
number of models in ensemble:3
predicting zero-inflated regressor
===================
DEBUG OF ZIR MODELS
===================
rf
<class 'abil.zir.ZeroInflatedRegressor'>
<class 'pandas.core.frame.DataFrame'>
<class 'pandas.core.frame.DataFrame'>
transformation is None
model is classifier
model is classifier
model not cloned
model is:  RandomForestClassifier(max_depth=50, max_samples=0.8, oob_score=True,
                       random_state=1)
model inferred to be RF
features_for_members: [[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]]
model is a classifier
model is:  RandomForestClassifier(max_depth=50, max_samples=0.8, oob_score=True,
                       random_state=1)
model inferred to be RF
features_for_members: [[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]]
results.shape:  (436, 100)
model does not use log
model not cloned
model is:  RandomForestClassifier(max_depth=50, max_samples=0.8, oob_score=True,
                       random_state=1)
model inferred to be RF
features_for_members: [[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]]
model is a classifier
model is:  RandomForestClassifier(max_depth=50, max_samples=0.8, oob_score=True,
                       random_state=1)
model inferred to be RF
features_for_members: [[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]]
results.shape:  (436, 100)
model does not use log
model not cloned
model is:  RandomForestClassifier(max_depth=50, max_samples=0.8, oob_score=True,
                       random_state=1)
model inferred to be RF
features_for_members: [[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]]
model is a classifier
model is:  RandomForestClassifier(max_depth=50, max_samples=0.8, oob_score=True,
                       random_state=1)
model inferred to be RF
features_for_members: [[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]]
results.shape:  (436, 100)
model does not use log
model is:  RandomForestClassifier(max_depth=50, max_samples=0.8, oob_score=True,
                       random_state=1)
model inferred to be RF
features_for_members: [[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]]
results.shape:  (300, 100)
model does not use log
y transformer is : <bound method LogGridSearch.do_log of <abil.log_grid_search.LogGridSearch object at 0x7389af15ce00>>
if log should be ~6.9:  [[6.90875478]]
y inverse transformer is : <bound method LogGridSearch.do_exp of <abil.log_grid_search.LogGridSearch object at 0x7389af15ebd0>>
if log should be ~1000:  [[991.27471561]]
model not cloned
model is:  RandomForestRegressor(max_depth=50, max_features=4, max_samples=0.5,
                      min_samples_leaf=0.5, oob_score=True, random_state=1)
model inferred to be RF
features_for_members: [[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]]
model is a regressor
model is:  RandomForestRegressor(max_depth=50, max_features=4, max_samples=0.5,
                      min_samples_leaf=0.5, oob_score=True, random_state=1)
model inferred to be RF
features_for_members: [[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]]
results.shape:  (436, 100)
model not cloned
model is:  RandomForestRegressor(max_depth=50, max_features=4, max_samples=0.5,
                      min_samples_leaf=0.5, oob_score=True, random_state=1)
model inferred to be RF
features_for_members: [[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]]
model is a regressor
model is:  RandomForestRegressor(max_depth=50, max_features=4, max_samples=0.5,
                      min_samples_leaf=0.5, oob_score=True, random_state=1)
model inferred to be RF
features_for_members: [[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]]
results.shape:  (436, 100)
model not cloned
model is:  RandomForestRegressor(max_depth=50, max_features=4, max_samples=0.5,
                      min_samples_leaf=0.5, oob_score=True, random_state=1)
model inferred to be RF
features_for_members: [[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]]
model is a regressor
model is:  RandomForestRegressor(max_depth=50, max_features=4, max_samples=0.5,
                      min_samples_leaf=0.5, oob_score=True, random_state=1)
model inferred to be RF
features_for_members: [[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]]
results.shape:  (436, 100)
model is:  RandomForestRegressor(max_depth=50, max_features=4, max_samples=0.5,
                      min_samples_leaf=0.5, oob_score=True, random_state=1)
model inferred to be RF
features_for_members: [[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]]
results.shape:  (300, 100)
finished exporting summary stats to:  ./tests/ModelOutput/2-phase/predictions/rf/clf/Emiliania_huxleyi.nc
finished exporting summary stats to:  ./tests/ModelOutput/2-phase/predictions/rf/reg/Emiliania_huxleyi.nc
finished exporting summary stats to:  ./tests/ModelOutput/2-phase/predictions/rf/Emiliania_huxleyi.nc
predicting zero-inflated regressor
===================
DEBUG OF ZIR MODELS
===================
knn
<class 'abil.zir.ZeroInflatedRegressor'>
<class 'pandas.core.frame.DataFrame'>
<class 'pandas.core.frame.DataFrame'>
transformation is None
model is classifier
model is classifier
model not cloned
model is:  BaggingClassifier(estimator=KNeighborsClassifier(leaf_size=25, n_neighbors=3),
                  max_features=0.2, max_samples=0.2, n_estimators=3,
                  random_state=1)
model inferred to be B-KNN
features_for_members: [array([1]), array([2]), array([0])]
model is a classifier
model is:  BaggingClassifier(estimator=KNeighborsClassifier(leaf_size=25, n_neighbors=3),
                  max_features=0.2, max_samples=0.2, n_estimators=3,
                  random_state=1)
model inferred to be B-KNN
features_for_members: [array([1]), array([2]), array([0])]
results.shape:  (436, 3)
model does not use log
model not cloned
model is:  BaggingClassifier(estimator=KNeighborsClassifier(leaf_size=25, n_neighbors=3),
                  max_features=0.2, max_samples=0.2, n_estimators=3,
                  random_state=1)
model inferred to be B-KNN
features_for_members: [array([1]), array([2]), array([0])]
model is a classifier
model is:  BaggingClassifier(estimator=KNeighborsClassifier(leaf_size=25, n_neighbors=3),
                  max_features=0.2, max_samples=0.2, n_estimators=3,
                  random_state=1)
model inferred to be B-KNN
features_for_members: [array([1]), array([2]), array([0])]
results.shape:  (436, 3)
model does not use log
model not cloned
model is:  BaggingClassifier(estimator=KNeighborsClassifier(leaf_size=25, n_neighbors=3),
                  max_features=0.2, max_samples=0.2, n_estimators=3,
                  random_state=1)
model inferred to be B-KNN
features_for_members: [array([1]), array([2]), array([0])]
model is a classifier
model is:  BaggingClassifier(estimator=KNeighborsClassifier(leaf_size=25, n_neighbors=3),
                  max_features=0.2, max_samples=0.2, n_estimators=3,
                  random_state=1)
model inferred to be B-KNN
features_for_members: [array([1]), array([2]), array([0])]
results.shape:  (436, 3)
model does not use log
model is:  BaggingClassifier(estimator=KNeighborsClassifier(leaf_size=25, n_neighbors=3),
                  max_features=0.2, max_samples=0.2, n_estimators=3,
                  random_state=1)
model inferred to be B-KNN
features_for_members: [array([1]), array([2]), array([0])]
results.shape:  (300, 3)
model does not use log
y transformer is : <bound method LogGridSearch.do_log of <abil.log_grid_search.LogGridSearch object at 0x738a3bb43c50>>
if log should be ~6.9:  [[6.90875478]]
y inverse transformer is : <bound method LogGridSearch.do_exp of <abil.log_grid_search.LogGridSearch object at 0x738a3bb43e90>>
if log should be ~1000:  [[991.27471561]]
model not cloned
model is:  BaggingRegressor(estimator=KNeighborsRegressor(leaf_size=25, n_neighbors=3),
                 max_features=0.2, max_samples=0.2, n_estimators=3,
                 random_state=1)
model inferred to be B-KNN
features_for_members: [array([1]), array([2]), array([0])]
model is a regressor
model is:  BaggingRegressor(estimator=KNeighborsRegressor(leaf_size=25, n_neighbors=3),
                 max_features=0.2, max_samples=0.2, n_estimators=3,
                 random_state=1)
model inferred to be B-KNN
features_for_members: [array([1]), array([2]), array([0])]
results.shape:  (436, 3)
model not cloned
model is:  BaggingRegressor(estimator=KNeighborsRegressor(leaf_size=25, n_neighbors=3),
                 max_features=0.2, max_samples=0.2, n_estimators=3,
                 random_state=1)
model inferred to be B-KNN
features_for_members: [array([1]), array([2]), array([0])]
model is a regressor
model is:  BaggingRegressor(estimator=KNeighborsRegressor(leaf_size=25, n_neighbors=3),
                 max_features=0.2, max_samples=0.2, n_estimators=3,
                 random_state=1)
model inferred to be B-KNN
features_for_members: [array([1]), array([2]), array([0])]
results.shape:  (436, 3)
model not cloned
model is:  BaggingRegressor(estimator=KNeighborsRegressor(leaf_size=25, n_neighbors=3),
                 max_features=0.2, max_samples=0.2, n_estimators=3,
                 random_state=1)
model inferred to be B-KNN
features_for_members: [array([1]), array([2]), array([0])]
model is a regressor
model is:  BaggingRegressor(estimator=KNeighborsRegressor(leaf_size=25, n_neighbors=3),
                 max_features=0.2, max_samples=0.2, n_estimators=3,
                 random_state=1)
model inferred to be B-KNN
features_for_members: [array([1]), array([2]), array([0])]
results.shape:  (436, 3)
model is:  BaggingRegressor(estimator=KNeighborsRegressor(leaf_size=25, n_neighbors=3),
                 max_features=0.2, max_samples=0.2, n_estimators=3,
                 random_state=1)
model inferred to be B-KNN
features_for_members: [array([1]), array([2]), array([0])]
results.shape:  (300, 3)
finished exporting summary stats to:  ./tests/ModelOutput/2-phase/predictions/knn/clf/Emiliania_huxleyi.nc
finished exporting summary stats to:  ./tests/ModelOutput/2-phase/predictions/knn/reg/Emiliania_huxleyi.nc
finished exporting summary stats to:  ./tests/ModelOutput/2-phase/predictions/knn/Emiliania_huxleyi.nc
predicting zero-inflated regressor
===================
DEBUG OF ZIR MODELS
===================
xgb
<class 'abil.zir.ZeroInflatedRegressor'>
<class 'pandas.core.frame.DataFrame'>
<class 'pandas.core.frame.DataFrame'>
transformation is None
model is classifier
model is classifier
model not cloned
model is:  XGBClassifier(base_score=None, booster=None, callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=1, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=4, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=69, n_jobs=None, nthread=1, num_parallel_tree=None,
              predictor=None, ...)
model is XGBoost
n_estimators:  69
booster : <xgboost.core.Booster object at 0x738a3bb92ba0>
model is a classifier
model is:  XGBClassifier(base_score=None, booster=None, callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=1, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=4, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=69, n_jobs=None, nthread=1, num_parallel_tree=None,
              predictor=None, ...)
model is XGBoost
n_estimators:  69
booster : <xgboost.core.Booster object at 0x738a3bb92ba0>
results.shape:  (436, 69)
model does not use log
model not cloned
model is:  XGBClassifier(base_score=None, booster=None, callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=1, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=4, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=69, n_jobs=None, nthread=1, num_parallel_tree=None,
              predictor=None, ...)
model is XGBoost
n_estimators:  69
booster : <xgboost.core.Booster object at 0x738a3bb92ba0>
model is a classifier
model is:  XGBClassifier(base_score=None, booster=None, callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=1, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=4, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=69, n_jobs=None, nthread=1, num_parallel_tree=None,
              predictor=None, ...)
model is XGBoost
n_estimators:  69
booster : <xgboost.core.Booster object at 0x738a3bb92ba0>
results.shape:  (436, 69)
model does not use log
model not cloned
model is:  XGBClassifier(base_score=None, booster=None, callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=1, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=4, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=69, n_jobs=None, nthread=1, num_parallel_tree=None,
              predictor=None, ...)
model is XGBoost
n_estimators:  69
booster : <xgboost.core.Booster object at 0x738a3bb92ba0>
model is a classifier
model is:  XGBClassifier(base_score=None, booster=None, callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=1, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=4, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=69, n_jobs=None, nthread=1, num_parallel_tree=None,
              predictor=None, ...)
model is XGBoost
n_estimators:  69
booster : <xgboost.core.Booster object at 0x738a3bb92ba0>
results.shape:  (436, 69)
model does not use log
model is:  XGBClassifier(base_score=None, booster=None, callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=1, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=4, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              n_estimators=69, n_jobs=None, nthread=1, num_parallel_tree=None,
              predictor=None, ...)
model is XGBoost
n_estimators:  69
booster : <xgboost.core.Booster object at 0x738a3bb92ba0>
results.shape:  (300, 69)
model does not use log
y transformer is : <bound method LogGridSearch.do_log of <abil.log_grid_search.LogGridSearch object at 0x738a3bb909e0>>
if log should be ~6.9:  [[6.90875478]]
y inverse transformer is : <bound method LogGridSearch.do_exp of <abil.log_grid_search.LogGridSearch object at 0x738a3bb93200>>
if log should be ~1000:  [[991.27471561]]
model not cloned
model is:  XGBRegressor(base_score=None, booster=None, callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.5, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=1, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=0.05, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=7, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=69, n_jobs=None, nthread=1, num_parallel_tree=None,
             objective='reg:tweedie', ...)
model is XGBoost
n_estimators:  69
booster : <xgboost.core.Booster object at 0x738a3bb5ef30>
model is a regressor
model is:  XGBRegressor(base_score=None, booster=None, callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.5, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=1, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=0.05, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=7, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=69, n_jobs=None, nthread=1, num_parallel_tree=None,
             objective='reg:tweedie', ...)
model is XGBoost
n_estimators:  69
booster : <xgboost.core.Booster object at 0x738a3bb5ef30>
results.shape:  (436, 69)
model not cloned
model is:  XGBRegressor(base_score=None, booster=None, callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.5, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=1, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=0.05, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=7, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=69, n_jobs=None, nthread=1, num_parallel_tree=None,
             objective='reg:tweedie', ...)
model is XGBoost
n_estimators:  69
booster : <xgboost.core.Booster object at 0x738a3bb5ef30>
model is a regressor
model is:  XGBRegressor(base_score=None, booster=None, callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.5, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=1, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=0.05, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=7, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=69, n_jobs=None, nthread=1, num_parallel_tree=None,
             objective='reg:tweedie', ...)
model is XGBoost
n_estimators:  69
booster : <xgboost.core.Booster object at 0x738a3bb5ef30>
results.shape:  (436, 69)
model not cloned
model is:  XGBRegressor(base_score=None, booster=None, callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.5, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=1, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=0.05, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=7, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=69, n_jobs=None, nthread=1, num_parallel_tree=None,
             objective='reg:tweedie', ...)
model is XGBoost
n_estimators:  69
booster : <xgboost.core.Booster object at 0x738a3bb5ef30>
model is a regressor
model is:  XGBRegressor(base_score=None, booster=None, callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.5, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=1, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=0.05, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=7, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=69, n_jobs=None, nthread=1, num_parallel_tree=None,
             objective='reg:tweedie', ...)
model is XGBoost
n_estimators:  69
booster : <xgboost.core.Booster object at 0x738a3bb5ef30>
results.shape:  (436, 69)
model is:  XGBRegressor(base_score=None, booster=None, callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.5, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=1, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=0.05, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=7, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=69, n_jobs=None, nthread=1, num_parallel_tree=None,
             objective='reg:tweedie', ...)
model is XGBoost
n_estimators:  69
booster : <xgboost.core.Booster object at 0x738a3bb5ef30>
results.shape:  (300, 69)
finished exporting summary stats to:  ./tests/ModelOutput/2-phase/predictions/xgb/clf/Emiliania_huxleyi.nc
finished exporting summary stats to:  ./tests/ModelOutput/2-phase/predictions/xgb/reg/Emiliania_huxleyi.nc
