"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from .chatcompletionmessage import ChatCompletionMessage, ChatCompletionMessageTypedDict
from atoma_sdk.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from pydantic import model_serializer
from typing import Any, Dict, List, Optional
from typing_extensions import NotRequired, TypedDict


class CreateChatCompletionRequestTypedDict(TypedDict):
    messages: List[ChatCompletionMessageTypedDict]
    r"""A list of messages comprising the conversation so far"""
    model: str
    r"""ID of the model to use"""
    frequency_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their
    existing frequency in the text so far
    """
    function_call: NotRequired[Any]
    r"""Controls how the model responds to function calls"""
    functions: NotRequired[Nullable[List[Any]]]
    r"""A list of functions the model may generate JSON inputs for"""
    logit_bias: NotRequired[Nullable[Dict[str, float]]]
    r"""Modify the likelihood of specified tokens appearing in the completion"""
    max_tokens: NotRequired[Nullable[int]]
    r"""The maximum number of tokens to generate in the chat completion"""
    n: NotRequired[Nullable[int]]
    r"""How many chat completion choices to generate for each input message"""
    presence_penalty: NotRequired[Nullable[float]]
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on
    whether they appear in the text so far
    """
    response_format: NotRequired[Any]
    r"""The format to return the response in"""
    seed: NotRequired[Nullable[int]]
    r"""If specified, our system will make a best effort to sample deterministically"""
    stop: NotRequired[Nullable[List[str]]]
    r"""Up to 4 sequences where the API will stop generating further tokens"""
    stream: NotRequired[Nullable[bool]]
    r"""Whether to stream back partial progress. Must be false for this request type."""
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, between 0 and 2"""
    tool_choice: NotRequired[Any]
    r"""Controls which (if any) tool the model should use"""
    tools: NotRequired[Nullable[List[Any]]]
    r"""A list of tools the model may call"""
    top_p: NotRequired[Nullable[float]]
    r"""An alternative to sampling with temperature"""
    user: NotRequired[Nullable[str]]
    r"""A unique identifier representing your end-user"""


class CreateChatCompletionRequest(BaseModel):
    messages: List[ChatCompletionMessage]
    r"""A list of messages comprising the conversation so far"""

    model: str
    r"""ID of the model to use"""

    frequency_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on their
    existing frequency in the text so far
    """

    function_call: Optional[Any] = None
    r"""Controls how the model responds to function calls"""

    functions: OptionalNullable[List[Any]] = UNSET
    r"""A list of functions the model may generate JSON inputs for"""

    logit_bias: OptionalNullable[Dict[str, float]] = UNSET
    r"""Modify the likelihood of specified tokens appearing in the completion"""

    max_tokens: OptionalNullable[int] = UNSET
    r"""The maximum number of tokens to generate in the chat completion"""

    n: OptionalNullable[int] = UNSET
    r"""How many chat completion choices to generate for each input message"""

    presence_penalty: OptionalNullable[float] = UNSET
    r"""Number between -2.0 and 2.0. Positive values penalize new tokens based on
    whether they appear in the text so far
    """

    response_format: Optional[Any] = None
    r"""The format to return the response in"""

    seed: OptionalNullable[int] = UNSET
    r"""If specified, our system will make a best effort to sample deterministically"""

    stop: OptionalNullable[List[str]] = UNSET
    r"""Up to 4 sequences where the API will stop generating further tokens"""

    stream: OptionalNullable[bool] = False
    r"""Whether to stream back partial progress. Must be false for this request type."""

    temperature: OptionalNullable[float] = UNSET
    r"""What sampling temperature to use, between 0 and 2"""

    tool_choice: Optional[Any] = None
    r"""Controls which (if any) tool the model should use"""

    tools: OptionalNullable[List[Any]] = UNSET
    r"""A list of tools the model may call"""

    top_p: OptionalNullable[float] = UNSET
    r"""An alternative to sampling with temperature"""

    user: OptionalNullable[str] = UNSET
    r"""A unique identifier representing your end-user"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "frequency_penalty",
            "function_call",
            "functions",
            "logit_bias",
            "max_tokens",
            "n",
            "presence_penalty",
            "response_format",
            "seed",
            "stop",
            "stream",
            "temperature",
            "tool_choice",
            "tools",
            "top_p",
            "user",
        ]
        nullable_fields = [
            "frequency_penalty",
            "functions",
            "logit_bias",
            "max_tokens",
            "n",
            "presence_penalty",
            "seed",
            "stop",
            "stream",
            "temperature",
            "tools",
            "top_p",
            "user",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
