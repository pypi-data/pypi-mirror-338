{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ship Dataset\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp ship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from nonlinear_benchmarks.utilities import get_tmp_benchmark_directory\n",
    "from pathlib import Path\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from easyDataverse import Dataverse\n",
    "import pandas as pd\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def ship(\n",
    "        save_path: Path, #directory the files are written to, created if it does not exist\n",
    "        force_download: bool = False, # force download the dataset\n",
    "        remove_download = True\n",
    "):\n",
    "    save_path = Path(save_path)\n",
    "    download_dir = Path(get_tmp_benchmark_directory()) / 'Ship'\n",
    "\n",
    "    if force_download and download_dir.exists():\n",
    "        print(f\"Force reload: Removing existing directory: {download_dir}\")\n",
    "        shutil.rmtree(download_dir)\n",
    "\n",
    "    dataverse = Dataverse('https://darus.uni-stuttgart.de/')\n",
    "    dataverse.load_dataset(\n",
    "        pid='doi:10.18419/darus-2905',\n",
    "        filedir=download_dir,\n",
    "    )\n",
    "\n",
    "    #str to Path to be plattform independent\n",
    "    structure_mapping = {\n",
    "        Path('patrol_ship_routine/processed/train'): 'train',\n",
    "        Path('patrol_ship_routine/processed/validation'): 'valid',\n",
    "        Path('patrol_ship_routine/processed/test'): 'test',\n",
    "        Path('patrol_ship_ood/processed/test'): 'test_ood'\n",
    "    }\n",
    "\n",
    "    # Ensure desired directories exist\n",
    "    for subdir in structure_mapping.values():\n",
    "        os.makedirs(os.path.join(save_path, subdir), exist_ok=True)\n",
    "\n",
    "    def convert_tab_to_hdf5(tab_path, hdf5_path):\n",
    "        df = pd.read_csv(tab_path, sep='\\t')\n",
    "        with h5py.File(hdf5_path, 'w') as hdf:\n",
    "            for column in df.columns:\n",
    "                data = df[column].astype(np.float32).values\n",
    "                hdf.create_dataset(column, data=data, dtype='f4')\n",
    "\n",
    "    # Walk through the current directory structure and process files\n",
    "    for subdir, dirs, files in os.walk(download_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.tab'):\n",
    "                current_file_path = os.path.join(subdir, file)\n",
    "                \n",
    "                # Determine the relative path\n",
    "                relative_subdir = Path(os.path.relpath(subdir, download_dir))\n",
    "                \n",
    "                # Find the corresponding desired subdir\n",
    "                if relative_subdir in structure_mapping:\n",
    "                    desired_subdir = structure_mapping[relative_subdir]\n",
    "                    \n",
    "                    # Construct desired file paths\n",
    "                    base_filename = file.replace('.tab', '')\n",
    "                    desired_hdf5_path = os.path.join(save_path, desired_subdir, base_filename + '.hdf5')\n",
    "                    \n",
    "                    convert_tab_to_hdf5(current_file_path, desired_hdf5_path)\n",
    "\n",
    "    #remove downloaded files\n",
    "    if remove_download:\n",
    "        shutil.rmtree(download_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4680b2b464d4dd6aa3dd7c93a6ce8c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ðŸŽ‰ <span style=\"font-weight: bold\">Connected to </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'https://darus.uni-stuttgart.de/'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ðŸŽ‰ \u001b[1mConnected to \u001b[0m\u001b[1;32m'https://darus.uni-stuttgart.de/'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Fetching dataset </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'doi:10.18419/darus-2905'</span><span style=\"font-weight: bold\"> from </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'https://darus.uni-stuttgart.de/'</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mFetching dataset \u001b[0m\u001b[1;32m'doi:10.18419/darus-2905'\u001b[0m\u001b[1m from \u001b[0m\u001b[1;32m'https://darus.uni-stuttgart.de/'\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ <span style=\"font-weight: bold\">Dataset Information</span> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
       "â”‚ Title: <span style=\"font-weight: bold\">A Simulated 4-DOF Ship Motion Dataset for System Identification under Environmental Disturbances</span> â”‚\n",
       "â”‚ Version: latest                                                                                         â”‚\n",
       "â”‚ Files: 126                                                                                              â”‚\n",
       "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \u001b[1mDataset Information\u001b[0m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
       "â”‚ Title: \u001b[1mA Simulated 4-DOF Ship Motion Dataset for System Identification under Environmental Disturbances\u001b[0m â”‚\n",
       "â”‚ Version: latest                                                                                         â”‚\n",
       "â”‚ Files: 126                                                                                              â”‚\n",
       "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5c1003c22c4481880d33f4eb097820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">Downloading files</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1mDownloading files\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â•°â”€â”€ <span style=\"font-weight: bold\">âœ… Done </span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â•°â”€â”€ \u001b[1mâœ… Done \u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp_dir = Path('./tmp')\n",
    "ship(tmp_dir / 'ship')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#clean temporary hdf5 file\n",
    "import shutil\n",
    "# shutil.rmtree(tmp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
