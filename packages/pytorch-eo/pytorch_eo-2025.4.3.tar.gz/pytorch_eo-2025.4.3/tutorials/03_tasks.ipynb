{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch EO Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch EO core design revolves around `Tasks`. Examples of tasks are: image classification, object detection, image segmentation, etc.\n",
    "\n",
    "A task is defined by:\n",
    "1. `model`: a neural network that will be trained to perform the task\n",
    "2. `hparams`: hyperparameters used to train the model (optimizer, scheduler and additional information)\n",
    "3. `loss function`: criterion used during the optimization process\n",
    "4. `metrics`: evaluation of the model in the task\n",
    "5. `inputs` and `outputs`: lists with the names of the inputs-outputs\n",
    "6. `other parameters`: additional parameters that are task specific\n",
    "\n",
    "You don't have to provide all these paremeters, we offer good defaults for most of them. However, you can customize them as you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models are neural networks that take inputs and produce outputs. You can build your own or use third party models. By default we use `torchvision`, so you can use any model from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1000])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from pytorch_eo.tasks.BaseTask import BaseTask\n",
    "\n",
    "task = BaseTask('resnet18')\n",
    "\n",
    "output = task(torch.randn(32,3,224,224))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you use models from torchvision, you can pass any parameter in the `hparams` object as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/juan/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 63.9MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1000])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams = {\n",
    "    'model': {\n",
    "        'weights': 'ResNet18_Weights.IMAGENET1K_V1'\n",
    "    }\n",
    "}\n",
    "\n",
    "task = BaseTask('resnet18', hparams)\n",
    "\n",
    "output = task(torch.randn(32,3,224,224))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can always make your own models, for example modifying an existing model from torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision \n",
    "\n",
    "model = torchvision.models.resnet18()\n",
    "model.fc = torch.nn.Linear(512, 10)\n",
    "\n",
    "task = BaseTask(model)\n",
    "\n",
    "output = task(torch.randn(32,3,224,224))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or building your own model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, pretrained, num_classes):\n",
    "        super().__init__()\n",
    "        resnet = torchvision.models.resnet18(pretrained=pretrained)\n",
    "        self.backbone = torch.nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x)\n",
    "        return self.fc(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/Desktop/pytorch_eo/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/juan/Desktop/pytorch_eo/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyModel(pretrained=True, num_classes=10)\n",
    "\n",
    "task = BaseTask(model)\n",
    "\n",
    "output = task(torch.randn(32, 3, 224,224))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But you would probably want to use third party implementations with extra functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d427df2f8c3344c2ae23c871e402f1f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 112, 112])\n",
      "torch.Size([32, 64, 56, 56])\n",
      "torch.Size([32, 128, 28, 28])\n",
      "torch.Size([32, 256, 14, 14])\n",
      "torch.Size([32, 512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "import timm \n",
    "\n",
    "model = timm.create_model(\n",
    "    'resnet18',\n",
    "    pretrained='imagenet',\n",
    "    in_chans=3,\n",
    "    num_classes=10,\n",
    "    features_only=True\n",
    ")\n",
    "\n",
    "task = BaseTask(model)\n",
    "\n",
    "output = task(torch.randn(32,3,224,224))\n",
    "for o in output:\n",
    "    print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /home/juan/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 53.5MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2, 224, 224])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name='resnet18',\n",
    "    encoder_weights='imagenet',\n",
    "    in_channels=3,\n",
    "    classes=2,\n",
    ")\n",
    "\n",
    "task = BaseTask(model)\n",
    "\n",
    "output = task(torch.randn(32,3,224,224))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hparams\n",
    "\n",
    "This is a `dict` holding the hyperparameters used for training. Pytorch lightning will save the object in the model's checkpoint (that can be used to resume training, for example) so make sure to add any additional information that you want to save. In some cases, default parameters will be used if hparams are not provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    # hparams used for training\n",
    "    'model': { # only if you use default torchvision models\n",
    "        'weights': 'ResNet18_Weights.IMAGENET1K_V1'\n",
    "    },\n",
    "    'loss': 'CrossEntropyLoss', # choose one from pytorch docs\n",
    "    'loss_params': {}, \n",
    "    'optimizer': 'Adam', # choose one from pytorch docs\n",
    "    'optim_params': {\n",
    "        'lr': 1e-4\n",
    "    },\n",
    "    'scheduler': 'CosineAnnealingLR', # choose one from pytorch docs\n",
    "    'scheduler_params': {\n",
    "        'T_max': 10,\n",
    "        'verbose': True\n",
    "    }\n",
    "    # extra\n",
    "    # epochs, batch size, model, transforms, ...\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can specify the loss function to use during training as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/Desktop/pytorch_eo/.venv/lib/python3.12/site-packages/lightning/fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/juan/Desktop/pytorch_eo/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name    | Type               | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model   | ResNet             | 11.2 M | train\n",
      "1 | loss_fn | CrossEntropyLoss   | 0      | train\n",
      "2 | acc     | MulticlassAccuracy | 0      | train\n",
      "-------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.727    Total estimated model params size (MB)\n",
      "70        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57292cc76683453abb26136ccf363ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/Desktop/pytorch_eo/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "/home/juan/Desktop/pytorch_eo/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "/home/juan/Desktop/pytorch_eo/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fca194cbf444c1c8cb664903831271c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb47abc25f4947ceb9ba718da0875ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ff5c577c0d42229cd92768227d2916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec4d15efc0349d88cde619f02729e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "from pytorch_eo.datasets import EuroSATRGB\n",
    "from pytorch_eo.tasks import ImageClassification\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "trans = A.Compose([\n",
    "    A.Normalize(0, 1),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "ds = EuroSATRGB(batch_size=32, train_trans=trans, val_trans=trans)\n",
    "\n",
    "model = torchvision.models.resnet18()\n",
    "model.fc = torch.nn.Linear(512, 10)\n",
    "\n",
    "task = ImageClassification(model, loss_fn=torch.nn.CrossEntropyLoss(), num_classes=ds.num_classes)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator='cuda',\n",
    "    devices=1,\n",
    "    precision=16,\n",
    "    max_epochs=3,\n",
    "    limit_train_batches=10\n",
    ")\n",
    "\n",
    "trainer.fit(task, ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last piece to train a model is the metrics. You can train without metrics, use third party libraries like `torchmetrics` or use your own implementations. In most tasks we will use at least one metric even if they are not provided (they are that important !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "from pytorch_eo.datasets import EuroSATRGB\n",
    "from pytorch_eo.tasks import ImageClassification\n",
    "import torchmetrics\n",
    "\n",
    "ds = EuroSATRGB(batch_size=32, train_trans=trans, val_trans=trans)\n",
    "\n",
    "model = torchvision.models.resnet18()\n",
    "model.fc = torch.nn.Linear(512, ds.num_classes)\n",
    "\n",
    "metrics = {'acc': torchmetrics.Accuracy(task=\"multiclass\", num_classes=ds.num_classes)}\n",
    "\n",
    "task = ImageClassification(model, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/Desktop/pytorch_eo/.venv/lib/python3.12/site-packages/lightning/fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name    | Type               | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model   | ResNet             | 11.2 M | train\n",
      "1 | loss_fn | CrossEntropyLoss   | 0      | train\n",
      "2 | acc     | MulticlassAccuracy | 0      | train\n",
      "-------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.727    Total estimated model params size (MB)\n",
      "70        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea29fe726d94869a6c7e08154526161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/Desktop/pytorch_eo/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "/home/juan/Desktop/pytorch_eo/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "/home/juan/Desktop/pytorch_eo/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022ef3b0470b4e0facd3dffb22edfa9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02289172a5a94ff49c594ebdbed5da46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b278307fd61c4135880c25e4cb50c9ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f515b7bab01f47a3be9673b9c0b958c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    accelerator='cuda',\n",
    "    devices=1,\n",
    "    precision=16,\n",
    "    max_epochs=3,\n",
    "    limit_train_batches=10\n",
    ")\n",
    "\n",
    "trainer.fit(task, ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use as many metrics as you want (all will be logged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_accuracy(y_hat, y):\n",
    "    return (torch.argmax(y_hat, axis=1) == y).sum() / y.shape[0]\n",
    "\n",
    "metrics = {\n",
    "    'acc': torchmetrics.Accuracy(task=\"multiclass\", num_classes=ds.num_classes), \n",
    "    'my_acc': my_accuracy\n",
    "}\n",
    "\n",
    "task = ImageClassification(model, hparams, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/Desktop/pytorch_eo/.venv/lib/python3.12/site-packages/lightning/fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/juan/Desktop/pytorch_eo/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name    | Type               | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model   | ResNet             | 11.2 M | train\n",
      "1 | loss_fn | CrossEntropyLoss   | 0      | train\n",
      "2 | acc     | MulticlassAccuracy | 0      | train\n",
      "-------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.727    Total estimated model params size (MB)\n",
      "70        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533d2b06f05d4c978188b0353a3bd25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/Desktop/pytorch_eo/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "/home/juan/Desktop/pytorch_eo/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "/home/juan/Desktop/pytorch_eo/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163ab22819144dd6b8eb824b1815ff56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc05838005104775bbfce91991d1e9cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7cf003bda34672b0ee9cec6cbb9c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4351dedf42ed42e0a346eafe98408d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    accelerator='cuda',\n",
    "    devices=1,\n",
    "    precision=16,\n",
    "    max_epochs=3,\n",
    "    limit_train_batches=10\n",
    ")\n",
    "\n",
    "trainer.fit(task, ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs and Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PytorchEO datasets are designed to be flexible in the number and modality of inputs and outputs in order to allow for advanced applications such as data fusion, unsupervised learning or multi-task learning. For this reason, you must tell the task which fields in your datasets have to be used for inputs and outputs, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pytorch_eo.utils import download_url, unzip_file\n",
    "from pathlib import Path\n",
    "from pytorch_eo.datasets import RGBImageDataset\n",
    "from pytorch_eo.datasets import ConcatDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning as L\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def prepare_data(path):\n",
    "\tos.makedirs(path, exist_ok=True)\n",
    "\turl = \"http://madm.dfki.de/files/sentinel/EuroSAT.zip\"\n",
    "\tcompressed_data_filename = 'EuroSAT.zip'\n",
    "\tdata_folder = '2750'\n",
    "\tcompressed_data_path = path / compressed_data_filename\n",
    "\tdownload_url(url, compressed_data_path)\n",
    "\tunzip_file(compressed_data_path, path, msg=\"extracting data ...\")\n",
    "\tuncompressed_data_path = path / data_folder\n",
    "\tclasses = sorted(os.listdir(uncompressed_data_path))\n",
    "\timages, labels = [], []\n",
    "\tfor ix, label in enumerate(classes):\n",
    "\t\t_images = os.listdir(uncompressed_data_path / label)\n",
    "\t\timages += [str(uncompressed_data_path /\n",
    "\t\t\t\t\tlabel / img) for img in _images]\n",
    "\t\tlabels += [ix]*len(_images)\n",
    "\treturn images, labels\n",
    "\n",
    "class EuroSATDataset(L.LightningDataModule):\n",
    "\tdef __init__(self, path='data', trans=None):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.path = Path(path)\n",
    "\t\tself.trans = trans\n",
    "\n",
    "\tdef setup(self, stage=None):\n",
    "\t\timages, labels = prepare_data(self.path)\n",
    "\t\timages_ds = RGBImageDataset(images) \n",
    "\t\t# Custom keys in the dataset !\n",
    "\t\tself.ds = ConcatDataset({'abc': images_ds, 'def': labels}, trans=self.trans, image_key='abc')\n",
    "\n",
    "\tdef train_dataloader(self):\n",
    "\t\treturn DataLoader(self.ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/Desktop/pytorch_eo/.venv/lib/python3.12/site-packages/lightning/fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/juan/Desktop/pytorch_eo/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "extracting data ...: 100%|██████████| 27011/27011 [00:03<00:00, 8470.04it/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name    | Type               | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model   | ResNet             | 11.2 M | train\n",
      "1 | loss_fn | CrossEntropyLoss   | 0      | train\n",
      "2 | acc     | MulticlassAccuracy | 0      | train\n",
      "-------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.727    Total estimated model params size (MB)\n",
      "70        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/juan/Desktop/pytorch_eo/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "/home/juan/Desktop/pytorch_eo/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2bae90134648eeb2a91552008ff815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "trans = A.Compose([\n",
    "    A.Normalize(0, 1),\n",
    "    ToTensorV2(),\n",
    "], additional_targets={'abc': 'image'})\n",
    "ds = EuroSATDataset(trans=trans)\n",
    "model = torchvision.models.resnet18()\n",
    "model.fc = torch.nn.Linear(512, 10)\n",
    "\n",
    "task = ImageClassification(model, inputs=['abc'], outputs=['def'], num_classes=10)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator='cuda',\n",
    "    devices=1,\n",
    "    precision=16,\n",
    "    max_epochs=3,\n",
    "    limit_train_batches=10\n",
    ")\n",
    "\n",
    "trainer.fit(task, ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks are `LightningModule`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, note that `Tasks` are built on top of `Pytorch Lightning`'s `LightningModule`, so you can build your own tasks easily and use any of our `Datasets` (which are in turn `LightningDataModule`s)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
